{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1640934547396,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"},"user_tz":-480},"id":"BiO3TVc0x3Gl","outputId":"d3c3f6e7-24b2-4bc6-f662-4de799ecea45"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/model_inversion_lenet_svhn\n"]}],"source":["cd \"drive/MyDrive/model_inversion_lenet_svhn\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"s3WBnGOpyCdY","executionInfo":{"status":"ok","timestamp":1640934548710,"user_tz":-480,"elapsed":1317,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","class Extractor(nn.Module):\n","\n","    def __init__(self, channel=3):\n","        super(Extractor, self).__init__()\n","        self.extractor = nn.Sequential(\n","            nn.Conv2d(channel, 6, 5),\n","            nn.AvgPool2d(2, 2),\n","            nn.Sigmoid(),\n","            nn.Conv2d(6, 16, 5),\n","            nn.AvgPool2d(2, 2),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.extractor(x)\n","        return x\n","\n","    \n","class Classifier(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(Classifier, self).__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(16 * 5 * 5, 120),\n","            nn.Sigmoid(),\n","            nn.Linear(120, 84),\n","            nn.Sigmoid(),\n","            nn.Linear(84, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.classifier(x)\n","        return x\n","\n","\n","class Generator(nn.Module):\n","\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.embedding = nn.Embedding(10, 10)\n","        self.generator = nn.Sequential(\n","            nn.ConvTranspose2d(100 + 10, 512, 2, 1, 0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(512, 256, 2, 1, 0, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(256, 128, 2, 1, 0, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(128, 16, 2, 1, 0, bias=False),\n","            nn.Sigmoid(),\n","        )\n","        self.apply(weights_init)\n","\n","    def forward(self, z, y):\n","        y = self.embedding(y)\n","        y = y.unsqueeze(-1).unsqueeze(-1)\n","        feat = torch.cat([z, y], 1)\n","        feat = self.generator(feat)\n","        return feat\n","\n","    \n","    \n","class Discriminator(nn.Module):\n","\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.embedding = nn.Embedding(10, 10)\n","        self.discriminator = nn.Sequential(\n","            nn.Conv2d(16 + 10, 128, 2, 1, 0, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, 2, 1, 0, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 512, 2, 1, 0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(512, 1, 2, 1, 0, bias=False),\n","            nn.Sigmoid(),\n","        )\n","        self.apply(weights_init)\n","\n","    def forward(self, feat, y):\n","        y = self.embedding(y)\n","        y = y.unsqueeze(-1).unsqueeze(-1)\n","        y = y.expand(y.size(0), 10, 5, 5)\n","        feat = torch.cat([feat, y], 1)\n","        feat = self.discriminator(feat)\n","        feat = feat.squeeze(-1).squeeze(-1)\n","        return feat"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7767,"status":"ok","timestamp":1640934556476,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"},"user_tz":-480},"id":"cZ0VLFMex767","outputId":"998a0745-8c1a-4291-9c14-253ea0b3e9f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: ./data/train_32x32.mat\n","Using downloaded and verified file: ./data/test_32x32.mat\n"]}],"source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import transforms\n","import torchvision.datasets as datasets\n","import numpy as np\n","\n","svhn_transform = transforms.Compose([\n","    transforms.Resize([32, 32]),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","mnist_transform = transforms.Compose([\n","    transforms.Resize([32, 32]),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5), (0.5)),\n","])\n","\n","svhn_trainset = datasets.SVHN(root='./data', split='train', download=True, transform=svhn_transform)\n","svhn_testset = datasets.SVHN(root='./data', split='test', download=True, transform=svhn_transform)\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n","mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n","\n","size = len(svhn_trainset)\n","index = np.arange(size)\n","client_trainset = Subset(svhn_trainset, index[:2000])\n","server_iid_trainset = Subset(svhn_trainset, index[2000:4000])\n","server_niid_trainset = Subset(mnist_trainset, index[:2000])\n","\n","client_testset = Subset(svhn_testset, index[:2000])\n","server_iid_testset = Subset(svhn_testset, index[2000:4000])\n","server_niid_testset = Subset(mnist_testset, index[:2000])\n","\n","client_trainloader = DataLoader(client_trainset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n","server_iid_trainloader = DataLoader(server_iid_trainset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n","server_niid_trainloader = DataLoader(server_niid_trainset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n","\n","client_testloader = DataLoader(client_testset, batch_size=2000, shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"VbvFSIwRyEc0","executionInfo":{"status":"ok","timestamp":1640934556477,"user_tz":-480,"elapsed":4,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}}},"outputs":[],"source":["import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def get_params(net, modules):\n","    params = []\n","    for module in modules:\n","        params.append({\"params\": net[module].parameters()})\n","    return params\n","\n","def frozen_net(net, modules, frozen):\n","    for module in modules:\n","        for param in net[module].parameters():\n","            param.requires_grad = not frozen\n","        if frozen:\n","            net[module].eval()\n","        else:\n","            net[module].train()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577891,"status":"ok","timestamp":1640935134365,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"},"user_tz":-480},"id":"60HzDW-EyM3y","outputId":"53457087-4825-45cf-9d48-6f7bdd43602c"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:2.239776, acc:0.189000\n","epoch:[ 1], loss:2.230130, acc:0.189000\n","epoch:[ 2], loss:2.230889, acc:0.189000\n","epoch:[ 3], loss:2.230863, acc:0.189000\n","epoch:[ 4], loss:2.227734, acc:0.189000\n","epoch:[ 5], loss:2.230422, acc:0.189000\n","epoch:[ 6], loss:2.230324, acc:0.189000\n","epoch:[ 7], loss:2.227965, acc:0.189000\n","epoch:[ 8], loss:2.228403, acc:0.189000\n","epoch:[ 9], loss:2.227216, acc:0.189000\n","epoch:[10], loss:2.228623, acc:0.189000\n","epoch:[11], loss:2.227960, acc:0.189000\n","epoch:[12], loss:2.226628, acc:0.189000\n","epoch:[13], loss:2.224988, acc:0.189000\n","epoch:[14], loss:2.224798, acc:0.189000\n","epoch:[15], loss:2.223324, acc:0.189000\n","epoch:[16], loss:2.224984, acc:0.189000\n","epoch:[17], loss:2.223456, acc:0.189000\n","epoch:[18], loss:2.223081, acc:0.189000\n","epoch:[19], loss:2.222948, acc:0.189000\n","epoch:[20], loss:2.222226, acc:0.189000\n","epoch:[21], loss:2.221411, acc:0.189000\n","epoch:[22], loss:2.220594, acc:0.189000\n","epoch:[23], loss:2.218295, acc:0.189000\n","epoch:[24], loss:2.218994, acc:0.189000\n","epoch:[25], loss:2.216871, acc:0.189000\n","epoch:[26], loss:2.213584, acc:0.189000\n","epoch:[27], loss:2.213969, acc:0.187000\n","epoch:[28], loss:2.211908, acc:0.188500\n","epoch:[29], loss:2.210123, acc:0.188000\n","epoch:[30], loss:2.210778, acc:0.186500\n","epoch:[31], loss:2.207750, acc:0.187500\n","epoch:[32], loss:2.204942, acc:0.189500\n","epoch:[33], loss:2.203999, acc:0.188000\n","epoch:[34], loss:2.201361, acc:0.187500\n","epoch:[35], loss:2.200236, acc:0.186500\n","epoch:[36], loss:2.197400, acc:0.190000\n","epoch:[37], loss:2.195304, acc:0.189500\n","epoch:[38], loss:2.194206, acc:0.192000\n","epoch:[39], loss:2.190011, acc:0.180000\n","epoch:[40], loss:2.188928, acc:0.191000\n","epoch:[41], loss:2.186338, acc:0.192000\n","epoch:[42], loss:2.181203, acc:0.194500\n","epoch:[43], loss:2.178499, acc:0.194500\n","epoch:[44], loss:2.175938, acc:0.195000\n","epoch:[45], loss:2.170317, acc:0.197000\n","epoch:[46], loss:2.162719, acc:0.189500\n","epoch:[47], loss:2.156671, acc:0.202500\n","epoch:[48], loss:2.146332, acc:0.204500\n","epoch:[49], loss:2.132732, acc:0.211000\n","epoch:[50], loss:2.111478, acc:0.215500\n","epoch:[51], loss:2.082783, acc:0.236000\n","epoch:[52], loss:2.053642, acc:0.264000\n","epoch:[53], loss:2.023131, acc:0.261000\n","epoch:[54], loss:1.994907, acc:0.269000\n","epoch:[55], loss:1.967146, acc:0.290000\n","epoch:[56], loss:1.939073, acc:0.304500\n","epoch:[57], loss:1.911200, acc:0.314500\n","epoch:[58], loss:1.889563, acc:0.320000\n","epoch:[59], loss:1.865603, acc:0.331500\n","epoch:[60], loss:1.842446, acc:0.351000\n","epoch:[61], loss:1.815381, acc:0.341500\n","epoch:[62], loss:1.794639, acc:0.355500\n","epoch:[63], loss:1.769429, acc:0.360500\n","epoch:[64], loss:1.748615, acc:0.369500\n","epoch:[65], loss:1.727369, acc:0.369500\n","epoch:[66], loss:1.710718, acc:0.382000\n","epoch:[67], loss:1.689007, acc:0.392000\n","epoch:[68], loss:1.670272, acc:0.393000\n","epoch:[69], loss:1.654633, acc:0.397000\n","epoch:[70], loss:1.634644, acc:0.390000\n","epoch:[71], loss:1.619692, acc:0.398500\n","epoch:[72], loss:1.596357, acc:0.422000\n","epoch:[73], loss:1.582191, acc:0.413500\n","epoch:[74], loss:1.566084, acc:0.417000\n","epoch:[75], loss:1.546981, acc:0.439000\n","epoch:[76], loss:1.524353, acc:0.444500\n","epoch:[77], loss:1.506978, acc:0.446500\n","epoch:[78], loss:1.489458, acc:0.448500\n","epoch:[79], loss:1.470494, acc:0.458000\n","epoch:[80], loss:1.452247, acc:0.458500\n","epoch:[81], loss:1.435151, acc:0.463000\n","epoch:[82], loss:1.410705, acc:0.469000\n","epoch:[83], loss:1.395158, acc:0.461500\n","epoch:[84], loss:1.376237, acc:0.483000\n","epoch:[85], loss:1.355708, acc:0.478000\n","epoch:[86], loss:1.337709, acc:0.476000\n","epoch:[87], loss:1.320115, acc:0.476500\n","epoch:[88], loss:1.304076, acc:0.493500\n","epoch:[89], loss:1.285768, acc:0.494000\n","epoch:[90], loss:1.261886, acc:0.485500\n","epoch:[91], loss:1.248122, acc:0.494500\n","epoch:[92], loss:1.227457, acc:0.504500\n","epoch:[93], loss:1.209681, acc:0.512500\n","epoch:[94], loss:1.196173, acc:0.491500\n","epoch:[95], loss:1.181537, acc:0.516500\n","epoch:[96], loss:1.158599, acc:0.526000\n","epoch:[97], loss:1.143591, acc:0.516500\n","epoch:[98], loss:1.128951, acc:0.520000\n","epoch:[99], loss:1.111723, acc:0.531000\n","epoch:[100], loss:1.096396, acc:0.538500\n","epoch:[101], loss:1.079843, acc:0.531000\n","epoch:[102], loss:1.061617, acc:0.537000\n","epoch:[103], loss:1.051583, acc:0.540500\n","epoch:[104], loss:1.028754, acc:0.541500\n","epoch:[105], loss:1.013065, acc:0.538000\n","epoch:[106], loss:0.997574, acc:0.545000\n","epoch:[107], loss:0.983379, acc:0.555000\n","epoch:[108], loss:0.967074, acc:0.549500\n","epoch:[109], loss:0.951199, acc:0.554000\n","epoch:[110], loss:0.940143, acc:0.562000\n","epoch:[111], loss:0.924167, acc:0.563500\n","epoch:[112], loss:0.909568, acc:0.571000\n","epoch:[113], loss:0.887065, acc:0.564000\n","epoch:[114], loss:0.874473, acc:0.565500\n","epoch:[115], loss:0.862198, acc:0.572000\n","epoch:[116], loss:0.846201, acc:0.578000\n","epoch:[117], loss:0.827237, acc:0.573000\n","epoch:[118], loss:0.814678, acc:0.570000\n","epoch:[119], loss:0.804724, acc:0.582000\n","epoch:[120], loss:0.787448, acc:0.578000\n","epoch:[121], loss:0.767218, acc:0.588500\n","epoch:[122], loss:0.759906, acc:0.583000\n","epoch:[123], loss:0.746545, acc:0.597000\n","epoch:[124], loss:0.729881, acc:0.598000\n","epoch:[125], loss:0.712326, acc:0.588500\n","epoch:[126], loss:0.701329, acc:0.598500\n","epoch:[127], loss:0.682661, acc:0.589500\n","epoch:[128], loss:0.674870, acc:0.601000\n","epoch:[129], loss:0.657482, acc:0.595000\n","epoch:[130], loss:0.645791, acc:0.606500\n","epoch:[131], loss:0.633951, acc:0.610500\n","epoch:[132], loss:0.617386, acc:0.598500\n","epoch:[133], loss:0.604192, acc:0.597500\n","epoch:[134], loss:0.588559, acc:0.604500\n","epoch:[135], loss:0.578614, acc:0.610500\n","epoch:[136], loss:0.563656, acc:0.616500\n","epoch:[137], loss:0.554018, acc:0.618500\n","epoch:[138], loss:0.538972, acc:0.609500\n","epoch:[139], loss:0.527727, acc:0.615000\n","epoch:[140], loss:0.513847, acc:0.621000\n","epoch:[141], loss:0.498298, acc:0.609500\n","epoch:[142], loss:0.490045, acc:0.616000\n","epoch:[143], loss:0.476837, acc:0.616500\n","epoch:[144], loss:0.465811, acc:0.610500\n","epoch:[145], loss:0.453478, acc:0.620000\n","epoch:[146], loss:0.437807, acc:0.621000\n","epoch:[147], loss:0.426895, acc:0.629000\n","epoch:[148], loss:0.414876, acc:0.622500\n","epoch:[149], loss:0.404671, acc:0.614500\n","epoch:[150], loss:0.399014, acc:0.638500\n","epoch:[151], loss:0.379817, acc:0.625000\n","epoch:[152], loss:0.371884, acc:0.626500\n","epoch:[153], loss:0.358246, acc:0.633000\n","epoch:[154], loss:0.356075, acc:0.629500\n","epoch:[155], loss:0.340978, acc:0.635000\n","epoch:[156], loss:0.330504, acc:0.640500\n","epoch:[157], loss:0.322715, acc:0.629500\n","epoch:[158], loss:0.311526, acc:0.631000\n","epoch:[159], loss:0.301298, acc:0.633500\n","epoch:[160], loss:0.292684, acc:0.638500\n","epoch:[161], loss:0.282530, acc:0.632500\n","epoch:[162], loss:0.270015, acc:0.637500\n","epoch:[163], loss:0.263151, acc:0.640000\n","epoch:[164], loss:0.254371, acc:0.641500\n","epoch:[165], loss:0.245341, acc:0.638500\n","epoch:[166], loss:0.239281, acc:0.636000\n","epoch:[167], loss:0.224709, acc:0.634500\n","epoch:[168], loss:0.222334, acc:0.641000\n","epoch:[169], loss:0.217211, acc:0.630500\n","epoch:[170], loss:0.208632, acc:0.638000\n","epoch:[171], loss:0.195825, acc:0.648000\n","epoch:[172], loss:0.191998, acc:0.639500\n","epoch:[173], loss:0.182873, acc:0.640000\n","epoch:[174], loss:0.175351, acc:0.650500\n","epoch:[175], loss:0.168083, acc:0.638000\n","epoch:[176], loss:0.162060, acc:0.639500\n","epoch:[177], loss:0.155230, acc:0.646000\n","epoch:[178], loss:0.151141, acc:0.639000\n","epoch:[179], loss:0.143026, acc:0.638000\n","epoch:[180], loss:0.136855, acc:0.644500\n","epoch:[181], loss:0.129223, acc:0.640000\n","epoch:[182], loss:0.123218, acc:0.647500\n","epoch:[183], loss:0.123111, acc:0.646500\n","epoch:[184], loss:0.113709, acc:0.639500\n","epoch:[185], loss:0.108339, acc:0.640500\n","epoch:[186], loss:0.105238, acc:0.638000\n","epoch:[187], loss:0.100684, acc:0.645000\n","epoch:[188], loss:0.096310, acc:0.645000\n","epoch:[189], loss:0.090370, acc:0.645500\n","epoch:[190], loss:0.086813, acc:0.649000\n","epoch:[191], loss:0.084029, acc:0.654000\n","epoch:[192], loss:0.080349, acc:0.649000\n","epoch:[193], loss:0.075603, acc:0.641500\n","epoch:[194], loss:0.074014, acc:0.646000\n","epoch:[195], loss:0.068543, acc:0.649000\n","epoch:[196], loss:0.065992, acc:0.652000\n","epoch:[197], loss:0.065038, acc:0.648000\n","epoch:[198], loss:0.060283, acc:0.650000\n","epoch:[199], loss:0.057186, acc:0.648500\n","epoch:[200], loss:0.055464, acc:0.643500\n","epoch:[201], loss:0.053489, acc:0.641500\n","epoch:[202], loss:0.050780, acc:0.649500\n","epoch:[203], loss:0.048319, acc:0.640000\n","epoch:[204], loss:0.046018, acc:0.643500\n","epoch:[205], loss:0.045753, acc:0.641000\n","epoch:[206], loss:0.042412, acc:0.652000\n","epoch:[207], loss:0.041020, acc:0.649500\n","epoch:[208], loss:0.038694, acc:0.649500\n","epoch:[209], loss:0.038997, acc:0.652000\n","epoch:[210], loss:0.035826, acc:0.650500\n","epoch:[211], loss:0.036024, acc:0.651000\n","epoch:[212], loss:0.034579, acc:0.649500\n","epoch:[213], loss:0.032805, acc:0.649500\n","epoch:[214], loss:0.032346, acc:0.650000\n","epoch:[215], loss:0.029888, acc:0.647000\n","epoch:[216], loss:0.030354, acc:0.645000\n","epoch:[217], loss:0.028933, acc:0.644500\n","epoch:[218], loss:0.027984, acc:0.645000\n","epoch:[219], loss:0.028034, acc:0.647500\n","epoch:[220], loss:0.026363, acc:0.649000\n","epoch:[221], loss:0.025549, acc:0.648500\n","epoch:[222], loss:0.024986, acc:0.647500\n","epoch:[223], loss:0.028502, acc:0.647000\n","epoch:[224], loss:0.023212, acc:0.653000\n","epoch:[225], loss:0.023289, acc:0.653500\n","epoch:[226], loss:0.022266, acc:0.652000\n","epoch:[227], loss:0.021882, acc:0.651000\n","epoch:[228], loss:0.020730, acc:0.647000\n","epoch:[229], loss:0.021560, acc:0.651000\n","epoch:[230], loss:0.021808, acc:0.653000\n","epoch:[231], loss:0.020838, acc:0.651500\n","epoch:[232], loss:0.019914, acc:0.649000\n","epoch:[233], loss:0.020033, acc:0.652000\n","epoch:[234], loss:0.019651, acc:0.652000\n","epoch:[235], loss:0.019692, acc:0.648000\n","epoch:[236], loss:0.020463, acc:0.653000\n","epoch:[237], loss:0.019135, acc:0.646000\n","epoch:[238], loss:0.019193, acc:0.645000\n","epoch:[239], loss:0.017589, acc:0.644000\n","epoch:[240], loss:0.018914, acc:0.648500\n","epoch:[241], loss:0.018219, acc:0.642000\n"]}],"source":["# client extractor\n","\n","net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","EC_optimizer = optim.Adam(get_params(net, [\"extractor\", \"classifier\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","best_epoch = -1\n","best_acc = 0.\n","epoch = 0\n","while True:\n","    # train\n","    frozen_net(net, [\"extractor\", \"classifier\"], False)\n","    losses, batch = 0., 0\n","    for x, y in client_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        EC_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        EC_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\", \"classifier\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","    if acc > best_acc:\n","        best_acc = acc\n","        best_epoch = epoch\n","        torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/client_extractor.pkl\")\n","        torch.save(net[\"classifier\"].state_dict(), \"./checkpoint/client_classifier.pkl\")\n","    elif epoch >= best_epoch + 50:\n","        break\n","\n","    epoch += 1"]},{"cell_type":"code","source":["# server same extractor\n","\n","net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","E_optimizer = optim.Adam(get_params(net, [\"extractor\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","best_epoch = -1\n","best_acc = 0.\n","epoch = 0\n","while True:\n","    # train\n","    frozen_net(net, [\"extractor\"], False)\n","    losses, batch = 0., 0\n","    for x, y in client_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        E_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        E_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","    if acc > best_acc:\n","        best_acc = acc\n","        best_epoch = epoch\n","        torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/server_same_extractor.pkl\")\n","    elif epoch >= best_epoch + 50:\n","        break\n","\n","    epoch += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-B19J-auOk5w","executionInfo":{"status":"ok","timestamp":1640935585881,"user_tz":-480,"elapsed":451525,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"8a1f3b82-6e4f-4312-d028-2fee24a25ddc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:3.006706, acc:0.099000\n","epoch:[ 1], loss:2.391605, acc:0.189000\n","epoch:[ 2], loss:2.352580, acc:0.189000\n","epoch:[ 3], loss:2.343394, acc:0.189000\n","epoch:[ 4], loss:2.335238, acc:0.189000\n","epoch:[ 5], loss:2.328864, acc:0.189000\n","epoch:[ 6], loss:2.327594, acc:0.189000\n","epoch:[ 7], loss:2.323842, acc:0.189000\n","epoch:[ 8], loss:2.321650, acc:0.189000\n","epoch:[ 9], loss:2.318769, acc:0.189000\n","epoch:[10], loss:2.316730, acc:0.162500\n","epoch:[11], loss:2.313301, acc:0.192000\n","epoch:[12], loss:2.310248, acc:0.188000\n","epoch:[13], loss:2.307963, acc:0.190000\n","epoch:[14], loss:2.301159, acc:0.129500\n","epoch:[15], loss:2.298114, acc:0.166000\n","epoch:[16], loss:2.295166, acc:0.192000\n","epoch:[17], loss:2.294205, acc:0.167500\n","epoch:[18], loss:2.291090, acc:0.186500\n","epoch:[19], loss:2.290131, acc:0.187500\n","epoch:[20], loss:2.279679, acc:0.180500\n","epoch:[21], loss:2.280307, acc:0.175000\n","epoch:[22], loss:2.270947, acc:0.184500\n","epoch:[23], loss:2.259864, acc:0.174500\n","epoch:[24], loss:2.230835, acc:0.210500\n","epoch:[25], loss:2.191669, acc:0.249500\n","epoch:[26], loss:2.092671, acc:0.292000\n","epoch:[27], loss:1.964785, acc:0.340000\n","epoch:[28], loss:1.844017, acc:0.376000\n","epoch:[29], loss:1.711259, acc:0.417500\n","epoch:[30], loss:1.531251, acc:0.458000\n","epoch:[31], loss:1.343081, acc:0.487000\n","epoch:[32], loss:1.202140, acc:0.520000\n","epoch:[33], loss:1.091380, acc:0.531500\n","epoch:[34], loss:1.006098, acc:0.544500\n","epoch:[35], loss:0.935181, acc:0.551000\n","epoch:[36], loss:0.871806, acc:0.559000\n","epoch:[37], loss:0.825916, acc:0.575000\n","epoch:[38], loss:0.764997, acc:0.587500\n","epoch:[39], loss:0.701916, acc:0.600500\n","epoch:[40], loss:0.656045, acc:0.603500\n","epoch:[41], loss:0.615164, acc:0.610000\n","epoch:[42], loss:0.578717, acc:0.613000\n","epoch:[43], loss:0.547350, acc:0.623500\n","epoch:[44], loss:0.513460, acc:0.627500\n","epoch:[45], loss:0.486553, acc:0.627500\n","epoch:[46], loss:0.459340, acc:0.632500\n","epoch:[47], loss:0.435478, acc:0.635500\n","epoch:[48], loss:0.412112, acc:0.634000\n","epoch:[49], loss:0.392711, acc:0.639000\n","epoch:[50], loss:0.373144, acc:0.639500\n","epoch:[51], loss:0.355330, acc:0.639000\n","epoch:[52], loss:0.339173, acc:0.643000\n","epoch:[53], loss:0.323133, acc:0.645500\n","epoch:[54], loss:0.308526, acc:0.647500\n","epoch:[55], loss:0.295143, acc:0.648000\n","epoch:[56], loss:0.282021, acc:0.650500\n","epoch:[57], loss:0.271080, acc:0.646000\n","epoch:[58], loss:0.259659, acc:0.647000\n","epoch:[59], loss:0.249891, acc:0.648000\n","epoch:[60], loss:0.239668, acc:0.648500\n","epoch:[61], loss:0.230691, acc:0.651000\n","epoch:[62], loss:0.221530, acc:0.648500\n","epoch:[63], loss:0.214719, acc:0.649500\n","epoch:[64], loss:0.207087, acc:0.650500\n","epoch:[65], loss:0.200931, acc:0.650000\n","epoch:[66], loss:0.193900, acc:0.652500\n","epoch:[67], loss:0.188477, acc:0.650500\n","epoch:[68], loss:0.181959, acc:0.652500\n","epoch:[69], loss:0.176921, acc:0.653500\n","epoch:[70], loss:0.172247, acc:0.655000\n","epoch:[71], loss:0.167026, acc:0.652500\n","epoch:[72], loss:0.163145, acc:0.655000\n","epoch:[73], loss:0.158297, acc:0.652000\n","epoch:[74], loss:0.154788, acc:0.657500\n","epoch:[75], loss:0.151103, acc:0.654500\n","epoch:[76], loss:0.147674, acc:0.658000\n","epoch:[77], loss:0.144011, acc:0.655000\n","epoch:[78], loss:0.141167, acc:0.657000\n","epoch:[79], loss:0.138081, acc:0.656000\n","epoch:[80], loss:0.135366, acc:0.655500\n","epoch:[81], loss:0.132701, acc:0.656000\n","epoch:[82], loss:0.129954, acc:0.657000\n","epoch:[83], loss:0.128070, acc:0.657000\n","epoch:[84], loss:0.124868, acc:0.654500\n","epoch:[85], loss:0.122375, acc:0.656500\n","epoch:[86], loss:0.120840, acc:0.655000\n","epoch:[87], loss:0.119232, acc:0.655000\n","epoch:[88], loss:0.117103, acc:0.657000\n","epoch:[89], loss:0.114900, acc:0.657000\n","epoch:[90], loss:0.112904, acc:0.656000\n","epoch:[91], loss:0.111815, acc:0.657000\n","epoch:[92], loss:0.110412, acc:0.656000\n","epoch:[93], loss:0.108620, acc:0.656500\n","epoch:[94], loss:0.107195, acc:0.657500\n","epoch:[95], loss:0.106120, acc:0.657000\n","epoch:[96], loss:0.104821, acc:0.657000\n","epoch:[97], loss:0.103559, acc:0.659500\n","epoch:[98], loss:0.102020, acc:0.656500\n","epoch:[99], loss:0.101107, acc:0.656000\n","epoch:[100], loss:0.099702, acc:0.658000\n","epoch:[101], loss:0.098856, acc:0.657000\n","epoch:[102], loss:0.097861, acc:0.655000\n","epoch:[103], loss:0.096833, acc:0.656000\n","epoch:[104], loss:0.096155, acc:0.659500\n","epoch:[105], loss:0.094900, acc:0.656500\n","epoch:[106], loss:0.094186, acc:0.657000\n","epoch:[107], loss:0.093119, acc:0.657000\n","epoch:[108], loss:0.092209, acc:0.656500\n","epoch:[109], loss:0.091454, acc:0.657500\n","epoch:[110], loss:0.090598, acc:0.656500\n","epoch:[111], loss:0.090122, acc:0.656500\n","epoch:[112], loss:0.089380, acc:0.657500\n","epoch:[113], loss:0.088691, acc:0.658500\n","epoch:[114], loss:0.087910, acc:0.658500\n","epoch:[115], loss:0.087390, acc:0.655500\n","epoch:[116], loss:0.087000, acc:0.656500\n","epoch:[117], loss:0.086492, acc:0.658500\n","epoch:[118], loss:0.085752, acc:0.658500\n","epoch:[119], loss:0.085032, acc:0.659500\n","epoch:[120], loss:0.084621, acc:0.659500\n","epoch:[121], loss:0.084071, acc:0.657500\n","epoch:[122], loss:0.083663, acc:0.657000\n","epoch:[123], loss:0.083129, acc:0.656500\n","epoch:[124], loss:0.082727, acc:0.656500\n","epoch:[125], loss:0.082141, acc:0.657500\n","epoch:[126], loss:0.081560, acc:0.655500\n","epoch:[127], loss:0.081450, acc:0.657000\n","epoch:[128], loss:0.080866, acc:0.657000\n","epoch:[129], loss:0.080484, acc:0.663500\n","epoch:[130], loss:0.080292, acc:0.658500\n","epoch:[131], loss:0.079645, acc:0.658000\n","epoch:[132], loss:0.079194, acc:0.658000\n","epoch:[133], loss:0.079144, acc:0.657500\n","epoch:[134], loss:0.078522, acc:0.661500\n","epoch:[135], loss:0.078131, acc:0.659500\n","epoch:[136], loss:0.078027, acc:0.659500\n","epoch:[137], loss:0.077875, acc:0.659500\n","epoch:[138], loss:0.077215, acc:0.659500\n","epoch:[139], loss:0.076890, acc:0.659000\n","epoch:[140], loss:0.076745, acc:0.660000\n","epoch:[141], loss:0.076387, acc:0.661000\n","epoch:[142], loss:0.075986, acc:0.658500\n","epoch:[143], loss:0.075925, acc:0.660500\n","epoch:[144], loss:0.075621, acc:0.660500\n","epoch:[145], loss:0.075095, acc:0.660500\n","epoch:[146], loss:0.074741, acc:0.659000\n","epoch:[147], loss:0.074741, acc:0.659500\n","epoch:[148], loss:0.074477, acc:0.661500\n","epoch:[149], loss:0.074328, acc:0.661000\n","epoch:[150], loss:0.074218, acc:0.661500\n","epoch:[151], loss:0.073798, acc:0.659500\n","epoch:[152], loss:0.073638, acc:0.662500\n","epoch:[153], loss:0.073325, acc:0.659500\n","epoch:[154], loss:0.073172, acc:0.662500\n","epoch:[155], loss:0.072819, acc:0.661000\n","epoch:[156], loss:0.072711, acc:0.661000\n","epoch:[157], loss:0.072493, acc:0.661000\n","epoch:[158], loss:0.072482, acc:0.659500\n","epoch:[159], loss:0.072205, acc:0.661000\n","epoch:[160], loss:0.071987, acc:0.661500\n","epoch:[161], loss:0.071676, acc:0.661500\n","epoch:[162], loss:0.071681, acc:0.664500\n","epoch:[163], loss:0.071381, acc:0.660000\n","epoch:[164], loss:0.071188, acc:0.663000\n","epoch:[165], loss:0.071448, acc:0.661000\n","epoch:[166], loss:0.070878, acc:0.659500\n","epoch:[167], loss:0.070743, acc:0.659500\n","epoch:[168], loss:0.070463, acc:0.659500\n","epoch:[169], loss:0.070479, acc:0.662000\n","epoch:[170], loss:0.070160, acc:0.661500\n","epoch:[171], loss:0.070201, acc:0.662500\n","epoch:[172], loss:0.069985, acc:0.661500\n","epoch:[173], loss:0.069961, acc:0.662000\n","epoch:[174], loss:0.069746, acc:0.661500\n","epoch:[175], loss:0.069389, acc:0.663000\n","epoch:[176], loss:0.069177, acc:0.660000\n","epoch:[177], loss:0.069433, acc:0.664000\n","epoch:[178], loss:0.069299, acc:0.661500\n","epoch:[179], loss:0.068952, acc:0.662500\n","epoch:[180], loss:0.068846, acc:0.661000\n","epoch:[181], loss:0.068595, acc:0.663000\n","epoch:[182], loss:0.068601, acc:0.662500\n","epoch:[183], loss:0.068417, acc:0.660500\n","epoch:[184], loss:0.068296, acc:0.661000\n","epoch:[185], loss:0.068144, acc:0.661500\n","epoch:[186], loss:0.068082, acc:0.662000\n","epoch:[187], loss:0.067962, acc:0.662000\n","epoch:[188], loss:0.067781, acc:0.663000\n","epoch:[189], loss:0.067797, acc:0.663000\n","epoch:[190], loss:0.067696, acc:0.664500\n","epoch:[191], loss:0.067728, acc:0.659500\n","epoch:[192], loss:0.067433, acc:0.659000\n","epoch:[193], loss:0.067325, acc:0.664000\n","epoch:[194], loss:0.067109, acc:0.658500\n","epoch:[195], loss:0.066830, acc:0.660000\n","epoch:[196], loss:0.066798, acc:0.660000\n","epoch:[197], loss:0.066931, acc:0.662500\n","epoch:[198], loss:0.066653, acc:0.662000\n","epoch:[199], loss:0.066662, acc:0.662500\n","epoch:[200], loss:0.066568, acc:0.662000\n","epoch:[201], loss:0.066511, acc:0.663000\n","epoch:[202], loss:0.066456, acc:0.660000\n","epoch:[203], loss:0.066255, acc:0.661000\n","epoch:[204], loss:0.066268, acc:0.660000\n","epoch:[205], loss:0.066109, acc:0.661500\n","epoch:[206], loss:0.066022, acc:0.660000\n","epoch:[207], loss:0.065960, acc:0.661500\n","epoch:[208], loss:0.066159, acc:0.662500\n","epoch:[209], loss:0.065882, acc:0.660000\n","epoch:[210], loss:0.065495, acc:0.662000\n","epoch:[211], loss:0.065820, acc:0.661000\n","epoch:[212], loss:0.065686, acc:0.660500\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":480033,"status":"ok","timestamp":1640936065905,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"},"user_tz":-480},"id":"Gq_glU2S1myq","outputId":"850140bb-d5fb-48c1-c29e-12f5aac7ac43"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:2.794132, acc:0.158000\n","epoch:[ 1], loss:2.404628, acc:0.189000\n","epoch:[ 2], loss:2.365584, acc:0.167500\n","epoch:[ 3], loss:2.360456, acc:0.189000\n","epoch:[ 4], loss:2.350790, acc:0.189500\n","epoch:[ 5], loss:2.343019, acc:0.189000\n","epoch:[ 6], loss:2.342360, acc:0.187500\n","epoch:[ 7], loss:2.340567, acc:0.189000\n","epoch:[ 8], loss:2.333431, acc:0.189000\n","epoch:[ 9], loss:2.332549, acc:0.182500\n","epoch:[10], loss:2.329169, acc:0.188500\n","epoch:[11], loss:2.327687, acc:0.139000\n","epoch:[12], loss:2.328532, acc:0.189000\n","epoch:[13], loss:2.325533, acc:0.188500\n","epoch:[14], loss:2.324145, acc:0.187500\n","epoch:[15], loss:2.322481, acc:0.188500\n","epoch:[16], loss:2.318382, acc:0.184500\n","epoch:[17], loss:2.314682, acc:0.192000\n","epoch:[18], loss:2.311018, acc:0.186500\n","epoch:[19], loss:2.307302, acc:0.180000\n","epoch:[20], loss:2.309827, acc:0.186000\n","epoch:[21], loss:2.308529, acc:0.140000\n","epoch:[22], loss:2.307464, acc:0.189000\n","epoch:[23], loss:2.305111, acc:0.184000\n","epoch:[24], loss:2.299270, acc:0.189500\n","epoch:[25], loss:2.298970, acc:0.135500\n","epoch:[26], loss:2.297595, acc:0.185500\n","epoch:[27], loss:2.298368, acc:0.188000\n","epoch:[28], loss:2.296026, acc:0.181500\n","epoch:[29], loss:2.289573, acc:0.189000\n","epoch:[30], loss:2.293336, acc:0.183000\n","epoch:[31], loss:2.292035, acc:0.183500\n","epoch:[32], loss:2.287972, acc:0.190000\n","epoch:[33], loss:2.286616, acc:0.169000\n","epoch:[34], loss:2.282080, acc:0.150500\n","epoch:[35], loss:2.283566, acc:0.179000\n","epoch:[36], loss:2.280086, acc:0.196500\n","epoch:[37], loss:2.274116, acc:0.197500\n","epoch:[38], loss:2.275829, acc:0.203500\n","epoch:[39], loss:2.267515, acc:0.182500\n","epoch:[40], loss:2.258065, acc:0.204500\n","epoch:[41], loss:2.239634, acc:0.215500\n","epoch:[42], loss:2.221555, acc:0.217000\n","epoch:[43], loss:2.174880, acc:0.251000\n","epoch:[44], loss:2.098718, acc:0.269000\n","epoch:[45], loss:2.005531, acc:0.308000\n","epoch:[46], loss:1.920021, acc:0.331500\n","epoch:[47], loss:1.802552, acc:0.383000\n","epoch:[48], loss:1.674812, acc:0.428000\n","epoch:[49], loss:1.576332, acc:0.457000\n","epoch:[50], loss:1.494734, acc:0.494000\n","epoch:[51], loss:1.429483, acc:0.511000\n","epoch:[52], loss:1.383606, acc:0.516500\n","epoch:[53], loss:1.336286, acc:0.545000\n","epoch:[54], loss:1.296069, acc:0.557000\n","epoch:[55], loss:1.268475, acc:0.564500\n","epoch:[56], loss:1.238758, acc:0.567000\n","epoch:[57], loss:1.216395, acc:0.583500\n","epoch:[58], loss:1.201633, acc:0.584000\n","epoch:[59], loss:1.182762, acc:0.590500\n","epoch:[60], loss:1.169792, acc:0.592000\n","epoch:[61], loss:1.159955, acc:0.595500\n","epoch:[62], loss:1.150781, acc:0.598000\n","epoch:[63], loss:1.140809, acc:0.601500\n","epoch:[64], loss:1.131823, acc:0.603500\n","epoch:[65], loss:1.124436, acc:0.603500\n","epoch:[66], loss:1.115408, acc:0.603500\n","epoch:[67], loss:1.106646, acc:0.607500\n","epoch:[68], loss:1.103263, acc:0.609000\n","epoch:[69], loss:1.097449, acc:0.615500\n","epoch:[70], loss:1.093267, acc:0.617000\n","epoch:[71], loss:1.087407, acc:0.619000\n","epoch:[72], loss:1.082414, acc:0.613000\n","epoch:[73], loss:1.076247, acc:0.620000\n","epoch:[74], loss:1.074379, acc:0.614000\n","epoch:[75], loss:1.071709, acc:0.618000\n","epoch:[76], loss:1.064605, acc:0.615000\n","epoch:[77], loss:1.066709, acc:0.623000\n","epoch:[78], loss:1.053822, acc:0.621000\n","epoch:[79], loss:1.059587, acc:0.620500\n","epoch:[80], loss:1.052999, acc:0.621000\n","epoch:[81], loss:1.049949, acc:0.621500\n","epoch:[82], loss:1.044794, acc:0.618500\n","epoch:[83], loss:1.045117, acc:0.625500\n","epoch:[84], loss:1.040615, acc:0.622000\n","epoch:[85], loss:1.040474, acc:0.623000\n","epoch:[86], loss:1.035167, acc:0.620500\n","epoch:[87], loss:1.033377, acc:0.628000\n","epoch:[88], loss:1.029912, acc:0.625000\n","epoch:[89], loss:1.025396, acc:0.623000\n","epoch:[90], loss:1.024098, acc:0.632000\n","epoch:[91], loss:1.020357, acc:0.627500\n","epoch:[92], loss:1.011524, acc:0.621000\n","epoch:[93], loss:1.017244, acc:0.621500\n","epoch:[94], loss:1.014711, acc:0.629500\n","epoch:[95], loss:1.009964, acc:0.629000\n","epoch:[96], loss:1.007736, acc:0.631000\n","epoch:[97], loss:1.002828, acc:0.628000\n","epoch:[98], loss:1.003187, acc:0.633500\n","epoch:[99], loss:0.999870, acc:0.636500\n","epoch:[100], loss:0.999116, acc:0.633500\n","epoch:[101], loss:0.998691, acc:0.626500\n","epoch:[102], loss:0.993782, acc:0.632500\n","epoch:[103], loss:0.987500, acc:0.630500\n","epoch:[104], loss:0.990715, acc:0.629500\n","epoch:[105], loss:0.991771, acc:0.631500\n","epoch:[106], loss:0.985898, acc:0.636500\n","epoch:[107], loss:0.985535, acc:0.636000\n","epoch:[108], loss:0.985097, acc:0.632500\n","epoch:[109], loss:0.980594, acc:0.635500\n","epoch:[110], loss:0.979907, acc:0.631500\n","epoch:[111], loss:0.973630, acc:0.631000\n","epoch:[112], loss:0.974596, acc:0.636000\n","epoch:[113], loss:0.976173, acc:0.628500\n","epoch:[114], loss:0.971818, acc:0.629000\n","epoch:[115], loss:0.971829, acc:0.633500\n","epoch:[116], loss:0.968940, acc:0.637500\n","epoch:[117], loss:0.968619, acc:0.634000\n","epoch:[118], loss:0.965533, acc:0.638500\n","epoch:[119], loss:0.962235, acc:0.640500\n","epoch:[120], loss:0.960484, acc:0.630000\n","epoch:[121], loss:0.957192, acc:0.630500\n","epoch:[122], loss:0.957250, acc:0.635500\n","epoch:[123], loss:0.959890, acc:0.634500\n","epoch:[124], loss:0.957454, acc:0.641000\n","epoch:[125], loss:0.954151, acc:0.633000\n","epoch:[126], loss:0.952477, acc:0.640000\n","epoch:[127], loss:0.949318, acc:0.631000\n","epoch:[128], loss:0.952552, acc:0.637500\n","epoch:[129], loss:0.948313, acc:0.636500\n","epoch:[130], loss:0.945656, acc:0.639000\n","epoch:[131], loss:0.946800, acc:0.636500\n","epoch:[132], loss:0.943764, acc:0.635000\n","epoch:[133], loss:0.944554, acc:0.642500\n","epoch:[134], loss:0.942643, acc:0.641000\n","epoch:[135], loss:0.940082, acc:0.634500\n","epoch:[136], loss:0.940531, acc:0.640000\n","epoch:[137], loss:0.936449, acc:0.640500\n","epoch:[138], loss:0.936861, acc:0.643000\n","epoch:[139], loss:0.934918, acc:0.642500\n","epoch:[140], loss:0.931251, acc:0.638500\n","epoch:[141], loss:0.932033, acc:0.635000\n","epoch:[142], loss:0.928221, acc:0.639500\n","epoch:[143], loss:0.929447, acc:0.636500\n","epoch:[144], loss:0.928814, acc:0.643000\n","epoch:[145], loss:0.925676, acc:0.639500\n","epoch:[146], loss:0.925562, acc:0.640500\n","epoch:[147], loss:0.923927, acc:0.642500\n","epoch:[148], loss:0.922204, acc:0.636500\n","epoch:[149], loss:0.923147, acc:0.635000\n","epoch:[150], loss:0.919318, acc:0.636000\n","epoch:[151], loss:0.919580, acc:0.637500\n","epoch:[152], loss:0.915926, acc:0.640000\n","epoch:[153], loss:0.916961, acc:0.642000\n","epoch:[154], loss:0.916393, acc:0.639000\n","epoch:[155], loss:0.915241, acc:0.644000\n","epoch:[156], loss:0.912129, acc:0.641000\n","epoch:[157], loss:0.913229, acc:0.644000\n","epoch:[158], loss:0.909774, acc:0.637000\n","epoch:[159], loss:0.908789, acc:0.642000\n","epoch:[160], loss:0.906053, acc:0.637500\n","epoch:[161], loss:0.907429, acc:0.636000\n","epoch:[162], loss:0.905617, acc:0.635500\n","epoch:[163], loss:0.905369, acc:0.641000\n","epoch:[164], loss:0.907014, acc:0.638000\n","epoch:[165], loss:0.903367, acc:0.642500\n","epoch:[166], loss:0.903434, acc:0.642000\n","epoch:[167], loss:0.897962, acc:0.635500\n","epoch:[168], loss:0.899646, acc:0.639500\n","epoch:[169], loss:0.899995, acc:0.632000\n","epoch:[170], loss:0.900388, acc:0.640500\n","epoch:[171], loss:0.899350, acc:0.641000\n","epoch:[172], loss:0.894461, acc:0.641500\n","epoch:[173], loss:0.895898, acc:0.641500\n","epoch:[174], loss:0.893485, acc:0.639000\n","epoch:[175], loss:0.893231, acc:0.638500\n","epoch:[176], loss:0.890156, acc:0.644500\n","epoch:[177], loss:0.893463, acc:0.638500\n","epoch:[178], loss:0.889360, acc:0.641000\n","epoch:[179], loss:0.888825, acc:0.639500\n","epoch:[180], loss:0.889657, acc:0.639000\n","epoch:[181], loss:0.887996, acc:0.637000\n","epoch:[182], loss:0.888502, acc:0.631500\n","epoch:[183], loss:0.883424, acc:0.640500\n","epoch:[184], loss:0.884967, acc:0.639000\n","epoch:[185], loss:0.885678, acc:0.644500\n","epoch:[186], loss:0.880884, acc:0.638500\n","epoch:[187], loss:0.885200, acc:0.638000\n","epoch:[188], loss:0.880300, acc:0.638500\n","epoch:[189], loss:0.875704, acc:0.640500\n","epoch:[190], loss:0.883410, acc:0.642000\n","epoch:[191], loss:0.876151, acc:0.639000\n","epoch:[192], loss:0.877517, acc:0.630000\n","epoch:[193], loss:0.877770, acc:0.642000\n","epoch:[194], loss:0.874882, acc:0.640500\n","epoch:[195], loss:0.871838, acc:0.639500\n","epoch:[196], loss:0.874431, acc:0.634000\n","epoch:[197], loss:0.872210, acc:0.639000\n","epoch:[198], loss:0.871524, acc:0.635000\n","epoch:[199], loss:0.872437, acc:0.634500\n","epoch:[200], loss:0.873995, acc:0.639500\n","epoch:[201], loss:0.867382, acc:0.635000\n","epoch:[202], loss:0.866663, acc:0.637000\n","epoch:[203], loss:0.869998, acc:0.640000\n","epoch:[204], loss:0.869637, acc:0.641000\n","epoch:[205], loss:0.862974, acc:0.634500\n","epoch:[206], loss:0.866621, acc:0.636000\n","epoch:[207], loss:0.865141, acc:0.640500\n","epoch:[208], loss:0.862181, acc:0.642000\n","epoch:[209], loss:0.865185, acc:0.634500\n","epoch:[210], loss:0.864066, acc:0.640500\n","epoch:[211], loss:0.861880, acc:0.637500\n","epoch:[212], loss:0.859115, acc:0.634500\n","epoch:[213], loss:0.860338, acc:0.632000\n","epoch:[214], loss:0.861909, acc:0.637500\n","epoch:[215], loss:0.857041, acc:0.633500\n","epoch:[216], loss:0.860158, acc:0.639000\n","epoch:[217], loss:0.856010, acc:0.637000\n","epoch:[218], loss:0.854184, acc:0.640000\n","epoch:[219], loss:0.853302, acc:0.635000\n","epoch:[220], loss:0.853501, acc:0.632500\n","epoch:[221], loss:0.852616, acc:0.637500\n","epoch:[222], loss:0.852976, acc:0.636000\n","epoch:[223], loss:0.852488, acc:0.633000\n","epoch:[224], loss:0.851410, acc:0.640000\n","epoch:[225], loss:0.847812, acc:0.640500\n","epoch:[226], loss:0.849375, acc:0.633500\n"]}],"source":["# server iid extractor\n","\n","net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","E_optimizer = optim.Adam(get_params(net, [\"extractor\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","best_epoch = -1\n","best_acc = 0.\n","epoch = 0\n","while True:\n","    # train\n","    frozen_net(net, [\"extractor\"], False)\n","    losses, batch = 0., 0\n","    for x, y in server_iid_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        E_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        E_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","    if acc > best_acc:\n","        best_acc = acc\n","        best_epoch = epoch\n","        torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/server_iid_extractor.pkl\")\n","    elif epoch >= best_epoch + 50:\n","        break\n","\n","    epoch += 1"]},{"cell_type":"markdown","metadata":{"id":"7Xmr0Ufk_oCM"},"source":["**server niid train**"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520730,"status":"ok","timestamp":1640936586626,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"},"user_tz":-480},"id":"5Yvtf4Il_qRc","outputId":"5bbd471e-6698-4911-e8fc-b211ccac5fba"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:2.879653, acc:0.081000\n","epoch:[ 1], loss:2.092979, acc:0.110000\n","epoch:[ 2], loss:1.682676, acc:0.126000\n","epoch:[ 3], loss:1.443160, acc:0.148000\n","epoch:[ 4], loss:1.311270, acc:0.153000\n","epoch:[ 5], loss:1.226309, acc:0.157500\n","epoch:[ 6], loss:1.168398, acc:0.162500\n","epoch:[ 7], loss:1.126386, acc:0.167500\n","epoch:[ 8], loss:1.088700, acc:0.166500\n","epoch:[ 9], loss:1.057167, acc:0.174000\n","epoch:[10], loss:1.028666, acc:0.173500\n","epoch:[11], loss:1.004595, acc:0.175000\n","epoch:[12], loss:0.979222, acc:0.177500\n","epoch:[13], loss:0.954804, acc:0.183500\n","epoch:[14], loss:0.937470, acc:0.180500\n","epoch:[15], loss:0.913939, acc:0.180500\n","epoch:[16], loss:0.899343, acc:0.183500\n","epoch:[17], loss:0.886241, acc:0.188500\n","epoch:[18], loss:0.864898, acc:0.184000\n","epoch:[19], loss:0.858203, acc:0.188000\n","epoch:[20], loss:0.843205, acc:0.186500\n","epoch:[21], loss:0.831610, acc:0.184000\n","epoch:[22], loss:0.816509, acc:0.191500\n","epoch:[23], loss:0.808140, acc:0.188500\n","epoch:[24], loss:0.799057, acc:0.188000\n","epoch:[25], loss:0.789995, acc:0.188500\n","epoch:[26], loss:0.781497, acc:0.191000\n","epoch:[27], loss:0.771680, acc:0.196500\n","epoch:[28], loss:0.760990, acc:0.193000\n","epoch:[29], loss:0.755036, acc:0.190000\n","epoch:[30], loss:0.746267, acc:0.192500\n","epoch:[31], loss:0.739977, acc:0.189500\n","epoch:[32], loss:0.732124, acc:0.189500\n","epoch:[33], loss:0.726291, acc:0.186000\n","epoch:[34], loss:0.712826, acc:0.193500\n","epoch:[35], loss:0.713066, acc:0.191000\n","epoch:[36], loss:0.706094, acc:0.188500\n","epoch:[37], loss:0.698838, acc:0.189500\n","epoch:[38], loss:0.696739, acc:0.193000\n","epoch:[39], loss:0.687555, acc:0.191500\n","epoch:[40], loss:0.682623, acc:0.191500\n","epoch:[41], loss:0.674362, acc:0.193000\n","epoch:[42], loss:0.672358, acc:0.195500\n","epoch:[43], loss:0.666017, acc:0.186000\n","epoch:[44], loss:0.659695, acc:0.192500\n","epoch:[45], loss:0.657131, acc:0.195000\n","epoch:[46], loss:0.653083, acc:0.194500\n","epoch:[47], loss:0.646426, acc:0.199500\n","epoch:[48], loss:0.640361, acc:0.194000\n","epoch:[49], loss:0.637003, acc:0.193500\n","epoch:[50], loss:0.633773, acc:0.192500\n","epoch:[51], loss:0.628460, acc:0.197000\n","epoch:[52], loss:0.621914, acc:0.199500\n","epoch:[53], loss:0.619017, acc:0.187000\n","epoch:[54], loss:0.613601, acc:0.200000\n","epoch:[55], loss:0.610048, acc:0.192500\n","epoch:[56], loss:0.603536, acc:0.199500\n","epoch:[57], loss:0.599924, acc:0.187500\n","epoch:[58], loss:0.596104, acc:0.197500\n","epoch:[59], loss:0.594357, acc:0.194000\n","epoch:[60], loss:0.587715, acc:0.195500\n","epoch:[61], loss:0.585649, acc:0.198000\n","epoch:[62], loss:0.578426, acc:0.197000\n","epoch:[63], loss:0.578574, acc:0.199000\n","epoch:[64], loss:0.572596, acc:0.205000\n","epoch:[65], loss:0.569124, acc:0.202500\n","epoch:[66], loss:0.566591, acc:0.201000\n","epoch:[67], loss:0.563455, acc:0.204000\n","epoch:[68], loss:0.558305, acc:0.203500\n","epoch:[69], loss:0.554552, acc:0.204500\n","epoch:[70], loss:0.551481, acc:0.205500\n","epoch:[71], loss:0.547370, acc:0.209000\n","epoch:[72], loss:0.544851, acc:0.207000\n","epoch:[73], loss:0.541898, acc:0.200000\n","epoch:[74], loss:0.538350, acc:0.205500\n","epoch:[75], loss:0.535647, acc:0.207500\n","epoch:[76], loss:0.531574, acc:0.204500\n","epoch:[77], loss:0.529917, acc:0.203500\n","epoch:[78], loss:0.526636, acc:0.210500\n","epoch:[79], loss:0.523617, acc:0.209500\n","epoch:[80], loss:0.520105, acc:0.204500\n","epoch:[81], loss:0.517617, acc:0.203500\n","epoch:[82], loss:0.511907, acc:0.201500\n","epoch:[83], loss:0.510982, acc:0.210000\n","epoch:[84], loss:0.510515, acc:0.212000\n","epoch:[85], loss:0.505960, acc:0.213500\n","epoch:[86], loss:0.502624, acc:0.209500\n","epoch:[87], loss:0.500048, acc:0.210000\n","epoch:[88], loss:0.500453, acc:0.211000\n","epoch:[89], loss:0.494977, acc:0.208000\n","epoch:[90], loss:0.493873, acc:0.208500\n","epoch:[91], loss:0.490135, acc:0.209000\n","epoch:[92], loss:0.486644, acc:0.212500\n","epoch:[93], loss:0.487131, acc:0.208000\n","epoch:[94], loss:0.484996, acc:0.210000\n","epoch:[95], loss:0.481848, acc:0.207500\n","epoch:[96], loss:0.479048, acc:0.207000\n","epoch:[97], loss:0.476710, acc:0.208500\n","epoch:[98], loss:0.476887, acc:0.213000\n","epoch:[99], loss:0.473969, acc:0.210000\n","epoch:[100], loss:0.471923, acc:0.209500\n","epoch:[101], loss:0.468678, acc:0.208000\n","epoch:[102], loss:0.468161, acc:0.211500\n","epoch:[103], loss:0.465583, acc:0.214000\n","epoch:[104], loss:0.462920, acc:0.210500\n","epoch:[105], loss:0.460377, acc:0.213500\n","epoch:[106], loss:0.457590, acc:0.211500\n","epoch:[107], loss:0.457233, acc:0.209500\n","epoch:[108], loss:0.455743, acc:0.211000\n","epoch:[109], loss:0.451580, acc:0.213000\n","epoch:[110], loss:0.450636, acc:0.217500\n","epoch:[111], loss:0.451153, acc:0.214500\n","epoch:[112], loss:0.447096, acc:0.211000\n","epoch:[113], loss:0.443656, acc:0.212500\n","epoch:[114], loss:0.444372, acc:0.215500\n","epoch:[115], loss:0.444368, acc:0.215000\n","epoch:[116], loss:0.438183, acc:0.205500\n","epoch:[117], loss:0.438511, acc:0.211000\n","epoch:[118], loss:0.437508, acc:0.217000\n","epoch:[119], loss:0.434551, acc:0.214000\n","epoch:[120], loss:0.433393, acc:0.212000\n","epoch:[121], loss:0.433910, acc:0.218000\n","epoch:[122], loss:0.429783, acc:0.212000\n","epoch:[123], loss:0.429054, acc:0.216000\n","epoch:[124], loss:0.429090, acc:0.210500\n","epoch:[125], loss:0.425526, acc:0.217500\n","epoch:[126], loss:0.423828, acc:0.215000\n","epoch:[127], loss:0.424186, acc:0.212000\n","epoch:[128], loss:0.422092, acc:0.213000\n","epoch:[129], loss:0.419682, acc:0.217000\n","epoch:[130], loss:0.417163, acc:0.214000\n","epoch:[131], loss:0.418175, acc:0.215500\n","epoch:[132], loss:0.416069, acc:0.215500\n","epoch:[133], loss:0.416288, acc:0.215000\n","epoch:[134], loss:0.413790, acc:0.216000\n","epoch:[135], loss:0.412766, acc:0.217000\n","epoch:[136], loss:0.409816, acc:0.218000\n","epoch:[137], loss:0.410069, acc:0.216000\n","epoch:[138], loss:0.408977, acc:0.213500\n","epoch:[139], loss:0.407451, acc:0.215500\n","epoch:[140], loss:0.406348, acc:0.222500\n","epoch:[141], loss:0.404277, acc:0.214500\n","epoch:[142], loss:0.403606, acc:0.217000\n","epoch:[143], loss:0.402364, acc:0.219000\n","epoch:[144], loss:0.401887, acc:0.217500\n","epoch:[145], loss:0.400101, acc:0.219000\n","epoch:[146], loss:0.398426, acc:0.221000\n","epoch:[147], loss:0.397959, acc:0.220000\n","epoch:[148], loss:0.396127, acc:0.225500\n","epoch:[149], loss:0.396220, acc:0.219500\n","epoch:[150], loss:0.395237, acc:0.215000\n","epoch:[151], loss:0.393770, acc:0.216000\n","epoch:[152], loss:0.393963, acc:0.217500\n","epoch:[153], loss:0.390153, acc:0.220500\n","epoch:[154], loss:0.389859, acc:0.215500\n","epoch:[155], loss:0.389324, acc:0.217500\n","epoch:[156], loss:0.388311, acc:0.220000\n","epoch:[157], loss:0.386743, acc:0.220000\n","epoch:[158], loss:0.386572, acc:0.216000\n","epoch:[159], loss:0.382393, acc:0.218500\n","epoch:[160], loss:0.383676, acc:0.220000\n","epoch:[161], loss:0.382164, acc:0.223500\n","epoch:[162], loss:0.382324, acc:0.222000\n","epoch:[163], loss:0.380336, acc:0.214000\n","epoch:[164], loss:0.379230, acc:0.217500\n","epoch:[165], loss:0.380076, acc:0.220500\n","epoch:[166], loss:0.377571, acc:0.217500\n","epoch:[167], loss:0.377126, acc:0.222000\n","epoch:[168], loss:0.376980, acc:0.222000\n","epoch:[169], loss:0.376096, acc:0.221000\n","epoch:[170], loss:0.374352, acc:0.218500\n","epoch:[171], loss:0.374043, acc:0.217000\n","epoch:[172], loss:0.371790, acc:0.225000\n","epoch:[173], loss:0.371665, acc:0.224500\n","epoch:[174], loss:0.371350, acc:0.216500\n","epoch:[175], loss:0.371005, acc:0.217500\n","epoch:[176], loss:0.369274, acc:0.223000\n","epoch:[177], loss:0.368778, acc:0.218000\n","epoch:[178], loss:0.367142, acc:0.223500\n","epoch:[179], loss:0.365807, acc:0.223500\n","epoch:[180], loss:0.366245, acc:0.221500\n","epoch:[181], loss:0.363700, acc:0.226500\n","epoch:[182], loss:0.364458, acc:0.223500\n","epoch:[183], loss:0.363006, acc:0.219000\n","epoch:[184], loss:0.362087, acc:0.221500\n","epoch:[185], loss:0.361311, acc:0.219000\n","epoch:[186], loss:0.361582, acc:0.218500\n","epoch:[187], loss:0.359423, acc:0.219000\n","epoch:[188], loss:0.360787, acc:0.220000\n","epoch:[189], loss:0.358161, acc:0.220500\n","epoch:[190], loss:0.358019, acc:0.219000\n","epoch:[191], loss:0.357580, acc:0.218000\n","epoch:[192], loss:0.357888, acc:0.221500\n","epoch:[193], loss:0.356085, acc:0.217000\n","epoch:[194], loss:0.354035, acc:0.222500\n","epoch:[195], loss:0.353991, acc:0.222000\n","epoch:[196], loss:0.354044, acc:0.223500\n","epoch:[197], loss:0.354152, acc:0.220000\n","epoch:[198], loss:0.351696, acc:0.219500\n","epoch:[199], loss:0.353533, acc:0.221000\n","epoch:[200], loss:0.352117, acc:0.221000\n","epoch:[201], loss:0.348702, acc:0.220500\n","epoch:[202], loss:0.349697, acc:0.226500\n","epoch:[203], loss:0.349865, acc:0.220000\n","epoch:[204], loss:0.347277, acc:0.220000\n","epoch:[205], loss:0.347335, acc:0.222500\n","epoch:[206], loss:0.347010, acc:0.221000\n","epoch:[207], loss:0.345853, acc:0.221000\n","epoch:[208], loss:0.344442, acc:0.224500\n","epoch:[209], loss:0.343336, acc:0.228500\n","epoch:[210], loss:0.342785, acc:0.222500\n","epoch:[211], loss:0.342311, acc:0.217500\n","epoch:[212], loss:0.342447, acc:0.223500\n","epoch:[213], loss:0.342468, acc:0.223000\n","epoch:[214], loss:0.340907, acc:0.222000\n","epoch:[215], loss:0.339953, acc:0.221000\n","epoch:[216], loss:0.339399, acc:0.224000\n","epoch:[217], loss:0.338329, acc:0.218000\n","epoch:[218], loss:0.338568, acc:0.220500\n","epoch:[219], loss:0.337740, acc:0.222500\n","epoch:[220], loss:0.337968, acc:0.221500\n","epoch:[221], loss:0.337006, acc:0.223500\n","epoch:[222], loss:0.336710, acc:0.223000\n","epoch:[223], loss:0.334540, acc:0.220500\n","epoch:[224], loss:0.335812, acc:0.220500\n","epoch:[225], loss:0.333769, acc:0.219000\n","epoch:[226], loss:0.334236, acc:0.224000\n","epoch:[227], loss:0.333816, acc:0.223500\n","epoch:[228], loss:0.332266, acc:0.224000\n","epoch:[229], loss:0.332699, acc:0.221500\n","epoch:[230], loss:0.332828, acc:0.227500\n","epoch:[231], loss:0.330493, acc:0.222500\n","epoch:[232], loss:0.330131, acc:0.226000\n","epoch:[233], loss:0.330467, acc:0.222000\n","epoch:[234], loss:0.327512, acc:0.224500\n","epoch:[235], loss:0.327610, acc:0.223000\n","epoch:[236], loss:0.326874, acc:0.219000\n","epoch:[237], loss:0.327862, acc:0.224500\n","epoch:[238], loss:0.328248, acc:0.225000\n","epoch:[239], loss:0.325511, acc:0.223000\n","epoch:[240], loss:0.325499, acc:0.222500\n","epoch:[241], loss:0.325645, acc:0.221000\n","epoch:[242], loss:0.324100, acc:0.222000\n","epoch:[243], loss:0.322560, acc:0.220500\n","epoch:[244], loss:0.323682, acc:0.223500\n","epoch:[245], loss:0.323462, acc:0.223500\n","epoch:[246], loss:0.322491, acc:0.220500\n","epoch:[247], loss:0.322467, acc:0.223500\n","epoch:[248], loss:0.320893, acc:0.222000\n","epoch:[249], loss:0.320501, acc:0.221000\n","epoch:[250], loss:0.318890, acc:0.223000\n","epoch:[251], loss:0.319321, acc:0.222000\n","epoch:[252], loss:0.320392, acc:0.222000\n","epoch:[253], loss:0.317657, acc:0.224500\n","epoch:[254], loss:0.317360, acc:0.221000\n","epoch:[255], loss:0.318052, acc:0.222500\n","epoch:[256], loss:0.317218, acc:0.220000\n","epoch:[257], loss:0.315553, acc:0.221500\n","epoch:[258], loss:0.315690, acc:0.223500\n","epoch:[259], loss:0.314989, acc:0.223500\n"]}],"source":["# server niid extractor\n","\n","net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","E_optimizer = optim.Adam(get_params(net, [\"extractor\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","best_epoch = -1\n","best_acc = 0.\n","epoch = 0\n","while True:\n","    # train\n","    frozen_net(net, [\"extractor\"], False)\n","    losses, batch = 0., 0\n","    for x, y in server_niid_trainloader:\n","        x = x.to(device)\n","        x = torch.cat([x,x,x], 1)\n","        y = y.to(device)\n","        \n","        E_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        E_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","    if acc > best_acc:\n","        best_acc = acc\n","        best_epoch = epoch\n","        torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/server_niid_extractor.pkl\")\n","    elif epoch >= best_epoch + 50:\n","        break\n","\n","    epoch += 1"]},{"cell_type":"markdown","metadata":{"id":"xsQF1cMbQ8_D"},"source":["**GAN train**"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5109760,"status":"error","timestamp":1640949045886,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"},"user_tz":-480},"id":"0mcjqTXxQ_Y9","outputId":"917e1be3-956a-4bad-ae8f-6bfc3dca2343"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], D_loss:1.314085, G_loss:1.313853, acc:0.200000\n","epoch:[ 1], D_loss:0.959940, G_loss:2.463357, acc:0.063500\n","epoch:[ 2], D_loss:0.543719, G_loss:3.605326, acc:0.063500\n","epoch:[ 3], D_loss:0.505526, G_loss:4.241839, acc:0.143000\n","epoch:[ 4], D_loss:0.482702, G_loss:3.976800, acc:0.063500\n","epoch:[ 5], D_loss:0.448461, G_loss:4.144906, acc:0.195000\n","epoch:[ 6], D_loss:0.565051, G_loss:3.474952, acc:0.220500\n","epoch:[ 7], D_loss:0.556123, G_loss:2.887314, acc:0.125000\n","epoch:[ 8], D_loss:0.628947, G_loss:2.548627, acc:0.126500\n","epoch:[ 9], D_loss:0.635142, G_loss:2.658586, acc:0.122500\n","epoch:[10], D_loss:0.688017, G_loss:2.503990, acc:0.186000\n","epoch:[11], D_loss:0.711284, G_loss:2.500637, acc:0.195000\n","epoch:[12], D_loss:0.818079, G_loss:2.389879, acc:0.216500\n","epoch:[13], D_loss:0.825529, G_loss:2.439187, acc:0.236500\n","epoch:[14], D_loss:0.865326, G_loss:2.225194, acc:0.244000\n","epoch:[15], D_loss:0.953219, G_loss:2.076803, acc:0.283000\n","epoch:[16], D_loss:0.987765, G_loss:2.011696, acc:0.273500\n","epoch:[17], D_loss:1.035263, G_loss:1.901048, acc:0.264500\n","epoch:[18], D_loss:1.017820, G_loss:1.842115, acc:0.337000\n","epoch:[19], D_loss:1.102356, G_loss:1.706398, acc:0.370500\n","epoch:[20], D_loss:1.131675, G_loss:1.608557, acc:0.411000\n","epoch:[21], D_loss:1.128347, G_loss:1.537118, acc:0.399500\n","epoch:[22], D_loss:1.177865, G_loss:1.458496, acc:0.488000\n","epoch:[23], D_loss:1.176168, G_loss:1.490357, acc:0.413000\n","epoch:[24], D_loss:1.205866, G_loss:1.358589, acc:0.501000\n","epoch:[25], D_loss:1.184111, G_loss:1.427307, acc:0.460000\n","epoch:[26], D_loss:1.215689, G_loss:1.377425, acc:0.493000\n","epoch:[27], D_loss:1.189434, G_loss:1.339144, acc:0.499000\n","epoch:[28], D_loss:1.214896, G_loss:1.342362, acc:0.531000\n","epoch:[29], D_loss:1.219105, G_loss:1.307105, acc:0.532000\n","epoch:[30], D_loss:1.241585, G_loss:1.266945, acc:0.527000\n","epoch:[31], D_loss:1.212576, G_loss:1.254579, acc:0.554500\n","epoch:[32], D_loss:1.219924, G_loss:1.271806, acc:0.525000\n","epoch:[33], D_loss:1.214098, G_loss:1.302371, acc:0.564500\n","epoch:[34], D_loss:1.223629, G_loss:1.300393, acc:0.546000\n","epoch:[35], D_loss:1.206962, G_loss:1.252761, acc:0.594000\n","epoch:[36], D_loss:1.207487, G_loss:1.282449, acc:0.560500\n","epoch:[37], D_loss:1.228955, G_loss:1.254246, acc:0.595000\n","epoch:[38], D_loss:1.202080, G_loss:1.304858, acc:0.626000\n","epoch:[39], D_loss:1.199569, G_loss:1.262418, acc:0.561000\n","epoch:[40], D_loss:1.192660, G_loss:1.272670, acc:0.601000\n","epoch:[41], D_loss:1.184714, G_loss:1.302132, acc:0.569500\n","epoch:[42], D_loss:1.192218, G_loss:1.311688, acc:0.578000\n","epoch:[43], D_loss:1.179602, G_loss:1.305322, acc:0.666500\n","epoch:[44], D_loss:1.180968, G_loss:1.302100, acc:0.645000\n","epoch:[45], D_loss:1.139272, G_loss:1.376355, acc:0.575500\n","epoch:[46], D_loss:1.168119, G_loss:1.341753, acc:0.639000\n","epoch:[47], D_loss:1.154568, G_loss:1.361096, acc:0.641500\n","epoch:[48], D_loss:1.146260, G_loss:1.369933, acc:0.634000\n","epoch:[49], D_loss:1.122929, G_loss:1.407092, acc:0.644000\n","epoch:[50], D_loss:1.104922, G_loss:1.427858, acc:0.626500\n","epoch:[51], D_loss:1.105307, G_loss:1.442823, acc:0.655000\n","epoch:[52], D_loss:1.078084, G_loss:1.519713, acc:0.635000\n","epoch:[53], D_loss:1.109681, G_loss:1.447564, acc:0.638500\n","epoch:[54], D_loss:1.075480, G_loss:1.500911, acc:0.643000\n","epoch:[55], D_loss:1.103220, G_loss:1.494501, acc:0.654000\n","epoch:[56], D_loss:1.092177, G_loss:1.498951, acc:0.662000\n","epoch:[57], D_loss:1.049313, G_loss:1.543097, acc:0.644000\n","epoch:[58], D_loss:1.050984, G_loss:1.573850, acc:0.662000\n","epoch:[59], D_loss:1.046282, G_loss:1.587409, acc:0.666500\n","epoch:[60], D_loss:1.013355, G_loss:1.634204, acc:0.672500\n","epoch:[61], D_loss:1.020787, G_loss:1.643286, acc:0.641500\n","epoch:[62], D_loss:1.028057, G_loss:1.627032, acc:0.653500\n","epoch:[63], D_loss:1.025067, G_loss:1.652977, acc:0.700000\n","epoch:[64], D_loss:1.023961, G_loss:1.636657, acc:0.643500\n","epoch:[65], D_loss:0.983068, G_loss:1.691165, acc:0.672500\n","epoch:[66], D_loss:0.980839, G_loss:1.740533, acc:0.678500\n","epoch:[67], D_loss:1.000557, G_loss:1.740276, acc:0.674000\n","epoch:[68], D_loss:0.974695, G_loss:1.752700, acc:0.667000\n","epoch:[69], D_loss:0.960846, G_loss:1.786222, acc:0.691000\n","epoch:[70], D_loss:0.954951, G_loss:1.783154, acc:0.689000\n","epoch:[71], D_loss:0.930875, G_loss:1.865364, acc:0.685500\n","epoch:[72], D_loss:0.920248, G_loss:1.893563, acc:0.724000\n","epoch:[73], D_loss:0.922059, G_loss:1.901819, acc:0.700000\n","epoch:[74], D_loss:0.908178, G_loss:1.936710, acc:0.696500\n","epoch:[75], D_loss:0.892965, G_loss:1.955537, acc:0.692500\n","epoch:[76], D_loss:0.891578, G_loss:1.996011, acc:0.677500\n","epoch:[77], D_loss:0.887139, G_loss:1.985567, acc:0.685000\n","epoch:[78], D_loss:0.843686, G_loss:2.009178, acc:0.679500\n","epoch:[79], D_loss:0.825716, G_loss:2.097346, acc:0.701000\n","epoch:[80], D_loss:0.834621, G_loss:2.126711, acc:0.679500\n","epoch:[81], D_loss:0.841068, G_loss:2.184390, acc:0.700500\n","epoch:[82], D_loss:0.838733, G_loss:2.167435, acc:0.664500\n","epoch:[83], D_loss:0.798121, G_loss:2.192694, acc:0.674500\n","epoch:[84], D_loss:0.778962, G_loss:2.293300, acc:0.713500\n","epoch:[85], D_loss:0.781639, G_loss:2.255960, acc:0.696000\n","epoch:[86], D_loss:0.774118, G_loss:2.337081, acc:0.697500\n","epoch:[87], D_loss:0.760667, G_loss:2.302896, acc:0.708000\n","epoch:[88], D_loss:0.763723, G_loss:2.405502, acc:0.713500\n","epoch:[89], D_loss:0.759907, G_loss:2.397371, acc:0.705500\n","epoch:[90], D_loss:0.742428, G_loss:2.409179, acc:0.715500\n","epoch:[91], D_loss:0.729800, G_loss:2.508667, acc:0.735500\n","epoch:[92], D_loss:0.716128, G_loss:2.472764, acc:0.706500\n","epoch:[93], D_loss:0.710251, G_loss:2.531290, acc:0.708500\n","epoch:[94], D_loss:0.709853, G_loss:2.566538, acc:0.705000\n","epoch:[95], D_loss:0.698569, G_loss:2.576298, acc:0.714000\n","epoch:[96], D_loss:0.698857, G_loss:2.647621, acc:0.697000\n","epoch:[97], D_loss:0.668982, G_loss:2.683697, acc:0.726000\n","epoch:[98], D_loss:0.670689, G_loss:2.670774, acc:0.739000\n","epoch:[99], D_loss:0.682535, G_loss:2.740546, acc:0.695000\n","epoch:[100], D_loss:0.719464, G_loss:2.684167, acc:0.694000\n","epoch:[101], D_loss:0.675192, G_loss:2.685327, acc:0.711500\n","epoch:[102], D_loss:0.632382, G_loss:2.784658, acc:0.727000\n","epoch:[103], D_loss:0.616181, G_loss:2.843758, acc:0.741000\n","epoch:[104], D_loss:0.596075, G_loss:2.888227, acc:0.728500\n","epoch:[105], D_loss:0.602680, G_loss:2.936938, acc:0.709500\n","epoch:[106], D_loss:0.619513, G_loss:2.937703, acc:0.719000\n","epoch:[107], D_loss:0.624139, G_loss:2.947328, acc:0.697500\n","epoch:[108], D_loss:0.567687, G_loss:3.021079, acc:0.708000\n","epoch:[109], D_loss:0.587277, G_loss:3.040508, acc:0.728500\n","epoch:[110], D_loss:0.565391, G_loss:3.146211, acc:0.693000\n","epoch:[111], D_loss:0.550455, G_loss:3.138954, acc:0.715000\n","epoch:[112], D_loss:0.584781, G_loss:3.236881, acc:0.711000\n","epoch:[113], D_loss:0.551831, G_loss:3.175882, acc:0.742500\n","epoch:[114], D_loss:0.545009, G_loss:3.248563, acc:0.741500\n","epoch:[115], D_loss:0.563886, G_loss:3.250947, acc:0.731500\n","epoch:[116], D_loss:0.509033, G_loss:3.278816, acc:0.728000\n","epoch:[117], D_loss:0.588548, G_loss:3.274674, acc:0.745500\n","epoch:[118], D_loss:0.559292, G_loss:3.256393, acc:0.719000\n","epoch:[119], D_loss:0.562039, G_loss:3.287184, acc:0.713000\n","epoch:[120], D_loss:0.530335, G_loss:3.365134, acc:0.741000\n","epoch:[121], D_loss:0.505590, G_loss:3.353995, acc:0.722500\n","epoch:[122], D_loss:0.499974, G_loss:3.362764, acc:0.716000\n","epoch:[123], D_loss:0.474023, G_loss:3.492224, acc:0.744500\n","epoch:[124], D_loss:0.498189, G_loss:3.509409, acc:0.737000\n","epoch:[125], D_loss:0.535401, G_loss:3.573259, acc:0.723000\n","epoch:[126], D_loss:0.483658, G_loss:3.470141, acc:0.757000\n","epoch:[127], D_loss:0.538227, G_loss:3.539118, acc:0.749000\n","epoch:[128], D_loss:0.495139, G_loss:3.474047, acc:0.755000\n","epoch:[129], D_loss:0.467701, G_loss:3.599521, acc:0.719000\n","epoch:[130], D_loss:0.483439, G_loss:3.620089, acc:0.727500\n","epoch:[131], D_loss:0.502256, G_loss:3.653124, acc:0.751000\n","epoch:[132], D_loss:0.456437, G_loss:3.582853, acc:0.721000\n","epoch:[133], D_loss:0.446536, G_loss:3.748960, acc:0.713000\n","epoch:[134], D_loss:0.464560, G_loss:3.660789, acc:0.723000\n","epoch:[135], D_loss:0.449650, G_loss:3.711358, acc:0.728000\n","epoch:[136], D_loss:0.451087, G_loss:3.757981, acc:0.728500\n","epoch:[137], D_loss:0.485061, G_loss:3.766676, acc:0.743500\n","epoch:[138], D_loss:0.444196, G_loss:3.804368, acc:0.752000\n","epoch:[139], D_loss:0.440193, G_loss:3.875564, acc:0.743500\n","epoch:[140], D_loss:0.449935, G_loss:3.855529, acc:0.731500\n","epoch:[141], D_loss:0.456226, G_loss:3.855181, acc:0.723000\n","epoch:[142], D_loss:0.432792, G_loss:3.853018, acc:0.735500\n","epoch:[143], D_loss:0.422438, G_loss:3.866421, acc:0.730500\n","epoch:[144], D_loss:0.410535, G_loss:3.922859, acc:0.741000\n","epoch:[145], D_loss:0.394017, G_loss:3.957786, acc:0.737000\n","epoch:[146], D_loss:0.396859, G_loss:3.983073, acc:0.726500\n","epoch:[147], D_loss:0.414187, G_loss:4.042050, acc:0.712500\n","epoch:[148], D_loss:0.448961, G_loss:4.043241, acc:0.727000\n","epoch:[149], D_loss:0.361066, G_loss:4.033477, acc:0.722000\n","epoch:[150], D_loss:0.399894, G_loss:4.084240, acc:0.713500\n","epoch:[151], D_loss:0.385549, G_loss:4.121911, acc:0.740000\n","epoch:[152], D_loss:0.393442, G_loss:4.157164, acc:0.729000\n","epoch:[153], D_loss:0.372591, G_loss:4.098887, acc:0.733000\n","epoch:[154], D_loss:0.390086, G_loss:4.207849, acc:0.724500\n","epoch:[155], D_loss:0.361336, G_loss:4.162458, acc:0.719000\n","epoch:[156], D_loss:0.387490, G_loss:4.257944, acc:0.720500\n","epoch:[157], D_loss:0.396241, G_loss:4.281912, acc:0.745500\n","epoch:[158], D_loss:0.386414, G_loss:4.268062, acc:0.749500\n","epoch:[159], D_loss:0.382515, G_loss:4.289120, acc:0.717500\n","epoch:[160], D_loss:0.370714, G_loss:4.303989, acc:0.717500\n","epoch:[161], D_loss:0.367854, G_loss:4.271650, acc:0.707000\n","epoch:[162], D_loss:0.367131, G_loss:4.377724, acc:0.740000\n","epoch:[163], D_loss:0.362154, G_loss:4.380286, acc:0.730000\n","epoch:[164], D_loss:0.370820, G_loss:4.464994, acc:0.721500\n","epoch:[165], D_loss:0.366195, G_loss:4.455314, acc:0.731500\n","epoch:[166], D_loss:0.345558, G_loss:4.394421, acc:0.737000\n","epoch:[167], D_loss:0.355490, G_loss:4.493559, acc:0.726000\n","epoch:[168], D_loss:0.303059, G_loss:4.529039, acc:0.743500\n","epoch:[169], D_loss:0.365909, G_loss:4.565305, acc:0.711500\n","epoch:[170], D_loss:0.347042, G_loss:4.502783, acc:0.729000\n","epoch:[171], D_loss:0.342239, G_loss:4.512849, acc:0.725000\n","epoch:[172], D_loss:0.322759, G_loss:4.530474, acc:0.744500\n","epoch:[173], D_loss:0.334727, G_loss:4.601761, acc:0.769500\n","epoch:[174], D_loss:0.317622, G_loss:4.663116, acc:0.712500\n","epoch:[175], D_loss:0.334882, G_loss:4.648648, acc:0.742500\n","epoch:[176], D_loss:0.310494, G_loss:4.548490, acc:0.733000\n","epoch:[177], D_loss:0.332829, G_loss:4.703421, acc:0.760000\n","epoch:[178], D_loss:0.320329, G_loss:4.688110, acc:0.715500\n","epoch:[179], D_loss:0.340112, G_loss:4.751132, acc:0.724500\n","epoch:[180], D_loss:0.284979, G_loss:4.692520, acc:0.715500\n","epoch:[181], D_loss:0.326824, G_loss:4.831108, acc:0.712500\n","epoch:[182], D_loss:0.346701, G_loss:4.778651, acc:0.744500\n","epoch:[183], D_loss:0.312950, G_loss:4.777548, acc:0.738500\n","epoch:[184], D_loss:0.323113, G_loss:4.706221, acc:0.743500\n","epoch:[185], D_loss:0.398143, G_loss:4.703778, acc:0.737000\n","epoch:[186], D_loss:0.311553, G_loss:4.698085, acc:0.739000\n","epoch:[187], D_loss:0.311814, G_loss:4.839183, acc:0.762000\n","epoch:[188], D_loss:0.292137, G_loss:4.831097, acc:0.741000\n","epoch:[189], D_loss:0.286152, G_loss:4.791573, acc:0.761000\n","epoch:[190], D_loss:0.298294, G_loss:4.800395, acc:0.739500\n","epoch:[191], D_loss:0.287906, G_loss:4.877842, acc:0.734500\n","epoch:[192], D_loss:0.270081, G_loss:4.987023, acc:0.730000\n","epoch:[193], D_loss:0.313772, G_loss:4.975432, acc:0.733500\n","epoch:[194], D_loss:0.401692, G_loss:5.020582, acc:0.744000\n","epoch:[195], D_loss:0.333988, G_loss:4.804418, acc:0.743500\n","epoch:[196], D_loss:0.309994, G_loss:4.786574, acc:0.738500\n","epoch:[197], D_loss:0.263338, G_loss:4.866827, acc:0.736000\n","epoch:[198], D_loss:0.276769, G_loss:4.964170, acc:0.719000\n","epoch:[199], D_loss:0.302379, G_loss:4.998839, acc:0.733500\n","epoch:[200], D_loss:0.303400, G_loss:5.005927, acc:0.723000\n","epoch:[201], D_loss:0.299798, G_loss:5.004035, acc:0.731000\n","epoch:[202], D_loss:0.276801, G_loss:5.039178, acc:0.742000\n","epoch:[203], D_loss:0.283718, G_loss:5.055956, acc:0.726000\n","epoch:[204], D_loss:0.295090, G_loss:5.105644, acc:0.732000\n","epoch:[205], D_loss:0.234335, G_loss:5.041321, acc:0.727000\n","epoch:[206], D_loss:0.276978, G_loss:5.153378, acc:0.771000\n","epoch:[207], D_loss:0.242379, G_loss:5.263386, acc:0.737500\n","epoch:[208], D_loss:0.283914, G_loss:5.127651, acc:0.743500\n","epoch:[209], D_loss:0.228267, G_loss:5.232245, acc:0.745000\n","epoch:[210], D_loss:0.267515, G_loss:5.281006, acc:0.754000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-815448055a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mD_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()\n","net[\"generator\"] = Generator()\n","net[\"discriminator\"] = Discriminator()\n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\", \"generator\", \"discriminator\"], True)\n","\n","D_optimizer = optim.Adam(get_params(net, [\"discriminator\"]), lr=2e-4, betas=(0.5, 0.999))\n","G_optimizer = optim.Adam(get_params(net, [\"generator\"]), lr=2e-4, betas=(0.5, 0.999))\n","BCE_criterion = nn.BCELoss().to(device)\n","\n","E_checkpoint = torch.load(\"./checkpoint/client_extractor.pkl\", map_location=torch.device('cpu'))\n","net[\"extractor\"].load_state_dict(E_checkpoint)\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","\n","def discriminator_loss(E, G, y):\n","    ones = torch.ones((E.size(0), 1)).to(device)\n","    ED = net[\"discriminator\"](E.detach(), y)\n","    ED_loss = BCE_criterion(ED, ones)\n","\n","    zeros = torch.zeros((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G.detach(), y)\n","    GD_loss = BCE_criterion(GD, zeros)\n","    return ED_loss + GD_loss\n","\n","\n","def generator_loss(G, y):\n","    ones = torch.ones((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G, y)\n","    G_loss = BCE_criterion(GD, ones)\n","    return G_loss\n","\n","\n","for epoch in range(400):\n","    # train\n","    frozen_net(net, [\"generator\", \"discriminator\"], False)\n","\n","    D_losses, G_losses = [], []\n","    for batch, (x, y) in enumerate(client_trainloader):\n","        x = x.to(device)\n","        y = y.to(device)\n","        z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","\n","        with torch.no_grad():\n","            E = net[\"extractor\"](x)\n","        G = net[\"generator\"](z, y)\n","\n","        # update D\n","        D_optimizer.zero_grad()\n","        D_loss = discriminator_loss(E, G, y)\n","        D_loss.backward()\n","        D_optimizer.step()\n","        D_losses.append(D_loss.item())\n","\n","        # update G\n","        G_optimizer.zero_grad()\n","        G_loss = generator_loss(G, y)\n","        G_loss.backward()\n","        G_optimizer.step()\n","        G_losses.append(G_loss.item())\n","    \n","    frozen_net(net, [\"generator\", \"discriminator\"], True)\n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            y = y.to(device)\n","            z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","\n","            G = net[\"generator\"](z, y)\n","            GC = net[\"classifier\"](G)\n","\n","            correct += torch.sum((torch.argmax(GC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], D_loss:%2.6f, G_loss:%2.6f, acc:%2.6f\"\n","        %(epoch, np.mean(D_losses), np.mean(G_losses), acc))\n","\n","    # if (epoch+1) % 10 == 0:\n","    torch.save(net[\"generator\"].state_dict(), \"./checkpoint/client_generator.pkl\")\n","    torch.save(net[\"discriminator\"].state_dict(), \"./checkpoint/client_discriminator.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"Nj5bLTAsEguA"},"source":["**GAN div train**"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"rGpwDDm5EjjR","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1640943856313,"user_tz":-480,"elapsed":2525740,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"bad6e6ca-fc92-4135-bf44-e2b429e6c851"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], D_loss:1.056179, G_loss:1.330746, G_div:8.303213, acc:0.098000\n","epoch:[ 1], D_loss:0.643448, G_loss:2.833087, G_div:3.980607, acc:0.110000\n","epoch:[ 2], D_loss:0.570310, G_loss:2.610081, G_div:3.368141, acc:0.075500\n","epoch:[ 3], D_loss:0.375023, G_loss:2.506584, G_div:3.073811, acc:0.101500\n","epoch:[ 4], D_loss:0.268802, G_loss:2.715143, G_div:3.113455, acc:0.100500\n","epoch:[ 5], D_loss:0.478787, G_loss:3.205222, G_div:3.410010, acc:0.122000\n","epoch:[ 6], D_loss:0.685018, G_loss:3.270406, G_div:3.145173, acc:0.126000\n","epoch:[ 7], D_loss:0.698319, G_loss:2.811400, G_div:2.907139, acc:0.132000\n","epoch:[ 8], D_loss:0.672761, G_loss:2.805368, G_div:2.891236, acc:0.138000\n","epoch:[ 9], D_loss:0.725437, G_loss:2.482074, G_div:2.872471, acc:0.169500\n","epoch:[10], D_loss:0.781397, G_loss:2.498924, G_div:2.868101, acc:0.169500\n","epoch:[11], D_loss:0.752736, G_loss:2.460147, G_div:2.875949, acc:0.189500\n","epoch:[12], D_loss:0.797045, G_loss:2.300940, G_div:2.835150, acc:0.178500\n","epoch:[13], D_loss:0.804332, G_loss:2.410145, G_div:2.828812, acc:0.210500\n","epoch:[14], D_loss:0.803955, G_loss:2.240116, G_div:2.841821, acc:0.216000\n","epoch:[15], D_loss:0.787459, G_loss:2.170257, G_div:2.794754, acc:0.242000\n","epoch:[16], D_loss:0.864975, G_loss:2.106008, G_div:2.874552, acc:0.285500\n","epoch:[17], D_loss:0.871788, G_loss:2.015478, G_div:2.864116, acc:0.284000\n","epoch:[18], D_loss:0.851214, G_loss:2.034882, G_div:2.821174, acc:0.288500\n","epoch:[19], D_loss:0.892360, G_loss:2.006729, G_div:2.844309, acc:0.344000\n","epoch:[20], D_loss:0.937876, G_loss:1.770840, G_div:2.868493, acc:0.340500\n","epoch:[21], D_loss:0.931371, G_loss:1.711214, G_div:2.817508, acc:0.398000\n","epoch:[22], D_loss:0.907946, G_loss:1.770408, G_div:2.870598, acc:0.440500\n","epoch:[23], D_loss:0.943433, G_loss:1.755395, G_div:2.835171, acc:0.421000\n","epoch:[24], D_loss:0.968812, G_loss:1.717886, G_div:2.842416, acc:0.477000\n","epoch:[25], D_loss:0.941323, G_loss:1.667788, G_div:2.884471, acc:0.460500\n","epoch:[26], D_loss:0.979916, G_loss:1.668516, G_div:2.852203, acc:0.477500\n","epoch:[27], D_loss:0.962976, G_loss:1.627373, G_div:2.843704, acc:0.522500\n","epoch:[28], D_loss:0.994041, G_loss:1.616027, G_div:2.836320, acc:0.527000\n","epoch:[29], D_loss:0.954872, G_loss:1.635726, G_div:2.865668, acc:0.526000\n","epoch:[30], D_loss:0.987227, G_loss:1.608015, G_div:2.874310, acc:0.566500\n","epoch:[31], D_loss:0.975116, G_loss:1.626608, G_div:2.886223, acc:0.547500\n","epoch:[32], D_loss:0.952752, G_loss:1.734362, G_div:2.865305, acc:0.538500\n","epoch:[33], D_loss:0.963078, G_loss:1.630179, G_div:2.869049, acc:0.550500\n","epoch:[34], D_loss:0.947373, G_loss:1.575335, G_div:2.878881, acc:0.549000\n","epoch:[35], D_loss:0.951735, G_loss:1.585080, G_div:2.866988, acc:0.564500\n","epoch:[36], D_loss:0.927092, G_loss:1.615321, G_div:2.911978, acc:0.578000\n","epoch:[37], D_loss:0.929837, G_loss:1.671801, G_div:2.865096, acc:0.576500\n","epoch:[38], D_loss:0.916216, G_loss:1.618795, G_div:2.863593, acc:0.566500\n","epoch:[39], D_loss:0.968835, G_loss:1.558323, G_div:2.883821, acc:0.592500\n","epoch:[40], D_loss:0.911523, G_loss:1.660616, G_div:2.868679, acc:0.604000\n","epoch:[41], D_loss:0.925206, G_loss:1.776034, G_div:2.901468, acc:0.598500\n","epoch:[42], D_loss:0.879451, G_loss:1.682856, G_div:2.879593, acc:0.616500\n","epoch:[43], D_loss:0.905054, G_loss:1.742339, G_div:2.918611, acc:0.562000\n","epoch:[44], D_loss:0.877652, G_loss:1.740366, G_div:2.846585, acc:0.635500\n","epoch:[45], D_loss:0.887081, G_loss:1.747500, G_div:2.941583, acc:0.583000\n","epoch:[46], D_loss:0.867727, G_loss:1.784978, G_div:2.893471, acc:0.625500\n","epoch:[47], D_loss:0.840726, G_loss:1.858057, G_div:2.880587, acc:0.620500\n","epoch:[48], D_loss:0.899269, G_loss:1.792647, G_div:2.938053, acc:0.602000\n","epoch:[49], D_loss:0.844795, G_loss:1.781218, G_div:2.892022, acc:0.624000\n","epoch:[50], D_loss:0.849559, G_loss:1.760818, G_div:2.928884, acc:0.637500\n","epoch:[51], D_loss:0.837853, G_loss:1.811675, G_div:2.956630, acc:0.625500\n","epoch:[52], D_loss:0.807347, G_loss:1.786640, G_div:2.907691, acc:0.639500\n","epoch:[53], D_loss:0.843480, G_loss:1.833758, G_div:2.936651, acc:0.669000\n","epoch:[54], D_loss:0.779539, G_loss:1.803616, G_div:2.922718, acc:0.655500\n","epoch:[55], D_loss:0.809813, G_loss:1.914927, G_div:2.937170, acc:0.642500\n","epoch:[56], D_loss:0.821042, G_loss:1.922270, G_div:2.952778, acc:0.657000\n","epoch:[57], D_loss:0.796186, G_loss:1.858421, G_div:2.943332, acc:0.649500\n","epoch:[58], D_loss:0.770601, G_loss:1.951671, G_div:2.937752, acc:0.681000\n","epoch:[59], D_loss:0.739822, G_loss:1.960738, G_div:2.975368, acc:0.683000\n","epoch:[60], D_loss:0.746343, G_loss:1.965993, G_div:2.940119, acc:0.669500\n","epoch:[61], D_loss:0.726735, G_loss:2.007966, G_div:2.960655, acc:0.664000\n","epoch:[62], D_loss:0.739724, G_loss:2.112707, G_div:3.003477, acc:0.674500\n","epoch:[63], D_loss:0.717758, G_loss:2.053628, G_div:2.990096, acc:0.668500\n","epoch:[64], D_loss:0.683786, G_loss:2.144924, G_div:2.951064, acc:0.690500\n","epoch:[65], D_loss:0.677807, G_loss:2.181302, G_div:2.941294, acc:0.686000\n","epoch:[66], D_loss:0.707130, G_loss:2.175466, G_div:2.987556, acc:0.680000\n","epoch:[67], D_loss:0.671259, G_loss:2.252197, G_div:2.982703, acc:0.706000\n","epoch:[68], D_loss:0.689911, G_loss:2.222429, G_div:2.956470, acc:0.695000\n","epoch:[69], D_loss:0.666154, G_loss:2.244801, G_div:2.982239, acc:0.710500\n","epoch:[70], D_loss:0.654980, G_loss:2.238938, G_div:2.990388, acc:0.692500\n","epoch:[71], D_loss:0.673780, G_loss:2.250534, G_div:3.012680, acc:0.680000\n","epoch:[72], D_loss:0.628387, G_loss:2.339176, G_div:3.033773, acc:0.659500\n","epoch:[73], D_loss:0.672970, G_loss:2.325123, G_div:3.025142, acc:0.686000\n","epoch:[74], D_loss:0.607301, G_loss:2.358855, G_div:2.993757, acc:0.680500\n","epoch:[75], D_loss:0.607319, G_loss:2.426337, G_div:2.993230, acc:0.687500\n","epoch:[76], D_loss:0.585775, G_loss:2.410477, G_div:3.044232, acc:0.673000\n","epoch:[77], D_loss:0.592724, G_loss:2.429489, G_div:2.930625, acc:0.697000\n","epoch:[78], D_loss:0.565933, G_loss:2.490667, G_div:2.977510, acc:0.684500\n","epoch:[79], D_loss:0.603396, G_loss:2.437009, G_div:3.007231, acc:0.690000\n","epoch:[80], D_loss:0.572225, G_loss:2.479017, G_div:2.996073, acc:0.699500\n","epoch:[81], D_loss:0.568773, G_loss:2.615817, G_div:3.007140, acc:0.690500\n","epoch:[82], D_loss:0.548455, G_loss:2.645991, G_div:2.978286, acc:0.694500\n","epoch:[83], D_loss:0.564722, G_loss:2.596211, G_div:3.022993, acc:0.687500\n","epoch:[84], D_loss:0.553517, G_loss:2.640186, G_div:3.004673, acc:0.702000\n","epoch:[85], D_loss:0.499427, G_loss:2.819606, G_div:3.005090, acc:0.699500\n","epoch:[86], D_loss:0.544317, G_loss:2.700478, G_div:3.007457, acc:0.658000\n","epoch:[87], D_loss:0.511566, G_loss:2.670654, G_div:2.965099, acc:0.701500\n","epoch:[88], D_loss:0.496981, G_loss:2.816351, G_div:3.010737, acc:0.697500\n","epoch:[89], D_loss:0.505570, G_loss:2.822855, G_div:3.028031, acc:0.721000\n","epoch:[90], D_loss:0.512565, G_loss:2.805509, G_div:2.993989, acc:0.724000\n","epoch:[91], D_loss:0.531800, G_loss:2.858917, G_div:3.000033, acc:0.684000\n","epoch:[92], D_loss:0.486006, G_loss:2.929994, G_div:2.991709, acc:0.707500\n","epoch:[93], D_loss:0.477851, G_loss:2.895578, G_div:3.021962, acc:0.704500\n","epoch:[94], D_loss:0.433026, G_loss:2.940840, G_div:3.054146, acc:0.672500\n","epoch:[95], D_loss:0.476405, G_loss:2.917669, G_div:3.027709, acc:0.679500\n","epoch:[96], D_loss:0.454854, G_loss:2.927897, G_div:3.036729, acc:0.677500\n","epoch:[97], D_loss:0.488703, G_loss:3.126701, G_div:3.042861, acc:0.670000\n","epoch:[98], D_loss:0.451421, G_loss:2.896475, G_div:3.017578, acc:0.702000\n","epoch:[99], D_loss:0.437152, G_loss:3.009953, G_div:3.080534, acc:0.702000\n","epoch:[100], D_loss:0.469517, G_loss:3.045407, G_div:2.993585, acc:0.692500\n","epoch:[101], D_loss:0.465531, G_loss:3.191738, G_div:3.044387, acc:0.675500\n","epoch:[102], D_loss:0.459751, G_loss:3.027537, G_div:3.028368, acc:0.683000\n","epoch:[103], D_loss:0.416650, G_loss:3.212954, G_div:3.010454, acc:0.699500\n","epoch:[104], D_loss:0.489370, G_loss:3.176752, G_div:2.981864, acc:0.705000\n","epoch:[105], D_loss:0.400427, G_loss:3.130041, G_div:2.979308, acc:0.712000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-35a2534c3f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mD_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()\n","net[\"generator\"] = Generator()\n","net[\"discriminator\"] = Discriminator()\n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\", \"generator\", \"discriminator\"], True)\n","\n","D_optimizer = optim.Adam(get_params(net, [\"discriminator\"]), lr=2e-4, betas=(0.5, 0.999))\n","G_optimizer = optim.Adam(get_params(net, [\"generator\"]), lr=4e-4, betas=(0.5, 0.999))\n","BCE_criterion = nn.BCELoss().to(device)\n","\n","E_checkpoint = torch.load(\"./checkpoint/client_extractor.pkl\", map_location=torch.device('cpu'))\n","net[\"extractor\"].load_state_dict(E_checkpoint)\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","\n","def diversity_loss(G1, G2, z1, z2):\n","    lz = torch.mean(torch.abs(G2 - G1)) / torch.mean(torch.abs(z2 - z1))\n","    eps = 1 * 1e-5\n","    G_div = 1 / (lz + eps)\n","    return G_div\n","\n","\n","def discriminator_loss(E, G, y):\n","    ones = torch.ones((E.size(0), 1)).to(device)\n","    ED = net[\"discriminator\"](E.detach(), y)\n","    ED_loss = BCE_criterion(ED, ones)\n","\n","    zeros = torch.zeros((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G.detach(), y)\n","    GD_loss = BCE_criterion(GD, zeros)\n","    return ED_loss + GD_loss\n","\n","\n","def generator_loss(G, y):\n","    ones = torch.ones((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G, y)\n","    G_loss = BCE_criterion(GD, ones)\n","    return G_loss\n","\n","\n","for epoch in range(200):\n","    # train\n","    frozen_net(net, [\"generator\", \"discriminator\"], False)\n","\n","    D_losses, G_losses, G_divs = [], [], []\n","    for batch, (x, y) in enumerate(client_trainloader):\n","        x = x.to(device)\n","        y = y.to(device)\n","\n","        with torch.no_grad():\n","            E = net[\"extractor\"](x)\n","\n","        # update D\n","        D_optimizer.zero_grad()\n","        z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","        G = net[\"generator\"](z, y)\n","        D_loss = discriminator_loss(E, G, y)\n","\n","        D_loss.backward()\n","        D_optimizer.step()\n","        D_losses.append(D_loss.item())\n","\n","        # update G\n","        if (batch+1)%2 == 0:\n","            G_optimizer.zero_grad()\n","            ys = torch.cat([y,y], 0)\n","            zs = torch.randn(x.size(0)*2, 100, 1, 1).to(device)\n","            Gs = net[\"generator\"](zs, ys)\n","            G_loss = generator_loss(Gs, ys)\n","\n","            z1, z2 = torch.split(zs, x.size(0), 0)\n","            G1, G2 = torch.split(Gs, x.size(0), 0)\n","            G_div = diversity_loss(G1, G2, z1, z2)\n","\n","            (G_loss + G_div).backward()\n","            G_optimizer.step()\n","            G_losses.append(G_loss.item())\n","            G_divs.append(G_div.item())\n","    \n","    frozen_net(net, [\"generator\", \"discriminator\"], True)\n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            y = y.to(device)\n","            z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","\n","            G = net[\"generator\"](z, y)\n","            GC = net[\"classifier\"](G)\n","\n","            correct += torch.sum((torch.argmax(GC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], D_loss:%2.6f, G_loss:%2.6f, G_div:%2.6f, acc:%2.6f\"\n","        %(epoch, np.mean(D_losses), np.mean(G_losses), np.mean(G_divs), acc))\n","\n","    if (epoch+1) % 10 == 0:\n","        torch.save(net[\"generator\"].state_dict(), \"./checkpoint/client_generator_div.pkl\")\n","        torch.save(net[\"discriminator\"].state_dict(), \"./checkpoint/client_discriminator_div.pkl\")"]}],"metadata":{"colab":{"name":"model train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1DB7xMK0N2UDwsTpfCCmPtAlZd8MQy7KU","authorship_tag":"ABX9TyMX/HKOZpNDB4IGVyRPkGwK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}