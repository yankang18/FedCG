{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model train.ipynb","provenance":[],"mount_file_id":"1fy3xqfv73g0rgJjDwCTI_u3NIuEa6M7S","authorship_tag":"ABX9TyPsu1gWWro+Te9ClyTMrSFz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiO3TVc0x3Gl","executionInfo":{"status":"ok","timestamp":1640310767434,"user_tz":-480,"elapsed":4,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"55683ce9-383d-4c34-b6c1-0e9e3710b668"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/model inversion(lenet)\n"]}],"source":["cd \"drive/MyDrive/model inversion(lenet)\""]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","class Extractor(nn.Module):\n","\n","    def __init__(self):\n","        super(Extractor, self).__init__()\n","        self.extractor = nn.Sequential(\n","            nn.Conv2d(1, 6, 5),\n","            nn.AvgPool2d(2, 2),\n","            nn.Sigmoid(),\n","            nn.Conv2d(6, 16, 5),\n","            nn.AvgPool2d(2, 2),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.extractor(x)\n","        return x\n","\n","    \n","class Classifier(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(Classifier, self).__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(16 * 5 * 5, 120),\n","            nn.Sigmoid(),\n","            nn.Linear(120, 84),\n","            nn.Sigmoid(),\n","            nn.Linear(84, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.classifier(x)\n","        return x\n","\n","\n","class Generator(nn.Module):\n","\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.generator = nn.Sequential(\n","            nn.ConvTranspose2d(100 + 10, 512, 2, 1, 0),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(512, 256, 2, 1, 0),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(256, 128, 2, 1, 0),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 16, 2, 1, 0),\n","            nn.Sigmoid(),\n","        )\n","        self.apply(weights_init)\n","        \n","    def forward(self, z, y):\n","        y = F.one_hot(y, 10)\n","        y = y.unsqueeze(-1).unsqueeze(-1)\n","        feat = torch.cat([z, y], 1)\n","        feat = self.generator(feat)\n","        return feat\n","    \n","    \n","class Discriminator(nn.Module):\n","\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.discriminator = nn.Sequential(\n","            nn.Conv2d(16 + 10, 128, 2, 1, 0),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(128, 256, 2, 1, 0),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(256, 512, 2, 1, 0),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(inplace=True),\n","            nn.Conv2d(512, 1, 2, 1, 0),\n","            nn.Sigmoid(),\n","        )\n","        self.apply(weights_init)\n","        \n","    def forward(self, feat, y):\n","        y = F.one_hot(y, 10)\n","        y = y.unsqueeze(-1).unsqueeze(-1)\n","        y = y.expand(y.size(0), 10, 5, 5)\n","        feat = torch.cat([feat, y], 1)\n","        feat = self.discriminator(feat)\n","        feat = feat.squeeze(-1).squeeze(-1)\n","        return feat"],"metadata":{"id":"s3WBnGOpyCdY","executionInfo":{"status":"ok","timestamp":1640310788664,"user_tz":-480,"elapsed":5972,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import transforms\n","import torchvision.datasets as datasets\n","import numpy as np\n","\n","transform=transforms.Compose([\n","    transforms.Resize([32, 32]),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5), (0.5)),\n","])\n","\n","usps_trainset = datasets.USPS(root='./data', train=True, download=True, transform=transform)\n","usps_testset = datasets.USPS(root='./data', train=False, download=True, transform=transform)\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","size = len(usps_trainset)\n","index = np.arange(size)\n","client_trainset = Subset(usps_trainset, index[:2000])\n","server_iid_trainset = Subset(usps_trainset, index[2000:4000])\n","server_niid_trainset = Subset(mnist_trainset, index[:2000])\n","\n","client_trainloader = DataLoader(client_trainset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n","server_iid_trainloader = DataLoader(server_iid_trainset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n","server_niid_trainloader = DataLoader(server_niid_trainset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n","\n","client_testloader = DataLoader(usps_testset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n","server_iid_testloader = DataLoader(usps_testset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n","server_niid_testloader = DataLoader(mnist_testset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)"],"metadata":{"id":"cZ0VLFMex767","executionInfo":{"status":"ok","timestamp":1640310800542,"user_tz":-480,"elapsed":9348,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def get_params(net, modules):\n","    params = []\n","    for module in modules:\n","        params.append({\"params\": net[module].parameters()})\n","    return params\n","\n","def frozen_net(net, modules, frozen):\n","    for module in modules:\n","        for param in net[module].parameters():\n","            param.requires_grad = not frozen\n","        if frozen:\n","            net[module].eval()\n","        else:\n","            net[module].train()\n"],"metadata":{"id":"VbvFSIwRyEc0","executionInfo":{"status":"ok","timestamp":1640310804140,"user_tz":-480,"elapsed":334,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**client train**"],"metadata":{"id":"x-o2BJ-4_DmO"}},{"cell_type":"code","source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","EC_optimizer = optim.Adam(get_params(net, [\"extractor\", \"classifier\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","for epoch in range(500):\n","    # train\n","    frozen_net(net, [\"extractor\", \"classifier\"], False)\n","    losses, batch = 0., 0\n","    for x, y in client_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        EC_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        EC_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\", \"classifier\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/client_extractor.pkl\")\n","torch.save(net[\"classifier\"].state_dict(), \"./checkpoint/client_classifier.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60HzDW-EyM3y","executionInfo":{"status":"ok","timestamp":1640263353379,"user_tz":-480,"elapsed":1048593,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"0d6fb13d-9975-4933-f8d7-19cef6de966f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:2.244613, acc:0.178874\n","epoch:[ 1], loss:2.226910, acc:0.178874\n","epoch:[ 2], loss:2.037559, acc:0.304933\n","epoch:[ 3], loss:1.624543, acc:0.520678\n","epoch:[ 4], loss:1.314992, acc:0.581963\n","epoch:[ 5], loss:1.090726, acc:0.645740\n","epoch:[ 6], loss:0.919579, acc:0.705531\n","epoch:[ 7], loss:0.782288, acc:0.728949\n","epoch:[ 8], loss:0.673135, acc:0.749377\n","epoch:[ 9], loss:0.581481, acc:0.779771\n","epoch:[10], loss:0.508161, acc:0.803687\n","epoch:[11], loss:0.450654, acc:0.804684\n","epoch:[12], loss:0.406532, acc:0.816143\n","epoch:[13], loss:0.372031, acc:0.827105\n","epoch:[14], loss:0.340752, acc:0.835077\n","epoch:[15], loss:0.316831, acc:0.845042\n","epoch:[16], loss:0.293685, acc:0.844544\n","epoch:[17], loss:0.275950, acc:0.851520\n","epoch:[18], loss:0.257414, acc:0.856502\n","epoch:[19], loss:0.240947, acc:0.862980\n","epoch:[20], loss:0.225517, acc:0.864973\n","epoch:[21], loss:0.213736, acc:0.869457\n","epoch:[22], loss:0.198991, acc:0.881415\n","epoch:[23], loss:0.184711, acc:0.877429\n","epoch:[24], loss:0.176864, acc:0.886398\n","epoch:[25], loss:0.165406, acc:0.889387\n","epoch:[26], loss:0.154113, acc:0.893373\n","epoch:[27], loss:0.147857, acc:0.896861\n","epoch:[28], loss:0.137037, acc:0.892377\n","epoch:[29], loss:0.128905, acc:0.887394\n","epoch:[30], loss:0.123040, acc:0.902840\n","epoch:[31], loss:0.117617, acc:0.898854\n","epoch:[32], loss:0.109792, acc:0.902342\n","epoch:[33], loss:0.102974, acc:0.899352\n","epoch:[34], loss:0.099697, acc:0.908819\n","epoch:[35], loss:0.092746, acc:0.907324\n","epoch:[36], loss:0.090080, acc:0.906328\n","epoch:[37], loss:0.086822, acc:0.911310\n","epoch:[38], loss:0.078991, acc:0.906328\n","epoch:[39], loss:0.077806, acc:0.903338\n","epoch:[40], loss:0.072274, acc:0.913303\n","epoch:[41], loss:0.069289, acc:0.907324\n","epoch:[42], loss:0.064112, acc:0.913303\n","epoch:[43], loss:0.062529, acc:0.908819\n","epoch:[44], loss:0.058439, acc:0.906826\n","epoch:[45], loss:0.055502, acc:0.913802\n","epoch:[46], loss:0.051385, acc:0.911310\n","epoch:[47], loss:0.051326, acc:0.911809\n","epoch:[48], loss:0.046362, acc:0.909816\n","epoch:[49], loss:0.044313, acc:0.912307\n","epoch:[50], loss:0.042942, acc:0.914300\n","epoch:[51], loss:0.038806, acc:0.913303\n","epoch:[52], loss:0.038106, acc:0.916791\n","epoch:[53], loss:0.035877, acc:0.911809\n","epoch:[54], loss:0.033196, acc:0.915795\n","epoch:[55], loss:0.031420, acc:0.915296\n","epoch:[56], loss:0.031700, acc:0.914300\n","epoch:[57], loss:0.029468, acc:0.910812\n","epoch:[58], loss:0.025648, acc:0.906328\n","epoch:[59], loss:0.026384, acc:0.915296\n","epoch:[60], loss:0.024746, acc:0.915296\n","epoch:[61], loss:0.023913, acc:0.915296\n","epoch:[62], loss:0.023072, acc:0.915795\n","epoch:[63], loss:0.022309, acc:0.917289\n","epoch:[64], loss:0.021645, acc:0.914300\n","epoch:[65], loss:0.019095, acc:0.917289\n","epoch:[66], loss:0.019456, acc:0.912805\n","epoch:[67], loss:0.018757, acc:0.916791\n","epoch:[68], loss:0.018356, acc:0.914798\n","epoch:[69], loss:0.016948, acc:0.919283\n","epoch:[70], loss:0.016403, acc:0.916791\n","epoch:[71], loss:0.016086, acc:0.918784\n","epoch:[72], loss:0.015676, acc:0.916791\n","epoch:[73], loss:0.015249, acc:0.917289\n","epoch:[74], loss:0.013355, acc:0.916293\n","epoch:[75], loss:0.013249, acc:0.915296\n","epoch:[76], loss:0.012498, acc:0.912805\n","epoch:[77], loss:0.013648, acc:0.915296\n","epoch:[78], loss:0.013263, acc:0.918286\n","epoch:[79], loss:0.010939, acc:0.918286\n","epoch:[80], loss:0.012411, acc:0.918286\n","epoch:[81], loss:0.011958, acc:0.914798\n","epoch:[82], loss:0.010112, acc:0.913303\n","epoch:[83], loss:0.011089, acc:0.916791\n","epoch:[84], loss:0.010910, acc:0.921276\n","epoch:[85], loss:0.010366, acc:0.920777\n","epoch:[86], loss:0.010007, acc:0.919283\n","epoch:[87], loss:0.010977, acc:0.915795\n","epoch:[88], loss:0.009747, acc:0.917788\n","epoch:[89], loss:0.009687, acc:0.919283\n","epoch:[90], loss:0.011922, acc:0.917788\n","epoch:[91], loss:0.009379, acc:0.922770\n","epoch:[92], loss:0.010044, acc:0.917289\n","epoch:[93], loss:0.008356, acc:0.918784\n","epoch:[94], loss:0.009193, acc:0.920777\n","epoch:[95], loss:0.010000, acc:0.920777\n","epoch:[96], loss:0.009313, acc:0.915296\n","epoch:[97], loss:0.008789, acc:0.917788\n","epoch:[98], loss:0.008714, acc:0.921276\n","epoch:[99], loss:0.009778, acc:0.910314\n","epoch:[100], loss:0.009073, acc:0.921276\n","epoch:[101], loss:0.007322, acc:0.919781\n","epoch:[102], loss:0.008522, acc:0.914798\n","epoch:[103], loss:0.009015, acc:0.914798\n","epoch:[104], loss:0.007170, acc:0.918286\n","epoch:[105], loss:0.011219, acc:0.914300\n","epoch:[106], loss:0.009217, acc:0.918784\n","epoch:[107], loss:0.007773, acc:0.916293\n","epoch:[108], loss:0.007773, acc:0.919781\n","epoch:[109], loss:0.007382, acc:0.920279\n","epoch:[110], loss:0.007384, acc:0.921774\n","epoch:[111], loss:0.007167, acc:0.916791\n","epoch:[112], loss:0.007758, acc:0.922272\n","epoch:[113], loss:0.007724, acc:0.917289\n","epoch:[114], loss:0.009402, acc:0.916293\n","epoch:[115], loss:0.007058, acc:0.918286\n","epoch:[116], loss:0.007363, acc:0.919781\n","epoch:[117], loss:0.006503, acc:0.917289\n","epoch:[118], loss:0.007385, acc:0.906826\n","epoch:[119], loss:0.008432, acc:0.916293\n","epoch:[120], loss:0.007128, acc:0.919781\n","epoch:[121], loss:0.009616, acc:0.922770\n","epoch:[122], loss:0.006806, acc:0.921276\n","epoch:[123], loss:0.006696, acc:0.918784\n","epoch:[124], loss:0.006868, acc:0.921774\n","epoch:[125], loss:0.013107, acc:0.921774\n","epoch:[126], loss:0.006434, acc:0.922272\n","epoch:[127], loss:0.006419, acc:0.918784\n","epoch:[128], loss:0.006085, acc:0.920777\n","epoch:[129], loss:0.006464, acc:0.917788\n","epoch:[130], loss:0.008241, acc:0.921774\n","epoch:[131], loss:0.006186, acc:0.923767\n","epoch:[132], loss:0.007219, acc:0.919781\n","epoch:[133], loss:0.007037, acc:0.918286\n","epoch:[134], loss:0.006526, acc:0.922272\n","epoch:[135], loss:0.006743, acc:0.922770\n","epoch:[136], loss:0.006762, acc:0.922770\n","epoch:[137], loss:0.008510, acc:0.920279\n","epoch:[138], loss:0.007493, acc:0.917788\n","epoch:[139], loss:0.007039, acc:0.921276\n","epoch:[140], loss:0.006531, acc:0.921276\n","epoch:[141], loss:0.007317, acc:0.916293\n","epoch:[142], loss:0.007512, acc:0.920777\n","epoch:[143], loss:0.006602, acc:0.918286\n","epoch:[144], loss:0.006501, acc:0.915296\n","epoch:[145], loss:0.007074, acc:0.924265\n","epoch:[146], loss:0.006036, acc:0.915795\n","epoch:[147], loss:0.006488, acc:0.916791\n","epoch:[148], loss:0.006183, acc:0.914798\n","epoch:[149], loss:0.006572, acc:0.916293\n","epoch:[150], loss:0.007535, acc:0.918784\n","epoch:[151], loss:0.006353, acc:0.921276\n","epoch:[152], loss:0.006210, acc:0.915795\n","epoch:[153], loss:0.007303, acc:0.920777\n","epoch:[154], loss:0.007976, acc:0.923269\n","epoch:[155], loss:0.005882, acc:0.919283\n","epoch:[156], loss:0.006405, acc:0.915795\n","epoch:[157], loss:0.005993, acc:0.920777\n","epoch:[158], loss:0.006328, acc:0.923767\n","epoch:[159], loss:0.007659, acc:0.917788\n","epoch:[160], loss:0.007853, acc:0.924265\n","epoch:[161], loss:0.005868, acc:0.923767\n","epoch:[162], loss:0.005772, acc:0.922770\n","epoch:[163], loss:0.005898, acc:0.919283\n","epoch:[164], loss:0.006100, acc:0.922770\n","epoch:[165], loss:0.008027, acc:0.921276\n","epoch:[166], loss:0.005511, acc:0.916791\n","epoch:[167], loss:0.005822, acc:0.923269\n","epoch:[168], loss:0.006095, acc:0.917788\n","epoch:[169], loss:0.006245, acc:0.910314\n","epoch:[170], loss:0.007506, acc:0.919781\n","epoch:[171], loss:0.005976, acc:0.918784\n","epoch:[172], loss:0.005475, acc:0.924265\n","epoch:[173], loss:0.008130, acc:0.920777\n","epoch:[174], loss:0.010731, acc:0.921276\n","epoch:[175], loss:0.005627, acc:0.922770\n","epoch:[176], loss:0.005137, acc:0.919781\n","epoch:[177], loss:0.005494, acc:0.920777\n","epoch:[178], loss:0.005545, acc:0.919283\n","epoch:[179], loss:0.006143, acc:0.918784\n","epoch:[180], loss:0.008239, acc:0.914798\n","epoch:[181], loss:0.014496, acc:0.916791\n","epoch:[182], loss:0.005313, acc:0.917289\n","epoch:[183], loss:0.004830, acc:0.920279\n","epoch:[184], loss:0.004783, acc:0.920777\n","epoch:[185], loss:0.004853, acc:0.918286\n","epoch:[186], loss:0.005157, acc:0.920279\n","epoch:[187], loss:0.005873, acc:0.919781\n","epoch:[188], loss:0.006384, acc:0.916293\n","epoch:[189], loss:0.007429, acc:0.912805\n","epoch:[190], loss:0.007654, acc:0.907324\n","epoch:[191], loss:0.005739, acc:0.920777\n","epoch:[192], loss:0.005599, acc:0.916791\n","epoch:[193], loss:0.005405, acc:0.922272\n","epoch:[194], loss:0.005643, acc:0.911310\n","epoch:[195], loss:0.005516, acc:0.922272\n","epoch:[196], loss:0.005759, acc:0.919781\n","epoch:[197], loss:0.012777, acc:0.914798\n","epoch:[198], loss:0.011653, acc:0.920777\n","epoch:[199], loss:0.004886, acc:0.922272\n","epoch:[200], loss:0.004618, acc:0.923269\n","epoch:[201], loss:0.004738, acc:0.922272\n","epoch:[202], loss:0.004923, acc:0.924265\n","epoch:[203], loss:0.005538, acc:0.917788\n","epoch:[204], loss:0.005480, acc:0.917289\n","epoch:[205], loss:0.005906, acc:0.920279\n","epoch:[206], loss:0.006214, acc:0.919781\n","epoch:[207], loss:0.006015, acc:0.918286\n","epoch:[208], loss:0.006756, acc:0.920279\n","epoch:[209], loss:0.005712, acc:0.915296\n","epoch:[210], loss:0.005697, acc:0.923767\n","epoch:[211], loss:0.009338, acc:0.910812\n","epoch:[212], loss:0.008073, acc:0.921276\n","epoch:[213], loss:0.004855, acc:0.920777\n","epoch:[214], loss:0.004492, acc:0.921276\n","epoch:[215], loss:0.005026, acc:0.920777\n","epoch:[216], loss:0.004799, acc:0.919283\n","epoch:[217], loss:0.005792, acc:0.917289\n","epoch:[218], loss:0.006164, acc:0.914798\n","epoch:[219], loss:0.007051, acc:0.924763\n","epoch:[220], loss:0.005272, acc:0.920777\n","epoch:[221], loss:0.011725, acc:0.917788\n","epoch:[222], loss:0.012029, acc:0.923767\n","epoch:[223], loss:0.004513, acc:0.923767\n","epoch:[224], loss:0.004899, acc:0.923269\n","epoch:[225], loss:0.004566, acc:0.920279\n","epoch:[226], loss:0.004634, acc:0.922770\n","epoch:[227], loss:0.004891, acc:0.917788\n","epoch:[228], loss:0.005250, acc:0.925760\n","epoch:[229], loss:0.004865, acc:0.913802\n","epoch:[230], loss:0.005858, acc:0.915795\n","epoch:[231], loss:0.006283, acc:0.920279\n","epoch:[232], loss:0.006611, acc:0.918286\n","epoch:[233], loss:0.005704, acc:0.926258\n","epoch:[234], loss:0.005529, acc:0.917289\n","epoch:[235], loss:0.005280, acc:0.915795\n","epoch:[236], loss:0.005737, acc:0.922272\n","epoch:[237], loss:0.005550, acc:0.922272\n","epoch:[238], loss:0.005310, acc:0.922770\n","epoch:[239], loss:0.009835, acc:0.920279\n","epoch:[240], loss:0.006041, acc:0.916293\n","epoch:[241], loss:0.005429, acc:0.922272\n","epoch:[242], loss:0.005851, acc:0.922770\n","epoch:[243], loss:0.005265, acc:0.922272\n","epoch:[244], loss:0.005388, acc:0.917788\n","epoch:[245], loss:0.006253, acc:0.919781\n","epoch:[246], loss:0.005126, acc:0.915296\n","epoch:[247], loss:0.008836, acc:0.923767\n","epoch:[248], loss:0.004601, acc:0.924763\n","epoch:[249], loss:0.004743, acc:0.919283\n","epoch:[250], loss:0.004899, acc:0.920279\n","epoch:[251], loss:0.008131, acc:0.919781\n","epoch:[252], loss:0.006949, acc:0.923269\n","epoch:[253], loss:0.004791, acc:0.923767\n","epoch:[254], loss:0.005765, acc:0.918784\n","epoch:[255], loss:0.005040, acc:0.924265\n","epoch:[256], loss:0.004928, acc:0.920279\n","epoch:[257], loss:0.004824, acc:0.921774\n","epoch:[258], loss:0.005458, acc:0.919781\n","epoch:[259], loss:0.008835, acc:0.915795\n","epoch:[260], loss:0.006021, acc:0.921276\n","epoch:[261], loss:0.004949, acc:0.923767\n","epoch:[262], loss:0.004882, acc:0.919781\n","epoch:[263], loss:0.004795, acc:0.919781\n","epoch:[264], loss:0.006149, acc:0.925262\n","epoch:[265], loss:0.007259, acc:0.918286\n","epoch:[266], loss:0.005713, acc:0.924265\n","epoch:[267], loss:0.005456, acc:0.917788\n","epoch:[268], loss:0.005100, acc:0.918784\n","epoch:[269], loss:0.004761, acc:0.920777\n","epoch:[270], loss:0.009396, acc:0.919283\n","epoch:[271], loss:0.005387, acc:0.924265\n","epoch:[272], loss:0.004820, acc:0.918784\n","epoch:[273], loss:0.005029, acc:0.922770\n","epoch:[274], loss:0.004976, acc:0.921276\n","epoch:[275], loss:0.005635, acc:0.917289\n","epoch:[276], loss:0.005789, acc:0.922272\n","epoch:[277], loss:0.006306, acc:0.922770\n","epoch:[278], loss:0.005613, acc:0.922272\n","epoch:[279], loss:0.005636, acc:0.920777\n","epoch:[280], loss:0.005008, acc:0.919781\n","epoch:[281], loss:0.007031, acc:0.920777\n","epoch:[282], loss:0.006742, acc:0.923269\n","epoch:[283], loss:0.004935, acc:0.922272\n","epoch:[284], loss:0.004741, acc:0.920279\n","epoch:[285], loss:0.006979, acc:0.923269\n","epoch:[286], loss:0.005311, acc:0.924265\n","epoch:[287], loss:0.005166, acc:0.921774\n","epoch:[288], loss:0.005160, acc:0.923767\n","epoch:[289], loss:0.004769, acc:0.922770\n","epoch:[290], loss:0.005655, acc:0.923269\n","epoch:[291], loss:0.005478, acc:0.924265\n","epoch:[292], loss:0.013320, acc:0.915296\n","epoch:[293], loss:0.006050, acc:0.926258\n","epoch:[294], loss:0.004538, acc:0.926258\n","epoch:[295], loss:0.004714, acc:0.914300\n","epoch:[296], loss:0.005116, acc:0.923767\n","epoch:[297], loss:0.005204, acc:0.924265\n","epoch:[298], loss:0.004664, acc:0.920777\n","epoch:[299], loss:0.004853, acc:0.920777\n","epoch:[300], loss:0.008305, acc:0.922272\n","epoch:[301], loss:0.007398, acc:0.925760\n","epoch:[302], loss:0.004689, acc:0.924763\n","epoch:[303], loss:0.004629, acc:0.925760\n","epoch:[304], loss:0.005291, acc:0.923269\n","epoch:[305], loss:0.004873, acc:0.922272\n","epoch:[306], loss:0.005246, acc:0.923269\n","epoch:[307], loss:0.005674, acc:0.916791\n","epoch:[308], loss:0.004651, acc:0.921276\n","epoch:[309], loss:0.006293, acc:0.922272\n","epoch:[310], loss:0.007880, acc:0.921774\n","epoch:[311], loss:0.004862, acc:0.917788\n","epoch:[312], loss:0.005079, acc:0.924265\n","epoch:[313], loss:0.005073, acc:0.918286\n","epoch:[314], loss:0.005573, acc:0.921276\n","epoch:[315], loss:0.005849, acc:0.915795\n","epoch:[316], loss:0.007750, acc:0.920777\n","epoch:[317], loss:0.004561, acc:0.923269\n","epoch:[318], loss:0.004900, acc:0.922272\n","epoch:[319], loss:0.004735, acc:0.921774\n","epoch:[320], loss:0.005179, acc:0.922770\n","epoch:[321], loss:0.006338, acc:0.924265\n","epoch:[322], loss:0.004821, acc:0.921276\n","epoch:[323], loss:0.005687, acc:0.924265\n","epoch:[324], loss:0.005789, acc:0.922770\n","epoch:[325], loss:0.005158, acc:0.923767\n","epoch:[326], loss:0.006950, acc:0.922770\n","epoch:[327], loss:0.005535, acc:0.924265\n","epoch:[328], loss:0.004956, acc:0.921276\n","epoch:[329], loss:0.005021, acc:0.922770\n","epoch:[330], loss:0.005060, acc:0.924265\n","epoch:[331], loss:0.005266, acc:0.920777\n","epoch:[332], loss:0.005035, acc:0.924763\n","epoch:[333], loss:0.005673, acc:0.921774\n","epoch:[334], loss:0.007727, acc:0.920279\n","epoch:[335], loss:0.004866, acc:0.922770\n","epoch:[336], loss:0.005071, acc:0.922770\n","epoch:[337], loss:0.004870, acc:0.921774\n","epoch:[338], loss:0.004549, acc:0.923767\n","epoch:[339], loss:0.005233, acc:0.918784\n","epoch:[340], loss:0.005644, acc:0.922770\n","epoch:[341], loss:0.007722, acc:0.918286\n","epoch:[342], loss:0.004916, acc:0.923767\n","epoch:[343], loss:0.004978, acc:0.923269\n","epoch:[344], loss:0.011568, acc:0.922272\n","epoch:[345], loss:0.005784, acc:0.924265\n","epoch:[346], loss:0.004162, acc:0.923767\n","epoch:[347], loss:0.004114, acc:0.925262\n","epoch:[348], loss:0.004246, acc:0.923767\n","epoch:[349], loss:0.004732, acc:0.924265\n","epoch:[350], loss:0.004722, acc:0.922272\n","epoch:[351], loss:0.004830, acc:0.923767\n","epoch:[352], loss:0.005250, acc:0.918784\n","epoch:[353], loss:0.006316, acc:0.922272\n","epoch:[354], loss:0.005010, acc:0.918286\n","epoch:[355], loss:0.005519, acc:0.920777\n","epoch:[356], loss:0.005157, acc:0.922770\n","epoch:[357], loss:0.006253, acc:0.917788\n","epoch:[358], loss:0.005374, acc:0.926258\n","epoch:[359], loss:0.004554, acc:0.925760\n","epoch:[360], loss:0.004828, acc:0.922770\n","epoch:[361], loss:0.005496, acc:0.923269\n","epoch:[362], loss:0.005267, acc:0.926258\n","epoch:[363], loss:0.008789, acc:0.921774\n","epoch:[364], loss:0.006872, acc:0.923767\n","epoch:[365], loss:0.004435, acc:0.924265\n","epoch:[366], loss:0.004299, acc:0.921276\n","epoch:[367], loss:0.004468, acc:0.924763\n","epoch:[368], loss:0.004760, acc:0.926756\n","epoch:[369], loss:0.004463, acc:0.923767\n","epoch:[370], loss:0.004829, acc:0.921276\n","epoch:[371], loss:0.007316, acc:0.917788\n","epoch:[372], loss:0.005516, acc:0.919283\n","epoch:[373], loss:0.005989, acc:0.921276\n","epoch:[374], loss:0.004795, acc:0.921774\n","epoch:[375], loss:0.005383, acc:0.922272\n","epoch:[376], loss:0.005938, acc:0.917289\n","epoch:[377], loss:0.004499, acc:0.923269\n","epoch:[378], loss:0.005126, acc:0.923269\n","epoch:[379], loss:0.004774, acc:0.925760\n","epoch:[380], loss:0.005239, acc:0.915296\n","epoch:[381], loss:0.016653, acc:0.921774\n","epoch:[382], loss:0.004566, acc:0.923767\n","epoch:[383], loss:0.003948, acc:0.923767\n","epoch:[384], loss:0.004026, acc:0.926258\n","epoch:[385], loss:0.004041, acc:0.922770\n","epoch:[386], loss:0.004870, acc:0.922770\n","epoch:[387], loss:0.004590, acc:0.924265\n","epoch:[388], loss:0.004833, acc:0.923767\n","epoch:[389], loss:0.004496, acc:0.920279\n","epoch:[390], loss:0.004953, acc:0.919781\n","epoch:[391], loss:0.005644, acc:0.925262\n","epoch:[392], loss:0.005120, acc:0.926258\n","epoch:[393], loss:0.007361, acc:0.921774\n","epoch:[394], loss:0.005501, acc:0.922272\n","epoch:[395], loss:0.004793, acc:0.923269\n","epoch:[396], loss:0.006369, acc:0.923767\n","epoch:[397], loss:0.004450, acc:0.921774\n","epoch:[398], loss:0.004647, acc:0.924763\n","epoch:[399], loss:0.005819, acc:0.924265\n","epoch:[400], loss:0.005083, acc:0.924265\n","epoch:[401], loss:0.008307, acc:0.925262\n","epoch:[402], loss:0.004468, acc:0.925262\n","epoch:[403], loss:0.004609, acc:0.927753\n","epoch:[404], loss:0.004447, acc:0.924763\n","epoch:[405], loss:0.006797, acc:0.918286\n","epoch:[406], loss:0.005370, acc:0.924265\n","epoch:[407], loss:0.004502, acc:0.922272\n","epoch:[408], loss:0.006013, acc:0.923269\n","epoch:[409], loss:0.004415, acc:0.924265\n","epoch:[410], loss:0.004663, acc:0.924265\n","epoch:[411], loss:0.005012, acc:0.925760\n","epoch:[412], loss:0.006002, acc:0.927255\n","epoch:[413], loss:0.004729, acc:0.925760\n","epoch:[414], loss:0.007182, acc:0.921276\n","epoch:[415], loss:0.005746, acc:0.926258\n","epoch:[416], loss:0.004702, acc:0.922770\n","epoch:[417], loss:0.004611, acc:0.922770\n","epoch:[418], loss:0.004940, acc:0.914300\n","epoch:[419], loss:0.006809, acc:0.926756\n","epoch:[420], loss:0.004585, acc:0.924763\n","epoch:[421], loss:0.005005, acc:0.924763\n","epoch:[422], loss:0.009202, acc:0.924265\n","epoch:[423], loss:0.004047, acc:0.922272\n","epoch:[424], loss:0.004277, acc:0.926258\n","epoch:[425], loss:0.004424, acc:0.924763\n","epoch:[426], loss:0.004611, acc:0.924763\n","epoch:[427], loss:0.005057, acc:0.926258\n","epoch:[428], loss:0.008613, acc:0.922770\n","epoch:[429], loss:0.004973, acc:0.915296\n","epoch:[430], loss:0.005429, acc:0.921276\n","epoch:[431], loss:0.004358, acc:0.925262\n","epoch:[432], loss:0.004548, acc:0.928251\n","epoch:[433], loss:0.004460, acc:0.921774\n","epoch:[434], loss:0.006299, acc:0.923767\n","epoch:[435], loss:0.005198, acc:0.922770\n","epoch:[436], loss:0.005322, acc:0.925760\n","epoch:[437], loss:0.005225, acc:0.927753\n","epoch:[438], loss:0.004325, acc:0.926756\n","epoch:[439], loss:0.005319, acc:0.924265\n","epoch:[440], loss:0.004635, acc:0.923767\n","epoch:[441], loss:0.010089, acc:0.922770\n","epoch:[442], loss:0.010190, acc:0.914300\n","epoch:[443], loss:0.004638, acc:0.922272\n","epoch:[444], loss:0.003696, acc:0.923269\n","epoch:[445], loss:0.003891, acc:0.925262\n","epoch:[446], loss:0.004263, acc:0.924265\n","epoch:[447], loss:0.004583, acc:0.923767\n","epoch:[448], loss:0.005009, acc:0.922272\n","epoch:[449], loss:0.004767, acc:0.920279\n","epoch:[450], loss:0.004771, acc:0.924265\n","epoch:[451], loss:0.005469, acc:0.920777\n","epoch:[452], loss:0.005096, acc:0.920279\n","epoch:[453], loss:0.005021, acc:0.926258\n","epoch:[454], loss:0.004740, acc:0.921774\n","epoch:[455], loss:0.006561, acc:0.925760\n","epoch:[456], loss:0.004764, acc:0.923767\n","epoch:[457], loss:0.005234, acc:0.921774\n","epoch:[458], loss:0.007206, acc:0.920777\n","epoch:[459], loss:0.005055, acc:0.927753\n","epoch:[460], loss:0.004324, acc:0.924265\n","epoch:[461], loss:0.004423, acc:0.921276\n","epoch:[462], loss:0.004999, acc:0.924265\n","epoch:[463], loss:0.004587, acc:0.919283\n","epoch:[464], loss:0.005170, acc:0.924265\n","epoch:[465], loss:0.005815, acc:0.926756\n","epoch:[466], loss:0.004828, acc:0.926258\n","epoch:[467], loss:0.006484, acc:0.921774\n","epoch:[468], loss:0.004668, acc:0.927255\n","epoch:[469], loss:0.004426, acc:0.928251\n","epoch:[470], loss:0.005709, acc:0.926258\n","epoch:[471], loss:0.005712, acc:0.920777\n","epoch:[472], loss:0.006042, acc:0.924763\n","epoch:[473], loss:0.004377, acc:0.927255\n","epoch:[474], loss:0.004711, acc:0.924763\n","epoch:[475], loss:0.007108, acc:0.922770\n","epoch:[476], loss:0.004430, acc:0.922272\n","epoch:[477], loss:0.004234, acc:0.924763\n","epoch:[478], loss:0.004975, acc:0.922770\n","epoch:[479], loss:0.004685, acc:0.923269\n","epoch:[480], loss:0.006956, acc:0.920279\n","epoch:[481], loss:0.004360, acc:0.923269\n","epoch:[482], loss:0.004429, acc:0.919781\n","epoch:[483], loss:0.005411, acc:0.922770\n","epoch:[484], loss:0.004887, acc:0.919283\n","epoch:[485], loss:0.005275, acc:0.921774\n","epoch:[486], loss:0.004672, acc:0.923269\n","epoch:[487], loss:0.011579, acc:0.924265\n","epoch:[488], loss:0.004887, acc:0.925760\n","epoch:[489], loss:0.003925, acc:0.926756\n","epoch:[490], loss:0.003812, acc:0.922770\n","epoch:[491], loss:0.004266, acc:0.924265\n","epoch:[492], loss:0.004814, acc:0.926258\n","epoch:[493], loss:0.004471, acc:0.924265\n","epoch:[494], loss:0.004827, acc:0.919781\n","epoch:[495], loss:0.004543, acc:0.923269\n","epoch:[496], loss:0.005323, acc:0.924265\n","epoch:[497], loss:0.005332, acc:0.928251\n","epoch:[498], loss:0.006431, acc:0.926258\n","epoch:[499], loss:0.004781, acc:0.924763\n"]}]},{"cell_type":"markdown","source":["**server same train**"],"metadata":{"id":"rZSfcmIxIc3B"}},{"cell_type":"code","source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","E_optimizer = optim.Adam(get_params(net, [\"extractor\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","for epoch in range(500):\n","    # train\n","    frozen_net(net, [\"extractor\"], False)\n","    losses, batch = 0., 0\n","    for x, y in client_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        E_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        E_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_trainloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/server_same_extractor.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIeNU2djIfUx","executionInfo":{"status":"ok","timestamp":1640311762539,"user_tz":-480,"elapsed":885216,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"6411ff9a-1481-49c8-dbc8-25cdccd0a51c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:2.823008, acc:0.228000\n","epoch:[ 1], loss:1.291809, acc:0.771500\n","epoch:[ 2], loss:0.637843, acc:0.861500\n","epoch:[ 3], loss:0.463013, acc:0.898500\n","epoch:[ 4], loss:0.380704, acc:0.912500\n","epoch:[ 5], loss:0.327878, acc:0.918500\n","epoch:[ 6], loss:0.286583, acc:0.933000\n","epoch:[ 7], loss:0.255209, acc:0.937500\n","epoch:[ 8], loss:0.225938, acc:0.934500\n","epoch:[ 9], loss:0.203992, acc:0.951500\n","epoch:[10], loss:0.184406, acc:0.956000\n","epoch:[11], loss:0.166611, acc:0.956000\n","epoch:[12], loss:0.152320, acc:0.964000\n","epoch:[13], loss:0.136847, acc:0.967500\n","epoch:[14], loss:0.125411, acc:0.968000\n","epoch:[15], loss:0.114898, acc:0.974000\n","epoch:[16], loss:0.105867, acc:0.974000\n","epoch:[17], loss:0.097604, acc:0.975500\n","epoch:[18], loss:0.089657, acc:0.978000\n","epoch:[19], loss:0.082775, acc:0.982000\n","epoch:[20], loss:0.076285, acc:0.983500\n","epoch:[21], loss:0.069355, acc:0.985000\n","epoch:[22], loss:0.064773, acc:0.987000\n","epoch:[23], loss:0.062098, acc:0.991500\n","epoch:[24], loss:0.056152, acc:0.993000\n","epoch:[25], loss:0.052892, acc:0.992500\n","epoch:[26], loss:0.049622, acc:0.993500\n","epoch:[27], loss:0.045812, acc:0.995000\n","epoch:[28], loss:0.042546, acc:0.996000\n","epoch:[29], loss:0.040019, acc:0.996000\n","epoch:[30], loss:0.037708, acc:0.996500\n","epoch:[31], loss:0.035396, acc:0.996500\n","epoch:[32], loss:0.033405, acc:0.998000\n","epoch:[33], loss:0.031788, acc:0.996500\n","epoch:[34], loss:0.029743, acc:0.997500\n","epoch:[35], loss:0.028267, acc:0.998000\n","epoch:[36], loss:0.025476, acc:0.998000\n","epoch:[37], loss:0.024869, acc:0.999000\n","epoch:[38], loss:0.024185, acc:0.999000\n","epoch:[39], loss:0.022697, acc:0.998500\n","epoch:[40], loss:0.022079, acc:0.998500\n","epoch:[41], loss:0.020255, acc:0.999000\n","epoch:[42], loss:0.019333, acc:1.000000\n","epoch:[43], loss:0.018428, acc:1.000000\n","epoch:[44], loss:0.017426, acc:0.999500\n","epoch:[45], loss:0.016785, acc:0.999500\n","epoch:[46], loss:0.016208, acc:1.000000\n","epoch:[47], loss:0.015656, acc:1.000000\n","epoch:[48], loss:0.014986, acc:1.000000\n","epoch:[49], loss:0.014306, acc:1.000000\n","epoch:[50], loss:0.013657, acc:0.999500\n","epoch:[51], loss:0.013352, acc:1.000000\n","epoch:[52], loss:0.012600, acc:1.000000\n","epoch:[53], loss:0.012385, acc:1.000000\n","epoch:[54], loss:0.011685, acc:1.000000\n","epoch:[55], loss:0.011179, acc:1.000000\n","epoch:[56], loss:0.011108, acc:1.000000\n","epoch:[57], loss:0.010585, acc:1.000000\n","epoch:[58], loss:0.010202, acc:1.000000\n","epoch:[59], loss:0.009927, acc:1.000000\n","epoch:[60], loss:0.009791, acc:1.000000\n","epoch:[61], loss:0.009352, acc:1.000000\n","epoch:[62], loss:0.009228, acc:1.000000\n","epoch:[63], loss:0.008814, acc:1.000000\n","epoch:[64], loss:0.008585, acc:1.000000\n","epoch:[65], loss:0.008291, acc:1.000000\n","epoch:[66], loss:0.008171, acc:1.000000\n","epoch:[67], loss:0.008012, acc:1.000000\n","epoch:[68], loss:0.007866, acc:1.000000\n","epoch:[69], loss:0.007607, acc:1.000000\n","epoch:[70], loss:0.007434, acc:1.000000\n","epoch:[71], loss:0.007233, acc:1.000000\n","epoch:[72], loss:0.007030, acc:1.000000\n","epoch:[73], loss:0.006917, acc:1.000000\n","epoch:[74], loss:0.006924, acc:1.000000\n","epoch:[75], loss:0.006692, acc:1.000000\n","epoch:[76], loss:0.006506, acc:1.000000\n","epoch:[77], loss:0.006566, acc:1.000000\n","epoch:[78], loss:0.006286, acc:1.000000\n","epoch:[79], loss:0.006224, acc:1.000000\n","epoch:[80], loss:0.006112, acc:1.000000\n","epoch:[81], loss:0.006142, acc:1.000000\n","epoch:[82], loss:0.005952, acc:1.000000\n","epoch:[83], loss:0.005918, acc:1.000000\n","epoch:[84], loss:0.005847, acc:1.000000\n","epoch:[85], loss:0.005763, acc:1.000000\n","epoch:[86], loss:0.005635, acc:1.000000\n","epoch:[87], loss:0.005577, acc:1.000000\n","epoch:[88], loss:0.005551, acc:1.000000\n","epoch:[89], loss:0.005499, acc:1.000000\n","epoch:[90], loss:0.005383, acc:1.000000\n","epoch:[91], loss:0.005383, acc:1.000000\n","epoch:[92], loss:0.005279, acc:1.000000\n","epoch:[93], loss:0.005337, acc:1.000000\n","epoch:[94], loss:0.005190, acc:1.000000\n","epoch:[95], loss:0.005192, acc:1.000000\n","epoch:[96], loss:0.005130, acc:1.000000\n","epoch:[97], loss:0.005121, acc:1.000000\n","epoch:[98], loss:0.005130, acc:1.000000\n","epoch:[99], loss:0.005026, acc:1.000000\n","epoch:[100], loss:0.004959, acc:1.000000\n","epoch:[101], loss:0.004885, acc:1.000000\n","epoch:[102], loss:0.004902, acc:1.000000\n","epoch:[103], loss:0.004889, acc:1.000000\n","epoch:[104], loss:0.004803, acc:1.000000\n","epoch:[105], loss:0.004910, acc:1.000000\n","epoch:[106], loss:0.004806, acc:1.000000\n","epoch:[107], loss:0.004771, acc:1.000000\n","epoch:[108], loss:0.004797, acc:1.000000\n","epoch:[109], loss:0.004729, acc:1.000000\n","epoch:[110], loss:0.004721, acc:1.000000\n","epoch:[111], loss:0.004705, acc:1.000000\n","epoch:[112], loss:0.004630, acc:1.000000\n","epoch:[113], loss:0.004665, acc:1.000000\n","epoch:[114], loss:0.004609, acc:1.000000\n","epoch:[115], loss:0.004630, acc:1.000000\n","epoch:[116], loss:0.004630, acc:1.000000\n","epoch:[117], loss:0.004580, acc:1.000000\n","epoch:[118], loss:0.004633, acc:1.000000\n","epoch:[119], loss:0.004546, acc:1.000000\n","epoch:[120], loss:0.004590, acc:1.000000\n","epoch:[121], loss:0.004562, acc:1.000000\n","epoch:[122], loss:0.004558, acc:1.000000\n","epoch:[123], loss:0.004462, acc:1.000000\n","epoch:[124], loss:0.004579, acc:1.000000\n","epoch:[125], loss:0.004463, acc:1.000000\n","epoch:[126], loss:0.004522, acc:1.000000\n","epoch:[127], loss:0.004554, acc:1.000000\n","epoch:[128], loss:0.004417, acc:1.000000\n","epoch:[129], loss:0.004420, acc:1.000000\n","epoch:[130], loss:0.004437, acc:1.000000\n","epoch:[131], loss:0.004421, acc:1.000000\n","epoch:[132], loss:0.004388, acc:1.000000\n","epoch:[133], loss:0.004432, acc:1.000000\n","epoch:[134], loss:0.004382, acc:1.000000\n","epoch:[135], loss:0.004382, acc:1.000000\n","epoch:[136], loss:0.004394, acc:1.000000\n","epoch:[137], loss:0.004448, acc:1.000000\n","epoch:[138], loss:0.004311, acc:1.000000\n","epoch:[139], loss:0.004453, acc:1.000000\n","epoch:[140], loss:0.004368, acc:1.000000\n","epoch:[141], loss:0.004381, acc:1.000000\n","epoch:[142], loss:0.004368, acc:1.000000\n","epoch:[143], loss:0.004371, acc:1.000000\n","epoch:[144], loss:0.004329, acc:1.000000\n","epoch:[145], loss:0.004318, acc:1.000000\n","epoch:[146], loss:0.004348, acc:1.000000\n","epoch:[147], loss:0.004314, acc:1.000000\n","epoch:[148], loss:0.004324, acc:1.000000\n","epoch:[149], loss:0.004300, acc:1.000000\n","epoch:[150], loss:0.004294, acc:1.000000\n","epoch:[151], loss:0.004266, acc:1.000000\n","epoch:[152], loss:0.004292, acc:1.000000\n","epoch:[153], loss:0.004244, acc:1.000000\n","epoch:[154], loss:0.004278, acc:1.000000\n","epoch:[155], loss:0.004282, acc:1.000000\n","epoch:[156], loss:0.004255, acc:1.000000\n","epoch:[157], loss:0.004273, acc:1.000000\n","epoch:[158], loss:0.004341, acc:1.000000\n","epoch:[159], loss:0.004272, acc:1.000000\n","epoch:[160], loss:0.004288, acc:1.000000\n","epoch:[161], loss:0.004280, acc:1.000000\n","epoch:[162], loss:0.004240, acc:1.000000\n","epoch:[163], loss:0.004241, acc:1.000000\n","epoch:[164], loss:0.004256, acc:1.000000\n","epoch:[165], loss:0.004234, acc:1.000000\n","epoch:[166], loss:0.004228, acc:1.000000\n","epoch:[167], loss:0.004240, acc:1.000000\n","epoch:[168], loss:0.004209, acc:1.000000\n","epoch:[169], loss:0.004250, acc:1.000000\n","epoch:[170], loss:0.004239, acc:1.000000\n","epoch:[171], loss:0.004245, acc:1.000000\n","epoch:[172], loss:0.004172, acc:1.000000\n","epoch:[173], loss:0.004201, acc:1.000000\n","epoch:[174], loss:0.004224, acc:1.000000\n","epoch:[175], loss:0.004207, acc:1.000000\n","epoch:[176], loss:0.004221, acc:1.000000\n","epoch:[177], loss:0.004188, acc:1.000000\n","epoch:[178], loss:0.004228, acc:1.000000\n","epoch:[179], loss:0.004175, acc:1.000000\n","epoch:[180], loss:0.004188, acc:1.000000\n","epoch:[181], loss:0.004249, acc:1.000000\n","epoch:[182], loss:0.004202, acc:1.000000\n","epoch:[183], loss:0.004188, acc:1.000000\n","epoch:[184], loss:0.004264, acc:1.000000\n","epoch:[185], loss:0.004212, acc:1.000000\n","epoch:[186], loss:0.004136, acc:1.000000\n","epoch:[187], loss:0.004203, acc:1.000000\n","epoch:[188], loss:0.004214, acc:1.000000\n","epoch:[189], loss:0.004158, acc:1.000000\n","epoch:[190], loss:0.004204, acc:1.000000\n","epoch:[191], loss:0.004206, acc:1.000000\n","epoch:[192], loss:0.004170, acc:1.000000\n","epoch:[193], loss:0.004164, acc:1.000000\n","epoch:[194], loss:0.004185, acc:1.000000\n","epoch:[195], loss:0.004118, acc:1.000000\n","epoch:[196], loss:0.004175, acc:1.000000\n","epoch:[197], loss:0.004258, acc:1.000000\n","epoch:[198], loss:0.004144, acc:1.000000\n","epoch:[199], loss:0.004142, acc:1.000000\n","epoch:[200], loss:0.004160, acc:1.000000\n","epoch:[201], loss:0.004129, acc:1.000000\n","epoch:[202], loss:0.004228, acc:1.000000\n","epoch:[203], loss:0.004073, acc:1.000000\n","epoch:[204], loss:0.004200, acc:1.000000\n","epoch:[205], loss:0.004170, acc:1.000000\n","epoch:[206], loss:0.004128, acc:1.000000\n","epoch:[207], loss:0.004148, acc:1.000000\n","epoch:[208], loss:0.004125, acc:1.000000\n","epoch:[209], loss:0.004138, acc:1.000000\n","epoch:[210], loss:0.004125, acc:1.000000\n","epoch:[211], loss:0.004164, acc:1.000000\n","epoch:[212], loss:0.004223, acc:1.000000\n","epoch:[213], loss:0.004147, acc:1.000000\n","epoch:[214], loss:0.004136, acc:1.000000\n","epoch:[215], loss:0.004074, acc:1.000000\n","epoch:[216], loss:0.004123, acc:1.000000\n","epoch:[217], loss:0.004169, acc:1.000000\n","epoch:[218], loss:0.004127, acc:1.000000\n","epoch:[219], loss:0.004123, acc:1.000000\n","epoch:[220], loss:0.004192, acc:1.000000\n","epoch:[221], loss:0.004144, acc:1.000000\n","epoch:[222], loss:0.004154, acc:1.000000\n","epoch:[223], loss:0.004097, acc:1.000000\n","epoch:[224], loss:0.004094, acc:1.000000\n","epoch:[225], loss:0.004070, acc:1.000000\n","epoch:[226], loss:0.004113, acc:1.000000\n","epoch:[227], loss:0.004115, acc:1.000000\n","epoch:[228], loss:0.004134, acc:1.000000\n","epoch:[229], loss:0.004132, acc:1.000000\n","epoch:[230], loss:0.004131, acc:1.000000\n","epoch:[231], loss:0.004178, acc:1.000000\n","epoch:[232], loss:0.004092, acc:1.000000\n","epoch:[233], loss:0.004100, acc:1.000000\n","epoch:[234], loss:0.004073, acc:1.000000\n","epoch:[235], loss:0.004120, acc:1.000000\n","epoch:[236], loss:0.004134, acc:1.000000\n","epoch:[237], loss:0.004104, acc:1.000000\n","epoch:[238], loss:0.004115, acc:1.000000\n","epoch:[239], loss:0.004108, acc:1.000000\n","epoch:[240], loss:0.004062, acc:1.000000\n","epoch:[241], loss:0.004114, acc:1.000000\n","epoch:[242], loss:0.004101, acc:1.000000\n","epoch:[243], loss:0.004082, acc:1.000000\n","epoch:[244], loss:0.004129, acc:1.000000\n","epoch:[245], loss:0.004115, acc:1.000000\n","epoch:[246], loss:0.004107, acc:1.000000\n","epoch:[247], loss:0.004099, acc:1.000000\n","epoch:[248], loss:0.004159, acc:1.000000\n","epoch:[249], loss:0.004088, acc:1.000000\n","epoch:[250], loss:0.004060, acc:1.000000\n","epoch:[251], loss:0.004138, acc:1.000000\n","epoch:[252], loss:0.004133, acc:1.000000\n","epoch:[253], loss:0.004075, acc:1.000000\n","epoch:[254], loss:0.004097, acc:1.000000\n","epoch:[255], loss:0.004042, acc:1.000000\n","epoch:[256], loss:0.004148, acc:1.000000\n","epoch:[257], loss:0.004093, acc:1.000000\n","epoch:[258], loss:0.004083, acc:1.000000\n","epoch:[259], loss:0.004068, acc:1.000000\n","epoch:[260], loss:0.004068, acc:1.000000\n","epoch:[261], loss:0.004079, acc:1.000000\n","epoch:[262], loss:0.004097, acc:1.000000\n","epoch:[263], loss:0.004086, acc:1.000000\n","epoch:[264], loss:0.004062, acc:1.000000\n","epoch:[265], loss:0.004068, acc:1.000000\n","epoch:[266], loss:0.004073, acc:1.000000\n","epoch:[267], loss:0.004066, acc:1.000000\n","epoch:[268], loss:0.004045, acc:1.000000\n","epoch:[269], loss:0.004100, acc:1.000000\n","epoch:[270], loss:0.004045, acc:1.000000\n","epoch:[271], loss:0.004063, acc:1.000000\n","epoch:[272], loss:0.004054, acc:1.000000\n","epoch:[273], loss:0.004069, acc:1.000000\n","epoch:[274], loss:0.004056, acc:1.000000\n","epoch:[275], loss:0.004063, acc:1.000000\n","epoch:[276], loss:0.004106, acc:1.000000\n","epoch:[277], loss:0.004065, acc:1.000000\n","epoch:[278], loss:0.004116, acc:1.000000\n","epoch:[279], loss:0.004096, acc:1.000000\n","epoch:[280], loss:0.004117, acc:1.000000\n","epoch:[281], loss:0.004051, acc:1.000000\n","epoch:[282], loss:0.004107, acc:1.000000\n","epoch:[283], loss:0.004095, acc:1.000000\n","epoch:[284], loss:0.004064, acc:1.000000\n","epoch:[285], loss:0.004069, acc:1.000000\n","epoch:[286], loss:0.004086, acc:1.000000\n","epoch:[287], loss:0.004052, acc:1.000000\n","epoch:[288], loss:0.004074, acc:1.000000\n","epoch:[289], loss:0.004032, acc:1.000000\n","epoch:[290], loss:0.004044, acc:1.000000\n","epoch:[291], loss:0.004053, acc:1.000000\n","epoch:[292], loss:0.004040, acc:1.000000\n","epoch:[293], loss:0.004098, acc:1.000000\n","epoch:[294], loss:0.004078, acc:1.000000\n","epoch:[295], loss:0.004057, acc:1.000000\n","epoch:[296], loss:0.004133, acc:1.000000\n","epoch:[297], loss:0.004080, acc:1.000000\n","epoch:[298], loss:0.003998, acc:1.000000\n","epoch:[299], loss:0.004022, acc:1.000000\n","epoch:[300], loss:0.004079, acc:1.000000\n","epoch:[301], loss:0.004060, acc:1.000000\n","epoch:[302], loss:0.004079, acc:1.000000\n","epoch:[303], loss:0.004001, acc:1.000000\n","epoch:[304], loss:0.004071, acc:1.000000\n","epoch:[305], loss:0.004056, acc:1.000000\n","epoch:[306], loss:0.004085, acc:1.000000\n","epoch:[307], loss:0.004085, acc:1.000000\n","epoch:[308], loss:0.004113, acc:1.000000\n","epoch:[309], loss:0.004096, acc:1.000000\n","epoch:[310], loss:0.004055, acc:1.000000\n","epoch:[311], loss:0.004082, acc:1.000000\n","epoch:[312], loss:0.004086, acc:1.000000\n","epoch:[313], loss:0.004061, acc:1.000000\n","epoch:[314], loss:0.004043, acc:1.000000\n","epoch:[315], loss:0.004078, acc:1.000000\n","epoch:[316], loss:0.004107, acc:1.000000\n","epoch:[317], loss:0.004041, acc:1.000000\n","epoch:[318], loss:0.004037, acc:1.000000\n","epoch:[319], loss:0.004014, acc:1.000000\n","epoch:[320], loss:0.004048, acc:1.000000\n","epoch:[321], loss:0.004080, acc:1.000000\n","epoch:[322], loss:0.004039, acc:1.000000\n","epoch:[323], loss:0.004093, acc:1.000000\n","epoch:[324], loss:0.004102, acc:1.000000\n","epoch:[325], loss:0.004038, acc:1.000000\n","epoch:[326], loss:0.004011, acc:1.000000\n","epoch:[327], loss:0.004032, acc:1.000000\n","epoch:[328], loss:0.004095, acc:1.000000\n","epoch:[329], loss:0.004081, acc:1.000000\n","epoch:[330], loss:0.004024, acc:1.000000\n","epoch:[331], loss:0.004032, acc:1.000000\n","epoch:[332], loss:0.004045, acc:1.000000\n","epoch:[333], loss:0.004003, acc:1.000000\n","epoch:[334], loss:0.004007, acc:1.000000\n","epoch:[335], loss:0.004107, acc:1.000000\n","epoch:[336], loss:0.004039, acc:1.000000\n","epoch:[337], loss:0.004063, acc:1.000000\n","epoch:[338], loss:0.004076, acc:1.000000\n","epoch:[339], loss:0.004038, acc:1.000000\n","epoch:[340], loss:0.004045, acc:1.000000\n","epoch:[341], loss:0.004026, acc:1.000000\n","epoch:[342], loss:0.004067, acc:1.000000\n","epoch:[343], loss:0.004030, acc:1.000000\n","epoch:[344], loss:0.003995, acc:1.000000\n","epoch:[345], loss:0.004049, acc:1.000000\n","epoch:[346], loss:0.004028, acc:1.000000\n","epoch:[347], loss:0.004063, acc:1.000000\n","epoch:[348], loss:0.004061, acc:1.000000\n","epoch:[349], loss:0.004070, acc:1.000000\n","epoch:[350], loss:0.004020, acc:1.000000\n","epoch:[351], loss:0.004027, acc:1.000000\n","epoch:[352], loss:0.004032, acc:1.000000\n","epoch:[353], loss:0.004067, acc:1.000000\n","epoch:[354], loss:0.004005, acc:1.000000\n","epoch:[355], loss:0.004042, acc:1.000000\n","epoch:[356], loss:0.004013, acc:1.000000\n","epoch:[357], loss:0.004020, acc:1.000000\n","epoch:[358], loss:0.004056, acc:1.000000\n","epoch:[359], loss:0.004054, acc:1.000000\n","epoch:[360], loss:0.004037, acc:1.000000\n","epoch:[361], loss:0.004070, acc:1.000000\n","epoch:[362], loss:0.004004, acc:1.000000\n","epoch:[363], loss:0.004017, acc:1.000000\n","epoch:[364], loss:0.004109, acc:1.000000\n","epoch:[365], loss:0.004031, acc:1.000000\n","epoch:[366], loss:0.004033, acc:1.000000\n","epoch:[367], loss:0.004043, acc:1.000000\n","epoch:[368], loss:0.004036, acc:1.000000\n","epoch:[369], loss:0.004006, acc:1.000000\n","epoch:[370], loss:0.003964, acc:1.000000\n","epoch:[371], loss:0.004042, acc:1.000000\n","epoch:[372], loss:0.004054, acc:1.000000\n","epoch:[373], loss:0.004116, acc:1.000000\n","epoch:[374], loss:0.004011, acc:1.000000\n","epoch:[375], loss:0.004069, acc:1.000000\n","epoch:[376], loss:0.004020, acc:1.000000\n","epoch:[377], loss:0.004013, acc:1.000000\n","epoch:[378], loss:0.004032, acc:1.000000\n","epoch:[379], loss:0.004051, acc:1.000000\n","epoch:[380], loss:0.004084, acc:1.000000\n","epoch:[381], loss:0.004063, acc:1.000000\n","epoch:[382], loss:0.004009, acc:1.000000\n","epoch:[383], loss:0.004014, acc:1.000000\n","epoch:[384], loss:0.004006, acc:1.000000\n","epoch:[385], loss:0.004037, acc:1.000000\n","epoch:[386], loss:0.004050, acc:1.000000\n","epoch:[387], loss:0.004045, acc:1.000000\n","epoch:[388], loss:0.004010, acc:1.000000\n","epoch:[389], loss:0.003995, acc:1.000000\n","epoch:[390], loss:0.004002, acc:1.000000\n","epoch:[391], loss:0.004046, acc:1.000000\n","epoch:[392], loss:0.004018, acc:1.000000\n","epoch:[393], loss:0.003981, acc:1.000000\n","epoch:[394], loss:0.004053, acc:1.000000\n","epoch:[395], loss:0.004058, acc:1.000000\n","epoch:[396], loss:0.004023, acc:1.000000\n","epoch:[397], loss:0.004064, acc:1.000000\n","epoch:[398], loss:0.004074, acc:1.000000\n","epoch:[399], loss:0.004079, acc:1.000000\n","epoch:[400], loss:0.003997, acc:1.000000\n","epoch:[401], loss:0.004046, acc:1.000000\n","epoch:[402], loss:0.004038, acc:1.000000\n","epoch:[403], loss:0.004008, acc:1.000000\n","epoch:[404], loss:0.004037, acc:1.000000\n","epoch:[405], loss:0.004051, acc:1.000000\n","epoch:[406], loss:0.003984, acc:1.000000\n","epoch:[407], loss:0.004079, acc:1.000000\n","epoch:[408], loss:0.004019, acc:1.000000\n","epoch:[409], loss:0.004025, acc:1.000000\n","epoch:[410], loss:0.003997, acc:1.000000\n","epoch:[411], loss:0.004046, acc:1.000000\n","epoch:[412], loss:0.003999, acc:1.000000\n","epoch:[413], loss:0.003984, acc:1.000000\n","epoch:[414], loss:0.004005, acc:1.000000\n","epoch:[415], loss:0.004005, acc:1.000000\n","epoch:[416], loss:0.004050, acc:1.000000\n","epoch:[417], loss:0.004018, acc:1.000000\n","epoch:[418], loss:0.004032, acc:1.000000\n","epoch:[419], loss:0.004015, acc:1.000000\n","epoch:[420], loss:0.004005, acc:1.000000\n","epoch:[421], loss:0.004118, acc:1.000000\n","epoch:[422], loss:0.003984, acc:1.000000\n","epoch:[423], loss:0.004055, acc:1.000000\n","epoch:[424], loss:0.004084, acc:1.000000\n","epoch:[425], loss:0.003968, acc:1.000000\n","epoch:[426], loss:0.004013, acc:1.000000\n","epoch:[427], loss:0.003998, acc:1.000000\n","epoch:[428], loss:0.004063, acc:1.000000\n","epoch:[429], loss:0.004021, acc:1.000000\n","epoch:[430], loss:0.004069, acc:1.000000\n","epoch:[431], loss:0.003999, acc:1.000000\n","epoch:[432], loss:0.004035, acc:1.000000\n","epoch:[433], loss:0.004020, acc:1.000000\n","epoch:[434], loss:0.004012, acc:1.000000\n","epoch:[435], loss:0.004014, acc:1.000000\n","epoch:[436], loss:0.003980, acc:1.000000\n","epoch:[437], loss:0.004057, acc:1.000000\n","epoch:[438], loss:0.004057, acc:1.000000\n","epoch:[439], loss:0.003989, acc:1.000000\n","epoch:[440], loss:0.004026, acc:1.000000\n","epoch:[441], loss:0.004013, acc:1.000000\n","epoch:[442], loss:0.004076, acc:1.000000\n","epoch:[443], loss:0.004021, acc:1.000000\n","epoch:[444], loss:0.004095, acc:1.000000\n","epoch:[445], loss:0.004011, acc:1.000000\n","epoch:[446], loss:0.004010, acc:1.000000\n","epoch:[447], loss:0.004005, acc:1.000000\n","epoch:[448], loss:0.004004, acc:1.000000\n","epoch:[449], loss:0.004007, acc:1.000000\n","epoch:[450], loss:0.004012, acc:1.000000\n","epoch:[451], loss:0.004027, acc:1.000000\n","epoch:[452], loss:0.004022, acc:1.000000\n","epoch:[453], loss:0.003947, acc:1.000000\n","epoch:[454], loss:0.004017, acc:1.000000\n","epoch:[455], loss:0.004030, acc:1.000000\n","epoch:[456], loss:0.004006, acc:1.000000\n","epoch:[457], loss:0.004010, acc:1.000000\n","epoch:[458], loss:0.004032, acc:1.000000\n","epoch:[459], loss:0.004010, acc:1.000000\n","epoch:[460], loss:0.004013, acc:1.000000\n","epoch:[461], loss:0.004021, acc:1.000000\n","epoch:[462], loss:0.004001, acc:1.000000\n","epoch:[463], loss:0.004035, acc:1.000000\n","epoch:[464], loss:0.004036, acc:1.000000\n","epoch:[465], loss:0.003980, acc:1.000000\n","epoch:[466], loss:0.004006, acc:1.000000\n","epoch:[467], loss:0.003995, acc:1.000000\n","epoch:[468], loss:0.004031, acc:1.000000\n","epoch:[469], loss:0.004099, acc:1.000000\n","epoch:[470], loss:0.004038, acc:1.000000\n","epoch:[471], loss:0.003955, acc:1.000000\n","epoch:[472], loss:0.004006, acc:1.000000\n","epoch:[473], loss:0.004005, acc:1.000000\n","epoch:[474], loss:0.004010, acc:1.000000\n","epoch:[475], loss:0.004003, acc:1.000000\n","epoch:[476], loss:0.004005, acc:1.000000\n","epoch:[477], loss:0.003995, acc:1.000000\n","epoch:[478], loss:0.004019, acc:1.000000\n","epoch:[479], loss:0.004031, acc:1.000000\n","epoch:[480], loss:0.004050, acc:1.000000\n","epoch:[481], loss:0.003982, acc:1.000000\n","epoch:[482], loss:0.004011, acc:1.000000\n","epoch:[483], loss:0.004031, acc:1.000000\n","epoch:[484], loss:0.003960, acc:1.000000\n","epoch:[485], loss:0.004016, acc:1.000000\n","epoch:[486], loss:0.003997, acc:1.000000\n","epoch:[487], loss:0.004018, acc:1.000000\n","epoch:[488], loss:0.004029, acc:1.000000\n","epoch:[489], loss:0.004055, acc:1.000000\n","epoch:[490], loss:0.004034, acc:1.000000\n","epoch:[491], loss:0.004000, acc:1.000000\n","epoch:[492], loss:0.004043, acc:1.000000\n","epoch:[493], loss:0.003970, acc:1.000000\n","epoch:[494], loss:0.004021, acc:1.000000\n","epoch:[495], loss:0.003969, acc:1.000000\n","epoch:[496], loss:0.003994, acc:1.000000\n","epoch:[497], loss:0.004016, acc:1.000000\n","epoch:[498], loss:0.003984, acc:1.000000\n","epoch:[499], loss:0.004059, acc:1.000000\n"]}]},{"cell_type":"markdown","source":["**server iid train**"],"metadata":{"id":"XkgJ-Z6M_KSw"}},{"cell_type":"code","source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","E_optimizer = optim.Adam(get_params(net, [\"extractor\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","for epoch in range(500):\n","    # train\n","    frozen_net(net, [\"extractor\"], False)\n","    losses, batch = 0., 0\n","    for x, y in server_iid_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        E_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        E_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in server_iid_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/server_iid_extractor.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gq_glU2S1myq","executionInfo":{"status":"ok","timestamp":1640265226115,"user_tz":-480,"elapsed":934356,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"f822d3dc-dc17-4aea-e1f0-c41c36fdb6a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:3.164569, acc:0.310414\n","epoch:[ 1], loss:1.286478, acc:0.720977\n","epoch:[ 2], loss:0.776650, acc:0.782262\n","epoch:[ 3], loss:0.629624, acc:0.809168\n","epoch:[ 4], loss:0.551195, acc:0.827105\n","epoch:[ 5], loss:0.500011, acc:0.834579\n","epoch:[ 6], loss:0.456238, acc:0.840060\n","epoch:[ 7], loss:0.423523, acc:0.850523\n","epoch:[ 8], loss:0.395591, acc:0.860987\n","epoch:[ 9], loss:0.372794, acc:0.865969\n","epoch:[10], loss:0.352234, acc:0.873443\n","epoch:[11], loss:0.334548, acc:0.878924\n","epoch:[12], loss:0.318099, acc:0.881913\n","epoch:[13], loss:0.303921, acc:0.887394\n","epoch:[14], loss:0.289544, acc:0.889387\n","epoch:[15], loss:0.280282, acc:0.895366\n","epoch:[16], loss:0.271393, acc:0.894370\n","epoch:[17], loss:0.260941, acc:0.899352\n","epoch:[18], loss:0.253638, acc:0.901345\n","epoch:[19], loss:0.243652, acc:0.902840\n","epoch:[20], loss:0.238138, acc:0.900349\n","epoch:[21], loss:0.230818, acc:0.897857\n","epoch:[22], loss:0.227455, acc:0.907324\n","epoch:[23], loss:0.220571, acc:0.906328\n","epoch:[24], loss:0.216667, acc:0.904335\n","epoch:[25], loss:0.213591, acc:0.908321\n","epoch:[26], loss:0.207525, acc:0.907823\n","epoch:[27], loss:0.205285, acc:0.909317\n","epoch:[28], loss:0.200496, acc:0.910314\n","epoch:[29], loss:0.197879, acc:0.910314\n","epoch:[30], loss:0.194246, acc:0.910314\n","epoch:[31], loss:0.192078, acc:0.910314\n","epoch:[32], loss:0.189183, acc:0.915296\n","epoch:[33], loss:0.185855, acc:0.913802\n","epoch:[34], loss:0.183550, acc:0.914300\n","epoch:[35], loss:0.182200, acc:0.915795\n","epoch:[36], loss:0.179429, acc:0.914798\n","epoch:[37], loss:0.177592, acc:0.915795\n","epoch:[38], loss:0.175624, acc:0.915296\n","epoch:[39], loss:0.173746, acc:0.913303\n","epoch:[40], loss:0.172613, acc:0.912307\n","epoch:[41], loss:0.171451, acc:0.913802\n","epoch:[42], loss:0.170046, acc:0.916791\n","epoch:[43], loss:0.167203, acc:0.915296\n","epoch:[44], loss:0.166205, acc:0.915795\n","epoch:[45], loss:0.164035, acc:0.918784\n","epoch:[46], loss:0.162657, acc:0.917788\n","epoch:[47], loss:0.163313, acc:0.917788\n","epoch:[48], loss:0.162136, acc:0.918286\n","epoch:[49], loss:0.159763, acc:0.918784\n","epoch:[50], loss:0.158641, acc:0.919781\n","epoch:[51], loss:0.157009, acc:0.918784\n","epoch:[52], loss:0.156264, acc:0.915296\n","epoch:[53], loss:0.154615, acc:0.920279\n","epoch:[54], loss:0.153998, acc:0.920777\n","epoch:[55], loss:0.152794, acc:0.919781\n","epoch:[56], loss:0.150766, acc:0.919283\n","epoch:[57], loss:0.150800, acc:0.918286\n","epoch:[58], loss:0.149770, acc:0.918784\n","epoch:[59], loss:0.148793, acc:0.920777\n","epoch:[60], loss:0.147550, acc:0.920279\n","epoch:[61], loss:0.147275, acc:0.920279\n","epoch:[62], loss:0.146930, acc:0.918784\n","epoch:[63], loss:0.145756, acc:0.921774\n","epoch:[64], loss:0.145598, acc:0.921276\n","epoch:[65], loss:0.141095, acc:0.916791\n","epoch:[66], loss:0.142270, acc:0.921276\n","epoch:[67], loss:0.142256, acc:0.920777\n","epoch:[68], loss:0.141245, acc:0.918784\n","epoch:[69], loss:0.140800, acc:0.920777\n","epoch:[70], loss:0.142271, acc:0.920279\n","epoch:[71], loss:0.139495, acc:0.921774\n","epoch:[72], loss:0.138331, acc:0.919283\n","epoch:[73], loss:0.138437, acc:0.922770\n","epoch:[74], loss:0.137509, acc:0.920777\n","epoch:[75], loss:0.135979, acc:0.921276\n","epoch:[76], loss:0.136984, acc:0.924265\n","epoch:[77], loss:0.135584, acc:0.925262\n","epoch:[78], loss:0.135127, acc:0.923767\n","epoch:[79], loss:0.134038, acc:0.920279\n","epoch:[80], loss:0.134261, acc:0.924265\n","epoch:[81], loss:0.132628, acc:0.923269\n","epoch:[82], loss:0.131230, acc:0.922272\n","epoch:[83], loss:0.132372, acc:0.921276\n","epoch:[84], loss:0.130771, acc:0.925760\n","epoch:[85], loss:0.130587, acc:0.923767\n","epoch:[86], loss:0.129447, acc:0.922272\n","epoch:[87], loss:0.129723, acc:0.925262\n","epoch:[88], loss:0.129073, acc:0.923767\n","epoch:[89], loss:0.128245, acc:0.924763\n","epoch:[90], loss:0.128233, acc:0.923269\n","epoch:[91], loss:0.126878, acc:0.920279\n","epoch:[92], loss:0.127254, acc:0.919283\n","epoch:[93], loss:0.126357, acc:0.923767\n","epoch:[94], loss:0.126449, acc:0.925262\n","epoch:[95], loss:0.124933, acc:0.923767\n","epoch:[96], loss:0.125779, acc:0.923767\n","epoch:[97], loss:0.125574, acc:0.924763\n","epoch:[98], loss:0.124084, acc:0.924763\n","epoch:[99], loss:0.123539, acc:0.924763\n","epoch:[100], loss:0.123047, acc:0.923269\n","epoch:[101], loss:0.122775, acc:0.924265\n","epoch:[102], loss:0.121588, acc:0.924265\n","epoch:[103], loss:0.122785, acc:0.926258\n","epoch:[104], loss:0.121861, acc:0.923269\n","epoch:[105], loss:0.121141, acc:0.923767\n","epoch:[106], loss:0.120412, acc:0.927255\n","epoch:[107], loss:0.121029, acc:0.923269\n","epoch:[108], loss:0.119294, acc:0.923269\n","epoch:[109], loss:0.119635, acc:0.925262\n","epoch:[110], loss:0.118673, acc:0.922272\n","epoch:[111], loss:0.117783, acc:0.925760\n","epoch:[112], loss:0.118671, acc:0.923269\n","epoch:[113], loss:0.117693, acc:0.926258\n","epoch:[114], loss:0.116899, acc:0.924763\n","epoch:[115], loss:0.116779, acc:0.925262\n","epoch:[116], loss:0.116137, acc:0.923767\n","epoch:[117], loss:0.117762, acc:0.924763\n","epoch:[118], loss:0.115149, acc:0.925760\n","epoch:[119], loss:0.115483, acc:0.925262\n","epoch:[120], loss:0.114977, acc:0.925760\n","epoch:[121], loss:0.115076, acc:0.926756\n","epoch:[122], loss:0.114533, acc:0.925262\n","epoch:[123], loss:0.114100, acc:0.924265\n","epoch:[124], loss:0.113507, acc:0.924763\n","epoch:[125], loss:0.114022, acc:0.925262\n","epoch:[126], loss:0.112873, acc:0.927753\n","epoch:[127], loss:0.113347, acc:0.924265\n","epoch:[128], loss:0.113300, acc:0.925262\n","epoch:[129], loss:0.112201, acc:0.926258\n","epoch:[130], loss:0.111917, acc:0.927753\n","epoch:[131], loss:0.112058, acc:0.924265\n","epoch:[132], loss:0.110989, acc:0.924265\n","epoch:[133], loss:0.111654, acc:0.929248\n","epoch:[134], loss:0.110380, acc:0.927255\n","epoch:[135], loss:0.109611, acc:0.924763\n","epoch:[136], loss:0.108338, acc:0.922770\n","epoch:[137], loss:0.109267, acc:0.928749\n","epoch:[138], loss:0.109492, acc:0.927753\n","epoch:[139], loss:0.108304, acc:0.923767\n","epoch:[140], loss:0.108563, acc:0.926258\n","epoch:[141], loss:0.108635, acc:0.926756\n","epoch:[142], loss:0.108587, acc:0.926258\n","epoch:[143], loss:0.108098, acc:0.928251\n","epoch:[144], loss:0.108146, acc:0.929746\n","epoch:[145], loss:0.107974, acc:0.929746\n","epoch:[146], loss:0.106639, acc:0.924763\n","epoch:[147], loss:0.106449, acc:0.927753\n","epoch:[148], loss:0.105949, acc:0.929248\n","epoch:[149], loss:0.106748, acc:0.927753\n","epoch:[150], loss:0.106310, acc:0.926756\n","epoch:[151], loss:0.105536, acc:0.927255\n","epoch:[152], loss:0.105215, acc:0.925760\n","epoch:[153], loss:0.104687, acc:0.924763\n","epoch:[154], loss:0.103810, acc:0.926258\n","epoch:[155], loss:0.105033, acc:0.926756\n","epoch:[156], loss:0.103827, acc:0.928251\n","epoch:[157], loss:0.103778, acc:0.928749\n","epoch:[158], loss:0.103928, acc:0.927753\n","epoch:[159], loss:0.103252, acc:0.926756\n","epoch:[160], loss:0.103263, acc:0.924763\n","epoch:[161], loss:0.103423, acc:0.927255\n","epoch:[162], loss:0.102991, acc:0.929248\n","epoch:[163], loss:0.103151, acc:0.927753\n","epoch:[164], loss:0.101933, acc:0.924265\n","epoch:[165], loss:0.101529, acc:0.927753\n","epoch:[166], loss:0.100890, acc:0.929746\n","epoch:[167], loss:0.101368, acc:0.930742\n","epoch:[168], loss:0.101199, acc:0.929746\n","epoch:[169], loss:0.100340, acc:0.923767\n","epoch:[170], loss:0.100043, acc:0.929746\n","epoch:[171], loss:0.099954, acc:0.930742\n","epoch:[172], loss:0.099768, acc:0.930244\n","epoch:[173], loss:0.100241, acc:0.929248\n","epoch:[174], loss:0.099047, acc:0.927753\n","epoch:[175], loss:0.099153, acc:0.928251\n","epoch:[176], loss:0.099119, acc:0.924763\n","epoch:[177], loss:0.099241, acc:0.928749\n","epoch:[178], loss:0.098577, acc:0.930742\n","epoch:[179], loss:0.098240, acc:0.927753\n","epoch:[180], loss:0.097540, acc:0.924763\n","epoch:[181], loss:0.098224, acc:0.925262\n","epoch:[182], loss:0.097748, acc:0.930244\n","epoch:[183], loss:0.096777, acc:0.927753\n","epoch:[184], loss:0.097297, acc:0.930742\n","epoch:[185], loss:0.096706, acc:0.928251\n","epoch:[186], loss:0.096505, acc:0.930244\n","epoch:[187], loss:0.096816, acc:0.928749\n","epoch:[188], loss:0.095926, acc:0.929248\n","epoch:[189], loss:0.097030, acc:0.930742\n","epoch:[190], loss:0.095752, acc:0.929248\n","epoch:[191], loss:0.095720, acc:0.928749\n","epoch:[192], loss:0.094766, acc:0.926258\n","epoch:[193], loss:0.095385, acc:0.929746\n","epoch:[194], loss:0.094215, acc:0.931739\n","epoch:[195], loss:0.094926, acc:0.932237\n","epoch:[196], loss:0.094594, acc:0.929746\n","epoch:[197], loss:0.094857, acc:0.928749\n","epoch:[198], loss:0.094228, acc:0.929248\n","epoch:[199], loss:0.093760, acc:0.931241\n","epoch:[200], loss:0.093923, acc:0.931241\n","epoch:[201], loss:0.093774, acc:0.928749\n","epoch:[202], loss:0.093306, acc:0.931241\n","epoch:[203], loss:0.092749, acc:0.928251\n","epoch:[204], loss:0.092972, acc:0.928251\n","epoch:[205], loss:0.092426, acc:0.927255\n","epoch:[206], loss:0.091964, acc:0.929248\n","epoch:[207], loss:0.092443, acc:0.927753\n","epoch:[208], loss:0.091731, acc:0.929248\n","epoch:[209], loss:0.091299, acc:0.928251\n","epoch:[210], loss:0.092261, acc:0.929248\n","epoch:[211], loss:0.091691, acc:0.929248\n","epoch:[212], loss:0.090686, acc:0.929248\n","epoch:[213], loss:0.090845, acc:0.928251\n","epoch:[214], loss:0.090416, acc:0.929248\n","epoch:[215], loss:0.090347, acc:0.930244\n","epoch:[216], loss:0.090396, acc:0.930244\n","epoch:[217], loss:0.090605, acc:0.930742\n","epoch:[218], loss:0.090119, acc:0.930244\n","epoch:[219], loss:0.088645, acc:0.930742\n","epoch:[220], loss:0.088757, acc:0.930244\n","epoch:[221], loss:0.088995, acc:0.930244\n","epoch:[222], loss:0.089087, acc:0.930244\n","epoch:[223], loss:0.088972, acc:0.930742\n","epoch:[224], loss:0.088218, acc:0.930742\n","epoch:[225], loss:0.088006, acc:0.928749\n","epoch:[226], loss:0.088362, acc:0.930742\n","epoch:[227], loss:0.087844, acc:0.929746\n","epoch:[228], loss:0.087909, acc:0.930742\n","epoch:[229], loss:0.087628, acc:0.929746\n","epoch:[230], loss:0.087324, acc:0.929746\n","epoch:[231], loss:0.087920, acc:0.930742\n","epoch:[232], loss:0.087298, acc:0.929746\n","epoch:[233], loss:0.086580, acc:0.929746\n","epoch:[234], loss:0.086383, acc:0.929248\n","epoch:[235], loss:0.086734, acc:0.929746\n","epoch:[236], loss:0.086381, acc:0.931739\n","epoch:[237], loss:0.086448, acc:0.930244\n","epoch:[238], loss:0.085851, acc:0.932735\n","epoch:[239], loss:0.085931, acc:0.929746\n","epoch:[240], loss:0.085776, acc:0.927753\n","epoch:[241], loss:0.085505, acc:0.930742\n","epoch:[242], loss:0.085510, acc:0.929746\n","epoch:[243], loss:0.084811, acc:0.927753\n","epoch:[244], loss:0.085107, acc:0.931739\n","epoch:[245], loss:0.085178, acc:0.929746\n","epoch:[246], loss:0.082927, acc:0.923767\n","epoch:[247], loss:0.084654, acc:0.929248\n","epoch:[248], loss:0.084883, acc:0.930244\n","epoch:[249], loss:0.083848, acc:0.931739\n","epoch:[250], loss:0.083791, acc:0.931739\n","epoch:[251], loss:0.083834, acc:0.930742\n","epoch:[252], loss:0.083710, acc:0.931241\n","epoch:[253], loss:0.084367, acc:0.929746\n","epoch:[254], loss:0.083048, acc:0.928749\n","epoch:[255], loss:0.082721, acc:0.930244\n","epoch:[256], loss:0.082737, acc:0.931739\n","epoch:[257], loss:0.082684, acc:0.930244\n","epoch:[258], loss:0.082184, acc:0.929248\n","epoch:[259], loss:0.081752, acc:0.927255\n","epoch:[260], loss:0.082971, acc:0.930244\n","epoch:[261], loss:0.082061, acc:0.931241\n","epoch:[262], loss:0.081972, acc:0.929248\n","epoch:[263], loss:0.081652, acc:0.929746\n","epoch:[264], loss:0.081486, acc:0.931739\n","epoch:[265], loss:0.081377, acc:0.929746\n","epoch:[266], loss:0.080658, acc:0.930244\n","epoch:[267], loss:0.080627, acc:0.929746\n","epoch:[268], loss:0.081073, acc:0.930742\n","epoch:[269], loss:0.080608, acc:0.928749\n","epoch:[270], loss:0.080621, acc:0.930244\n","epoch:[271], loss:0.080155, acc:0.930742\n","epoch:[272], loss:0.080155, acc:0.931739\n","epoch:[273], loss:0.080264, acc:0.930742\n","epoch:[274], loss:0.080011, acc:0.930742\n","epoch:[275], loss:0.080388, acc:0.930244\n","epoch:[276], loss:0.079348, acc:0.927753\n","epoch:[277], loss:0.079767, acc:0.930742\n","epoch:[278], loss:0.079407, acc:0.930742\n","epoch:[279], loss:0.078245, acc:0.930244\n","epoch:[280], loss:0.078793, acc:0.930244\n","epoch:[281], loss:0.079385, acc:0.930742\n","epoch:[282], loss:0.078856, acc:0.930244\n","epoch:[283], loss:0.078585, acc:0.931739\n","epoch:[284], loss:0.078346, acc:0.930244\n","epoch:[285], loss:0.078563, acc:0.927255\n","epoch:[286], loss:0.077970, acc:0.929248\n","epoch:[287], loss:0.078013, acc:0.930244\n","epoch:[288], loss:0.078247, acc:0.929746\n","epoch:[289], loss:0.077694, acc:0.929248\n","epoch:[290], loss:0.077861, acc:0.928749\n","epoch:[291], loss:0.077178, acc:0.930244\n","epoch:[292], loss:0.077662, acc:0.930742\n","epoch:[293], loss:0.076460, acc:0.928251\n","epoch:[294], loss:0.077570, acc:0.930244\n","epoch:[295], loss:0.076093, acc:0.927753\n","epoch:[296], loss:0.076902, acc:0.927753\n","epoch:[297], loss:0.077298, acc:0.930742\n","epoch:[298], loss:0.076648, acc:0.928749\n","epoch:[299], loss:0.076542, acc:0.929746\n","epoch:[300], loss:0.076734, acc:0.929248\n","epoch:[301], loss:0.075489, acc:0.930742\n","epoch:[302], loss:0.075897, acc:0.925760\n","epoch:[303], loss:0.076274, acc:0.930244\n","epoch:[304], loss:0.076022, acc:0.930742\n","epoch:[305], loss:0.075655, acc:0.928749\n","epoch:[306], loss:0.076004, acc:0.930742\n","epoch:[307], loss:0.075572, acc:0.930244\n","epoch:[308], loss:0.075762, acc:0.929248\n","epoch:[309], loss:0.075563, acc:0.930244\n","epoch:[310], loss:0.075017, acc:0.930742\n","epoch:[311], loss:0.074785, acc:0.929248\n","epoch:[312], loss:0.074185, acc:0.926258\n","epoch:[313], loss:0.075015, acc:0.930244\n","epoch:[314], loss:0.075595, acc:0.927753\n","epoch:[315], loss:0.074659, acc:0.930742\n","epoch:[316], loss:0.074303, acc:0.927255\n","epoch:[317], loss:0.073943, acc:0.929746\n","epoch:[318], loss:0.073739, acc:0.929248\n","epoch:[319], loss:0.074255, acc:0.926258\n","epoch:[320], loss:0.073402, acc:0.929746\n","epoch:[321], loss:0.073172, acc:0.929746\n","epoch:[322], loss:0.073506, acc:0.929746\n","epoch:[323], loss:0.072972, acc:0.926258\n","epoch:[324], loss:0.073732, acc:0.929746\n","epoch:[325], loss:0.072827, acc:0.931241\n","epoch:[326], loss:0.073195, acc:0.931241\n","epoch:[327], loss:0.072667, acc:0.929248\n","epoch:[328], loss:0.072418, acc:0.928251\n","epoch:[329], loss:0.073533, acc:0.929248\n","epoch:[330], loss:0.071830, acc:0.930244\n","epoch:[331], loss:0.072492, acc:0.928749\n","epoch:[332], loss:0.072661, acc:0.930742\n","epoch:[333], loss:0.071762, acc:0.929248\n","epoch:[334], loss:0.072297, acc:0.928749\n","epoch:[335], loss:0.072654, acc:0.928251\n","epoch:[336], loss:0.071739, acc:0.930244\n","epoch:[337], loss:0.071580, acc:0.928749\n","epoch:[338], loss:0.072219, acc:0.927753\n","epoch:[339], loss:0.071450, acc:0.930244\n","epoch:[340], loss:0.071468, acc:0.931241\n","epoch:[341], loss:0.071272, acc:0.929746\n","epoch:[342], loss:0.071752, acc:0.927753\n","epoch:[343], loss:0.071402, acc:0.929248\n","epoch:[344], loss:0.070542, acc:0.930244\n","epoch:[345], loss:0.071021, acc:0.929746\n","epoch:[346], loss:0.071462, acc:0.928251\n","epoch:[347], loss:0.070497, acc:0.929746\n","epoch:[348], loss:0.070645, acc:0.930244\n","epoch:[349], loss:0.069744, acc:0.927255\n","epoch:[350], loss:0.070471, acc:0.931241\n","epoch:[351], loss:0.071030, acc:0.929746\n","epoch:[352], loss:0.070014, acc:0.929746\n","epoch:[353], loss:0.069748, acc:0.929248\n","epoch:[354], loss:0.070063, acc:0.929746\n","epoch:[355], loss:0.071040, acc:0.929248\n","epoch:[356], loss:0.069406, acc:0.927753\n","epoch:[357], loss:0.069539, acc:0.930742\n","epoch:[358], loss:0.069937, acc:0.928749\n","epoch:[359], loss:0.069512, acc:0.930742\n","epoch:[360], loss:0.069703, acc:0.930742\n","epoch:[361], loss:0.069353, acc:0.930244\n","epoch:[362], loss:0.069269, acc:0.928749\n","epoch:[363], loss:0.069583, acc:0.930742\n","epoch:[364], loss:0.068613, acc:0.928251\n","epoch:[365], loss:0.069222, acc:0.929746\n","epoch:[366], loss:0.067668, acc:0.926756\n","epoch:[367], loss:0.069441, acc:0.931241\n","epoch:[368], loss:0.068808, acc:0.930742\n","epoch:[369], loss:0.069377, acc:0.929746\n","epoch:[370], loss:0.068252, acc:0.929746\n","epoch:[371], loss:0.069052, acc:0.930742\n","epoch:[372], loss:0.068382, acc:0.930244\n","epoch:[373], loss:0.068107, acc:0.929248\n","epoch:[374], loss:0.067868, acc:0.928251\n","epoch:[375], loss:0.068226, acc:0.929248\n","epoch:[376], loss:0.067216, acc:0.930244\n","epoch:[377], loss:0.067593, acc:0.928251\n","epoch:[378], loss:0.068074, acc:0.928251\n","epoch:[379], loss:0.067605, acc:0.927255\n","epoch:[380], loss:0.067502, acc:0.930742\n","epoch:[381], loss:0.067265, acc:0.927753\n","epoch:[382], loss:0.067208, acc:0.928749\n","epoch:[383], loss:0.066882, acc:0.929746\n","epoch:[384], loss:0.067244, acc:0.929248\n","epoch:[385], loss:0.066651, acc:0.928251\n","epoch:[386], loss:0.066891, acc:0.928749\n","epoch:[387], loss:0.066227, acc:0.930244\n","epoch:[388], loss:0.066254, acc:0.928251\n","epoch:[389], loss:0.066738, acc:0.928749\n","epoch:[390], loss:0.066723, acc:0.929746\n","epoch:[391], loss:0.066491, acc:0.929248\n","epoch:[392], loss:0.066569, acc:0.928251\n","epoch:[393], loss:0.066270, acc:0.928749\n","epoch:[394], loss:0.066328, acc:0.928749\n","epoch:[395], loss:0.066263, acc:0.926756\n","epoch:[396], loss:0.066219, acc:0.926756\n","epoch:[397], loss:0.066016, acc:0.929746\n","epoch:[398], loss:0.065964, acc:0.929746\n","epoch:[399], loss:0.065824, acc:0.928749\n","epoch:[400], loss:0.066185, acc:0.929248\n","epoch:[401], loss:0.065362, acc:0.928251\n","epoch:[402], loss:0.065210, acc:0.930742\n","epoch:[403], loss:0.065935, acc:0.931739\n","epoch:[404], loss:0.065739, acc:0.929746\n","epoch:[405], loss:0.065100, acc:0.930244\n","epoch:[406], loss:0.065447, acc:0.928749\n","epoch:[407], loss:0.065176, acc:0.928749\n","epoch:[408], loss:0.065311, acc:0.929746\n","epoch:[409], loss:0.065009, acc:0.930244\n","epoch:[410], loss:0.064795, acc:0.929248\n","epoch:[411], loss:0.065026, acc:0.928251\n","epoch:[412], loss:0.064469, acc:0.928749\n","epoch:[413], loss:0.064311, acc:0.927255\n","epoch:[414], loss:0.064659, acc:0.927753\n","epoch:[415], loss:0.064139, acc:0.928251\n","epoch:[416], loss:0.064844, acc:0.932237\n","epoch:[417], loss:0.063651, acc:0.928749\n","epoch:[418], loss:0.064901, acc:0.929746\n","epoch:[419], loss:0.063624, acc:0.932237\n","epoch:[420], loss:0.064257, acc:0.929248\n","epoch:[421], loss:0.064089, acc:0.929248\n","epoch:[422], loss:0.063790, acc:0.930244\n","epoch:[423], loss:0.064116, acc:0.930742\n","epoch:[424], loss:0.063733, acc:0.928251\n","epoch:[425], loss:0.063648, acc:0.927753\n","epoch:[426], loss:0.063277, acc:0.931739\n","epoch:[427], loss:0.063693, acc:0.928749\n","epoch:[428], loss:0.063182, acc:0.927255\n","epoch:[429], loss:0.063769, acc:0.930244\n","epoch:[430], loss:0.063291, acc:0.930244\n","epoch:[431], loss:0.063143, acc:0.927255\n","epoch:[432], loss:0.063074, acc:0.927753\n","epoch:[433], loss:0.062624, acc:0.929746\n","epoch:[434], loss:0.062902, acc:0.928251\n","epoch:[435], loss:0.063331, acc:0.928749\n","epoch:[436], loss:0.062674, acc:0.932237\n","epoch:[437], loss:0.063094, acc:0.928251\n","epoch:[438], loss:0.062461, acc:0.931241\n","epoch:[439], loss:0.062479, acc:0.930244\n","epoch:[440], loss:0.061931, acc:0.927753\n","epoch:[441], loss:0.062508, acc:0.927753\n","epoch:[442], loss:0.062975, acc:0.930742\n","epoch:[443], loss:0.062215, acc:0.926756\n","epoch:[444], loss:0.062407, acc:0.930244\n","epoch:[445], loss:0.061763, acc:0.929248\n","epoch:[446], loss:0.062112, acc:0.928251\n","epoch:[447], loss:0.062498, acc:0.930244\n","epoch:[448], loss:0.062007, acc:0.927255\n","epoch:[449], loss:0.061792, acc:0.929746\n","epoch:[450], loss:0.061440, acc:0.929248\n","epoch:[451], loss:0.061708, acc:0.929248\n","epoch:[452], loss:0.061744, acc:0.929746\n","epoch:[453], loss:0.061825, acc:0.928251\n","epoch:[454], loss:0.061399, acc:0.930742\n","epoch:[455], loss:0.060878, acc:0.928749\n","epoch:[456], loss:0.060861, acc:0.929746\n","epoch:[457], loss:0.060875, acc:0.930742\n","epoch:[458], loss:0.061618, acc:0.929746\n","epoch:[459], loss:0.060384, acc:0.929248\n","epoch:[460], loss:0.061075, acc:0.932237\n","epoch:[461], loss:0.060699, acc:0.927753\n","epoch:[462], loss:0.060891, acc:0.930244\n","epoch:[463], loss:0.060863, acc:0.930244\n","epoch:[464], loss:0.060382, acc:0.932735\n","epoch:[465], loss:0.061017, acc:0.928749\n","epoch:[466], loss:0.060738, acc:0.931241\n","epoch:[467], loss:0.060845, acc:0.929746\n","epoch:[468], loss:0.060824, acc:0.929746\n","epoch:[469], loss:0.060653, acc:0.928749\n","epoch:[470], loss:0.060282, acc:0.931241\n","epoch:[471], loss:0.060271, acc:0.930244\n","epoch:[472], loss:0.060187, acc:0.930244\n","epoch:[473], loss:0.060288, acc:0.928251\n","epoch:[474], loss:0.060287, acc:0.927255\n","epoch:[475], loss:0.060196, acc:0.929248\n","epoch:[476], loss:0.060147, acc:0.930742\n","epoch:[477], loss:0.059593, acc:0.930742\n","epoch:[478], loss:0.059652, acc:0.930742\n","epoch:[479], loss:0.059574, acc:0.929248\n","epoch:[480], loss:0.059767, acc:0.929746\n","epoch:[481], loss:0.059466, acc:0.931241\n","epoch:[482], loss:0.059389, acc:0.931241\n","epoch:[483], loss:0.059405, acc:0.929248\n","epoch:[484], loss:0.058828, acc:0.930244\n","epoch:[485], loss:0.059053, acc:0.929248\n","epoch:[486], loss:0.058873, acc:0.928749\n","epoch:[487], loss:0.058736, acc:0.929248\n","epoch:[488], loss:0.058963, acc:0.930244\n","epoch:[489], loss:0.058891, acc:0.930742\n","epoch:[490], loss:0.059121, acc:0.930244\n","epoch:[491], loss:0.059047, acc:0.930244\n","epoch:[492], loss:0.059143, acc:0.931241\n","epoch:[493], loss:0.058914, acc:0.928251\n","epoch:[494], loss:0.058283, acc:0.929746\n","epoch:[495], loss:0.058973, acc:0.928749\n","epoch:[496], loss:0.058770, acc:0.929746\n","epoch:[497], loss:0.058499, acc:0.930244\n","epoch:[498], loss:0.058293, acc:0.930742\n","epoch:[499], loss:0.058451, acc:0.931739\n"]}]},{"cell_type":"markdown","source":["**server niid train**"],"metadata":{"id":"7Xmr0Ufk_oCM"}},{"cell_type":"code","source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()    \n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\"], True)\n","E_optimizer = optim.Adam(get_params(net, [\"extractor\"]), lr=3e-4, weight_decay=1e-4)\n","CE_criterion = nn.CrossEntropyLoss().to(device)\n","\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","for epoch in range(500):\n","    # train\n","    frozen_net(net, [\"extractor\"], False)\n","    losses, batch = 0., 0\n","    for x, y in server_niid_trainloader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        E_optimizer.zero_grad()\n","        E = net[\"extractor\"](x)\n","        EC = net[\"classifier\"](E)\n","        loss = CE_criterion(EC, y)\n","        loss.backward()\n","        E_optimizer.step()\n","\n","        losses += loss.item()\n","        batch += 1\n","    avg_loss = losses / batch\n","    frozen_net(net, [\"extractor\"], True)\n","    \n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in server_niid_testloader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            E = net[\"extractor\"](x)\n","            EC = net[\"classifier\"](E)\n","\n","            correct += torch.sum((torch.argmax(EC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], loss:%2.6f, acc:%2.6f\"%(epoch, avg_loss, acc))\n","\n","torch.save(net[\"extractor\"].state_dict(), \"./checkpoint/server_niid_extractor.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Yvtf4Il_qRc","executionInfo":{"status":"ok","timestamp":1640267531930,"user_tz":-480,"elapsed":2305825,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}},"outputId":"4b17a2b8-cdfc-4aae-b2d7-140fabce5e61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], loss:3.135447, acc:0.098000\n","epoch:[ 1], loss:2.497407, acc:0.215600\n","epoch:[ 2], loss:2.160743, acc:0.368700\n","epoch:[ 3], loss:1.943198, acc:0.411400\n","epoch:[ 4], loss:1.767795, acc:0.458800\n","epoch:[ 5], loss:1.632686, acc:0.472300\n","epoch:[ 6], loss:1.531908, acc:0.511700\n","epoch:[ 7], loss:1.437794, acc:0.531200\n","epoch:[ 8], loss:1.355015, acc:0.569800\n","epoch:[ 9], loss:1.273691, acc:0.609500\n","epoch:[10], loss:1.209693, acc:0.628400\n","epoch:[11], loss:1.152437, acc:0.653700\n","epoch:[12], loss:1.100277, acc:0.618100\n","epoch:[13], loss:1.052451, acc:0.686200\n","epoch:[14], loss:1.014736, acc:0.679300\n","epoch:[15], loss:0.973673, acc:0.697800\n","epoch:[16], loss:0.942575, acc:0.715000\n","epoch:[17], loss:0.911199, acc:0.736400\n","epoch:[18], loss:0.882362, acc:0.737100\n","epoch:[19], loss:0.856386, acc:0.755500\n","epoch:[20], loss:0.830695, acc:0.760700\n","epoch:[21], loss:0.804255, acc:0.765300\n","epoch:[22], loss:0.784645, acc:0.774800\n","epoch:[23], loss:0.767280, acc:0.782500\n","epoch:[24], loss:0.745752, acc:0.788200\n","epoch:[25], loss:0.726577, acc:0.794700\n","epoch:[26], loss:0.708291, acc:0.792000\n","epoch:[27], loss:0.693582, acc:0.803000\n","epoch:[28], loss:0.679015, acc:0.805100\n","epoch:[29], loss:0.666155, acc:0.806500\n","epoch:[30], loss:0.649986, acc:0.807700\n","epoch:[31], loss:0.637056, acc:0.819000\n","epoch:[32], loss:0.626417, acc:0.819200\n","epoch:[33], loss:0.617637, acc:0.822800\n","epoch:[34], loss:0.601775, acc:0.828600\n","epoch:[35], loss:0.595273, acc:0.828000\n","epoch:[36], loss:0.582770, acc:0.827500\n","epoch:[37], loss:0.574151, acc:0.835800\n","epoch:[38], loss:0.563358, acc:0.834100\n","epoch:[39], loss:0.552724, acc:0.840300\n","epoch:[40], loss:0.551625, acc:0.843200\n","epoch:[41], loss:0.538455, acc:0.836600\n","epoch:[42], loss:0.532890, acc:0.847800\n","epoch:[43], loss:0.525561, acc:0.846900\n","epoch:[44], loss:0.515069, acc:0.852200\n","epoch:[45], loss:0.507926, acc:0.850700\n","epoch:[46], loss:0.502042, acc:0.853800\n","epoch:[47], loss:0.496772, acc:0.857000\n","epoch:[48], loss:0.490029, acc:0.857100\n","epoch:[49], loss:0.484423, acc:0.860900\n","epoch:[50], loss:0.476398, acc:0.861900\n","epoch:[51], loss:0.474738, acc:0.855500\n","epoch:[52], loss:0.467273, acc:0.863900\n","epoch:[53], loss:0.461555, acc:0.865500\n","epoch:[54], loss:0.458121, acc:0.869000\n","epoch:[55], loss:0.448985, acc:0.862600\n","epoch:[56], loss:0.445947, acc:0.868500\n","epoch:[57], loss:0.443192, acc:0.868000\n","epoch:[58], loss:0.440343, acc:0.870000\n","epoch:[59], loss:0.437112, acc:0.869900\n","epoch:[60], loss:0.431671, acc:0.871300\n","epoch:[61], loss:0.426209, acc:0.874000\n","epoch:[62], loss:0.423899, acc:0.874200\n","epoch:[63], loss:0.418022, acc:0.873200\n","epoch:[64], loss:0.416300, acc:0.877100\n","epoch:[65], loss:0.412555, acc:0.875400\n","epoch:[66], loss:0.411451, acc:0.878100\n","epoch:[67], loss:0.405457, acc:0.878700\n","epoch:[68], loss:0.402896, acc:0.876000\n","epoch:[69], loss:0.399585, acc:0.881800\n","epoch:[70], loss:0.394517, acc:0.880100\n","epoch:[71], loss:0.393764, acc:0.882900\n","epoch:[72], loss:0.392986, acc:0.879800\n","epoch:[73], loss:0.388879, acc:0.883300\n","epoch:[74], loss:0.382400, acc:0.882800\n","epoch:[75], loss:0.381234, acc:0.880500\n","epoch:[76], loss:0.381357, acc:0.881100\n","epoch:[77], loss:0.378357, acc:0.883800\n","epoch:[78], loss:0.375465, acc:0.884300\n","epoch:[79], loss:0.370438, acc:0.885200\n","epoch:[80], loss:0.369468, acc:0.883800\n","epoch:[81], loss:0.367770, acc:0.884100\n","epoch:[82], loss:0.366488, acc:0.887300\n","epoch:[83], loss:0.362302, acc:0.883400\n","epoch:[84], loss:0.358760, acc:0.889300\n","epoch:[85], loss:0.357401, acc:0.885800\n","epoch:[86], loss:0.359258, acc:0.889700\n","epoch:[87], loss:0.354829, acc:0.890600\n","epoch:[88], loss:0.352569, acc:0.890300\n","epoch:[89], loss:0.353691, acc:0.888000\n","epoch:[90], loss:0.349930, acc:0.893600\n","epoch:[91], loss:0.346127, acc:0.884700\n","epoch:[92], loss:0.344601, acc:0.887800\n","epoch:[93], loss:0.344199, acc:0.892600\n","epoch:[94], loss:0.341633, acc:0.893300\n","epoch:[95], loss:0.341093, acc:0.892600\n","epoch:[96], loss:0.337500, acc:0.894400\n","epoch:[97], loss:0.335307, acc:0.894600\n","epoch:[98], loss:0.335720, acc:0.896200\n","epoch:[99], loss:0.333328, acc:0.893700\n","epoch:[100], loss:0.331016, acc:0.894700\n","epoch:[101], loss:0.331187, acc:0.891800\n","epoch:[102], loss:0.328501, acc:0.891600\n","epoch:[103], loss:0.326685, acc:0.894300\n","epoch:[104], loss:0.324490, acc:0.892500\n","epoch:[105], loss:0.326088, acc:0.897000\n","epoch:[106], loss:0.322499, acc:0.897500\n","epoch:[107], loss:0.321269, acc:0.899300\n","epoch:[108], loss:0.320998, acc:0.901700\n","epoch:[109], loss:0.318659, acc:0.898700\n","epoch:[110], loss:0.316821, acc:0.899200\n","epoch:[111], loss:0.314005, acc:0.899700\n","epoch:[112], loss:0.312134, acc:0.900300\n","epoch:[113], loss:0.312114, acc:0.899100\n","epoch:[114], loss:0.311453, acc:0.900800\n","epoch:[115], loss:0.309937, acc:0.900100\n","epoch:[116], loss:0.311118, acc:0.901900\n","epoch:[117], loss:0.308752, acc:0.903600\n","epoch:[118], loss:0.304952, acc:0.901000\n","epoch:[119], loss:0.304450, acc:0.897700\n","epoch:[120], loss:0.303854, acc:0.901600\n","epoch:[121], loss:0.301408, acc:0.900300\n","epoch:[122], loss:0.302202, acc:0.903800\n","epoch:[123], loss:0.300925, acc:0.904000\n","epoch:[124], loss:0.300024, acc:0.902500\n","epoch:[125], loss:0.297135, acc:0.902800\n","epoch:[126], loss:0.297246, acc:0.902300\n","epoch:[127], loss:0.295765, acc:0.902300\n","epoch:[128], loss:0.294798, acc:0.907000\n","epoch:[129], loss:0.295081, acc:0.905500\n","epoch:[130], loss:0.295089, acc:0.902900\n","epoch:[131], loss:0.292430, acc:0.901400\n","epoch:[132], loss:0.290061, acc:0.905100\n","epoch:[133], loss:0.288962, acc:0.905700\n","epoch:[134], loss:0.288828, acc:0.907300\n","epoch:[135], loss:0.287792, acc:0.907900\n","epoch:[136], loss:0.285180, acc:0.907600\n","epoch:[137], loss:0.287084, acc:0.907800\n","epoch:[138], loss:0.284001, acc:0.908300\n","epoch:[139], loss:0.284060, acc:0.907800\n","epoch:[140], loss:0.281468, acc:0.907200\n","epoch:[141], loss:0.282544, acc:0.908500\n","epoch:[142], loss:0.281606, acc:0.904500\n","epoch:[143], loss:0.279078, acc:0.904300\n","epoch:[144], loss:0.278464, acc:0.904600\n","epoch:[145], loss:0.278717, acc:0.910400\n","epoch:[146], loss:0.279100, acc:0.907900\n","epoch:[147], loss:0.276653, acc:0.908200\n","epoch:[148], loss:0.275682, acc:0.910300\n","epoch:[149], loss:0.274070, acc:0.906100\n","epoch:[150], loss:0.274140, acc:0.909100\n","epoch:[151], loss:0.274270, acc:0.911100\n","epoch:[152], loss:0.270933, acc:0.910000\n","epoch:[153], loss:0.270539, acc:0.908300\n","epoch:[154], loss:0.272420, acc:0.909900\n","epoch:[155], loss:0.267501, acc:0.906500\n","epoch:[156], loss:0.267403, acc:0.910500\n","epoch:[157], loss:0.267270, acc:0.909100\n","epoch:[158], loss:0.265595, acc:0.910600\n","epoch:[159], loss:0.265918, acc:0.909200\n","epoch:[160], loss:0.264827, acc:0.901400\n","epoch:[161], loss:0.264213, acc:0.908500\n","epoch:[162], loss:0.263170, acc:0.910300\n","epoch:[163], loss:0.263513, acc:0.910600\n","epoch:[164], loss:0.261571, acc:0.911000\n","epoch:[165], loss:0.261523, acc:0.909100\n","epoch:[166], loss:0.261710, acc:0.910600\n","epoch:[167], loss:0.261040, acc:0.911200\n","epoch:[168], loss:0.260937, acc:0.912200\n","epoch:[169], loss:0.258412, acc:0.911500\n","epoch:[170], loss:0.256230, acc:0.914000\n","epoch:[171], loss:0.257974, acc:0.907600\n","epoch:[172], loss:0.258110, acc:0.914000\n","epoch:[173], loss:0.256067, acc:0.913200\n","epoch:[174], loss:0.254528, acc:0.913200\n","epoch:[175], loss:0.254808, acc:0.913600\n","epoch:[176], loss:0.253178, acc:0.913800\n","epoch:[177], loss:0.252184, acc:0.913600\n","epoch:[178], loss:0.251821, acc:0.911800\n","epoch:[179], loss:0.252507, acc:0.915400\n","epoch:[180], loss:0.249739, acc:0.911900\n","epoch:[181], loss:0.250442, acc:0.913800\n","epoch:[182], loss:0.248605, acc:0.913900\n","epoch:[183], loss:0.248463, acc:0.913300\n","epoch:[184], loss:0.249887, acc:0.913100\n","epoch:[185], loss:0.246523, acc:0.913400\n","epoch:[186], loss:0.246549, acc:0.915900\n","epoch:[187], loss:0.247944, acc:0.911700\n","epoch:[188], loss:0.245246, acc:0.914300\n","epoch:[189], loss:0.245433, acc:0.917400\n","epoch:[190], loss:0.244866, acc:0.913700\n","epoch:[191], loss:0.244399, acc:0.911800\n","epoch:[192], loss:0.241879, acc:0.914900\n","epoch:[193], loss:0.245206, acc:0.917300\n","epoch:[194], loss:0.241528, acc:0.916500\n","epoch:[195], loss:0.240698, acc:0.916800\n","epoch:[196], loss:0.241081, acc:0.917500\n","epoch:[197], loss:0.239289, acc:0.918400\n","epoch:[198], loss:0.241070, acc:0.917400\n","epoch:[199], loss:0.240249, acc:0.917400\n","epoch:[200], loss:0.239512, acc:0.919400\n","epoch:[201], loss:0.237274, acc:0.917300\n","epoch:[202], loss:0.238604, acc:0.917900\n","epoch:[203], loss:0.236078, acc:0.916900\n","epoch:[204], loss:0.237657, acc:0.913500\n","epoch:[205], loss:0.235488, acc:0.911800\n","epoch:[206], loss:0.235858, acc:0.916800\n","epoch:[207], loss:0.236109, acc:0.918200\n","epoch:[208], loss:0.233362, acc:0.918300\n","epoch:[209], loss:0.232826, acc:0.917100\n","epoch:[210], loss:0.232800, acc:0.919200\n","epoch:[211], loss:0.232920, acc:0.919000\n","epoch:[212], loss:0.232949, acc:0.918500\n","epoch:[213], loss:0.231016, acc:0.917500\n","epoch:[214], loss:0.231559, acc:0.918100\n","epoch:[215], loss:0.231285, acc:0.919100\n","epoch:[216], loss:0.228938, acc:0.918000\n","epoch:[217], loss:0.227291, acc:0.912200\n","epoch:[218], loss:0.230875, acc:0.920400\n","epoch:[219], loss:0.228985, acc:0.920500\n","epoch:[220], loss:0.228491, acc:0.919600\n","epoch:[221], loss:0.226701, acc:0.920100\n","epoch:[222], loss:0.225904, acc:0.919000\n","epoch:[223], loss:0.226002, acc:0.918100\n","epoch:[224], loss:0.227445, acc:0.920300\n","epoch:[225], loss:0.224740, acc:0.919200\n","epoch:[226], loss:0.223754, acc:0.921100\n","epoch:[227], loss:0.225243, acc:0.920800\n","epoch:[228], loss:0.224697, acc:0.921100\n","epoch:[229], loss:0.223598, acc:0.921200\n","epoch:[230], loss:0.222388, acc:0.917200\n","epoch:[231], loss:0.223618, acc:0.921300\n","epoch:[232], loss:0.222232, acc:0.921200\n","epoch:[233], loss:0.220728, acc:0.921300\n","epoch:[234], loss:0.219757, acc:0.917500\n","epoch:[235], loss:0.222264, acc:0.920800\n","epoch:[236], loss:0.218453, acc:0.921800\n","epoch:[237], loss:0.219633, acc:0.922000\n","epoch:[238], loss:0.218916, acc:0.921800\n","epoch:[239], loss:0.218519, acc:0.922900\n","epoch:[240], loss:0.218339, acc:0.921700\n","epoch:[241], loss:0.218167, acc:0.921500\n","epoch:[242], loss:0.217274, acc:0.923100\n","epoch:[243], loss:0.217145, acc:0.920400\n","epoch:[244], loss:0.217710, acc:0.922500\n","epoch:[245], loss:0.215452, acc:0.923100\n","epoch:[246], loss:0.215313, acc:0.922600\n","epoch:[247], loss:0.215017, acc:0.922600\n","epoch:[248], loss:0.213756, acc:0.923200\n","epoch:[249], loss:0.215274, acc:0.921400\n","epoch:[250], loss:0.214643, acc:0.924200\n","epoch:[251], loss:0.214592, acc:0.922400\n","epoch:[252], loss:0.213215, acc:0.921300\n","epoch:[253], loss:0.211679, acc:0.923900\n","epoch:[254], loss:0.211987, acc:0.920800\n","epoch:[255], loss:0.212007, acc:0.921100\n","epoch:[256], loss:0.211536, acc:0.924200\n","epoch:[257], loss:0.211480, acc:0.921500\n","epoch:[258], loss:0.210977, acc:0.921000\n","epoch:[259], loss:0.211441, acc:0.923000\n","epoch:[260], loss:0.210597, acc:0.922700\n","epoch:[261], loss:0.209789, acc:0.922700\n","epoch:[262], loss:0.208914, acc:0.921500\n","epoch:[263], loss:0.207857, acc:0.923200\n","epoch:[264], loss:0.210438, acc:0.923300\n","epoch:[265], loss:0.208621, acc:0.922300\n","epoch:[266], loss:0.210953, acc:0.921500\n","epoch:[267], loss:0.206970, acc:0.921400\n","epoch:[268], loss:0.205773, acc:0.924100\n","epoch:[269], loss:0.206743, acc:0.923100\n","epoch:[270], loss:0.206077, acc:0.924000\n","epoch:[271], loss:0.206817, acc:0.924300\n","epoch:[272], loss:0.204682, acc:0.922700\n","epoch:[273], loss:0.205378, acc:0.920600\n","epoch:[274], loss:0.205225, acc:0.921800\n","epoch:[275], loss:0.205439, acc:0.924400\n","epoch:[276], loss:0.205384, acc:0.924300\n","epoch:[277], loss:0.203758, acc:0.924000\n","epoch:[278], loss:0.203879, acc:0.924300\n","epoch:[279], loss:0.203477, acc:0.922700\n","epoch:[280], loss:0.204555, acc:0.922400\n","epoch:[281], loss:0.201890, acc:0.918700\n","epoch:[282], loss:0.202580, acc:0.924300\n","epoch:[283], loss:0.200338, acc:0.925900\n","epoch:[284], loss:0.201757, acc:0.916600\n","epoch:[285], loss:0.201867, acc:0.920600\n","epoch:[286], loss:0.202342, acc:0.926600\n","epoch:[287], loss:0.201510, acc:0.925600\n","epoch:[288], loss:0.200051, acc:0.924700\n","epoch:[289], loss:0.200257, acc:0.921200\n","epoch:[290], loss:0.198262, acc:0.923900\n","epoch:[291], loss:0.199885, acc:0.926700\n","epoch:[292], loss:0.198723, acc:0.926100\n","epoch:[293], loss:0.197903, acc:0.922800\n","epoch:[294], loss:0.199130, acc:0.923800\n","epoch:[295], loss:0.197011, acc:0.923900\n","epoch:[296], loss:0.197407, acc:0.925700\n","epoch:[297], loss:0.196406, acc:0.921900\n","epoch:[298], loss:0.196903, acc:0.926100\n","epoch:[299], loss:0.196043, acc:0.924900\n","epoch:[300], loss:0.195195, acc:0.922100\n","epoch:[301], loss:0.197369, acc:0.925600\n","epoch:[302], loss:0.195432, acc:0.926900\n","epoch:[303], loss:0.194630, acc:0.923000\n","epoch:[304], loss:0.195013, acc:0.923200\n","epoch:[305], loss:0.193230, acc:0.925400\n","epoch:[306], loss:0.194325, acc:0.925300\n","epoch:[307], loss:0.192840, acc:0.927000\n","epoch:[308], loss:0.192859, acc:0.925400\n","epoch:[309], loss:0.195070, acc:0.925500\n","epoch:[310], loss:0.191263, acc:0.923600\n","epoch:[311], loss:0.193250, acc:0.923900\n","epoch:[312], loss:0.192573, acc:0.927500\n","epoch:[313], loss:0.190975, acc:0.926900\n","epoch:[314], loss:0.193637, acc:0.925800\n","epoch:[315], loss:0.193838, acc:0.926800\n","epoch:[316], loss:0.191597, acc:0.926900\n","epoch:[317], loss:0.190295, acc:0.927100\n","epoch:[318], loss:0.190734, acc:0.927000\n","epoch:[319], loss:0.189488, acc:0.921200\n","epoch:[320], loss:0.188975, acc:0.926400\n","epoch:[321], loss:0.189541, acc:0.927200\n","epoch:[322], loss:0.189586, acc:0.926100\n","epoch:[323], loss:0.189694, acc:0.924500\n","epoch:[324], loss:0.189875, acc:0.925800\n","epoch:[325], loss:0.189010, acc:0.926300\n","epoch:[326], loss:0.187217, acc:0.926800\n","epoch:[327], loss:0.188495, acc:0.926600\n","epoch:[328], loss:0.186950, acc:0.925700\n","epoch:[329], loss:0.188514, acc:0.927600\n","epoch:[330], loss:0.186497, acc:0.927100\n","epoch:[331], loss:0.186427, acc:0.924300\n","epoch:[332], loss:0.184550, acc:0.924900\n","epoch:[333], loss:0.186344, acc:0.925800\n","epoch:[334], loss:0.186300, acc:0.923600\n","epoch:[335], loss:0.185792, acc:0.927400\n","epoch:[336], loss:0.186878, acc:0.923100\n","epoch:[337], loss:0.184769, acc:0.924400\n","epoch:[338], loss:0.185129, acc:0.928700\n","epoch:[339], loss:0.184501, acc:0.923200\n","epoch:[340], loss:0.186459, acc:0.928000\n","epoch:[341], loss:0.184544, acc:0.925700\n","epoch:[342], loss:0.183480, acc:0.929200\n","epoch:[343], loss:0.183620, acc:0.927400\n","epoch:[344], loss:0.184189, acc:0.927500\n","epoch:[345], loss:0.182803, acc:0.925600\n","epoch:[346], loss:0.181969, acc:0.925400\n","epoch:[347], loss:0.180771, acc:0.924000\n","epoch:[348], loss:0.182007, acc:0.928600\n","epoch:[349], loss:0.181854, acc:0.929700\n","epoch:[350], loss:0.181607, acc:0.926200\n","epoch:[351], loss:0.181360, acc:0.928500\n","epoch:[352], loss:0.182271, acc:0.927100\n","epoch:[353], loss:0.179229, acc:0.929100\n","epoch:[354], loss:0.180472, acc:0.927500\n","epoch:[355], loss:0.180064, acc:0.929400\n","epoch:[356], loss:0.180477, acc:0.928200\n","epoch:[357], loss:0.179564, acc:0.928800\n","epoch:[358], loss:0.181342, acc:0.925200\n","epoch:[359], loss:0.179376, acc:0.927400\n","epoch:[360], loss:0.179853, acc:0.927300\n","epoch:[361], loss:0.177812, acc:0.928500\n","epoch:[362], loss:0.178942, acc:0.929300\n","epoch:[363], loss:0.178402, acc:0.927500\n","epoch:[364], loss:0.177765, acc:0.926800\n","epoch:[365], loss:0.178753, acc:0.927800\n","epoch:[366], loss:0.177010, acc:0.927800\n","epoch:[367], loss:0.175931, acc:0.929200\n","epoch:[368], loss:0.177760, acc:0.926100\n","epoch:[369], loss:0.176907, acc:0.929000\n","epoch:[370], loss:0.176266, acc:0.924300\n","epoch:[371], loss:0.176248, acc:0.927500\n","epoch:[372], loss:0.176711, acc:0.927300\n","epoch:[373], loss:0.176002, acc:0.926800\n","epoch:[374], loss:0.174484, acc:0.927100\n","epoch:[375], loss:0.175077, acc:0.929700\n","epoch:[376], loss:0.173178, acc:0.926400\n","epoch:[377], loss:0.174470, acc:0.928400\n","epoch:[378], loss:0.173945, acc:0.929100\n","epoch:[379], loss:0.173205, acc:0.927800\n","epoch:[380], loss:0.172676, acc:0.928000\n","epoch:[381], loss:0.173659, acc:0.926400\n","epoch:[382], loss:0.174558, acc:0.928900\n","epoch:[383], loss:0.173831, acc:0.929400\n","epoch:[384], loss:0.174152, acc:0.927300\n","epoch:[385], loss:0.173996, acc:0.929800\n","epoch:[386], loss:0.171992, acc:0.929600\n","epoch:[387], loss:0.172587, acc:0.927400\n","epoch:[388], loss:0.171459, acc:0.928000\n","epoch:[389], loss:0.171213, acc:0.929400\n","epoch:[390], loss:0.171084, acc:0.930000\n","epoch:[391], loss:0.172009, acc:0.929900\n","epoch:[392], loss:0.170768, acc:0.929100\n","epoch:[393], loss:0.169565, acc:0.926800\n","epoch:[394], loss:0.169951, acc:0.929800\n","epoch:[395], loss:0.170257, acc:0.929800\n","epoch:[396], loss:0.171122, acc:0.928800\n","epoch:[397], loss:0.169465, acc:0.930400\n","epoch:[398], loss:0.171736, acc:0.929000\n","epoch:[399], loss:0.168461, acc:0.927700\n","epoch:[400], loss:0.168504, acc:0.922700\n","epoch:[401], loss:0.170090, acc:0.927800\n","epoch:[402], loss:0.167901, acc:0.929700\n","epoch:[403], loss:0.169465, acc:0.929600\n","epoch:[404], loss:0.168621, acc:0.931800\n","epoch:[405], loss:0.168712, acc:0.928200\n","epoch:[406], loss:0.168061, acc:0.931300\n","epoch:[407], loss:0.168016, acc:0.930000\n","epoch:[408], loss:0.167946, acc:0.929000\n","epoch:[409], loss:0.165860, acc:0.929800\n","epoch:[410], loss:0.167801, acc:0.928600\n","epoch:[411], loss:0.167952, acc:0.929400\n","epoch:[412], loss:0.167020, acc:0.932000\n","epoch:[413], loss:0.166416, acc:0.929600\n","epoch:[414], loss:0.166620, acc:0.928800\n","epoch:[415], loss:0.165651, acc:0.927900\n","epoch:[416], loss:0.165717, acc:0.930900\n","epoch:[417], loss:0.164877, acc:0.929700\n","epoch:[418], loss:0.165883, acc:0.931100\n","epoch:[419], loss:0.164616, acc:0.929400\n","epoch:[420], loss:0.165551, acc:0.930800\n","epoch:[421], loss:0.164565, acc:0.929600\n","epoch:[422], loss:0.163650, acc:0.929800\n","epoch:[423], loss:0.163613, acc:0.928000\n","epoch:[424], loss:0.163774, acc:0.929900\n","epoch:[425], loss:0.164801, acc:0.930100\n","epoch:[426], loss:0.164142, acc:0.932000\n","epoch:[427], loss:0.163409, acc:0.931800\n","epoch:[428], loss:0.163541, acc:0.928300\n","epoch:[429], loss:0.163033, acc:0.929800\n","epoch:[430], loss:0.164326, acc:0.930200\n","epoch:[431], loss:0.163346, acc:0.931100\n","epoch:[432], loss:0.162416, acc:0.930100\n","epoch:[433], loss:0.161974, acc:0.931200\n","epoch:[434], loss:0.161416, acc:0.932600\n","epoch:[435], loss:0.161560, acc:0.931700\n","epoch:[436], loss:0.162351, acc:0.930800\n","epoch:[437], loss:0.160240, acc:0.930700\n","epoch:[438], loss:0.160893, acc:0.930500\n","epoch:[439], loss:0.161549, acc:0.932000\n","epoch:[440], loss:0.161477, acc:0.931000\n","epoch:[441], loss:0.160407, acc:0.930400\n","epoch:[442], loss:0.159955, acc:0.931500\n","epoch:[443], loss:0.161488, acc:0.929900\n","epoch:[444], loss:0.159268, acc:0.932500\n","epoch:[445], loss:0.160070, acc:0.929600\n","epoch:[446], loss:0.160755, acc:0.931500\n","epoch:[447], loss:0.160679, acc:0.930300\n","epoch:[448], loss:0.160382, acc:0.932100\n","epoch:[449], loss:0.158061, acc:0.930300\n","epoch:[450], loss:0.159086, acc:0.932900\n","epoch:[451], loss:0.158072, acc:0.932800\n","epoch:[452], loss:0.159262, acc:0.931600\n","epoch:[453], loss:0.158704, acc:0.931800\n","epoch:[454], loss:0.159003, acc:0.931800\n","epoch:[455], loss:0.156726, acc:0.931400\n","epoch:[456], loss:0.156761, acc:0.929100\n","epoch:[457], loss:0.157374, acc:0.931500\n","epoch:[458], loss:0.156865, acc:0.931800\n","epoch:[459], loss:0.158726, acc:0.930400\n","epoch:[460], loss:0.158148, acc:0.929700\n","epoch:[461], loss:0.157960, acc:0.932700\n","epoch:[462], loss:0.156713, acc:0.932800\n","epoch:[463], loss:0.157154, acc:0.933200\n","epoch:[464], loss:0.155596, acc:0.933400\n","epoch:[465], loss:0.155419, acc:0.931100\n","epoch:[466], loss:0.156294, acc:0.929700\n","epoch:[467], loss:0.155925, acc:0.929000\n","epoch:[468], loss:0.155391, acc:0.933000\n","epoch:[469], loss:0.155967, acc:0.932400\n","epoch:[470], loss:0.155005, acc:0.933500\n","epoch:[471], loss:0.154432, acc:0.930400\n","epoch:[472], loss:0.153494, acc:0.931500\n","epoch:[473], loss:0.154961, acc:0.931600\n","epoch:[474], loss:0.154124, acc:0.930100\n","epoch:[475], loss:0.154779, acc:0.933400\n","epoch:[476], loss:0.154099, acc:0.929200\n","epoch:[477], loss:0.154937, acc:0.932900\n","epoch:[478], loss:0.154241, acc:0.933900\n","epoch:[479], loss:0.153271, acc:0.932400\n","epoch:[480], loss:0.151545, acc:0.926400\n","epoch:[481], loss:0.153390, acc:0.933700\n","epoch:[482], loss:0.152789, acc:0.931700\n","epoch:[483], loss:0.154497, acc:0.933100\n","epoch:[484], loss:0.150612, acc:0.930300\n","epoch:[485], loss:0.152562, acc:0.931700\n","epoch:[486], loss:0.151928, acc:0.933100\n","epoch:[487], loss:0.151969, acc:0.933300\n","epoch:[488], loss:0.150465, acc:0.933200\n","epoch:[489], loss:0.152127, acc:0.933800\n","epoch:[490], loss:0.151189, acc:0.934600\n","epoch:[491], loss:0.151304, acc:0.933400\n","epoch:[492], loss:0.151007, acc:0.932700\n","epoch:[493], loss:0.150559, acc:0.932200\n","epoch:[494], loss:0.151175, acc:0.933200\n","epoch:[495], loss:0.148959, acc:0.934000\n","epoch:[496], loss:0.150028, acc:0.932300\n","epoch:[497], loss:0.150001, acc:0.933200\n","epoch:[498], loss:0.150333, acc:0.933900\n","epoch:[499], loss:0.149504, acc:0.931800\n"]}]},{"cell_type":"markdown","source":["**GAN train**"],"metadata":{"id":"xsQF1cMbQ8_D"}},{"cell_type":"code","source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()\n","net[\"generator\"] = Generator()\n","net[\"discriminator\"] = Discriminator()\n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\", \"generator\", \"discriminator\"], True)\n","\n","D_optimizer = optim.Adam(get_params(net, [\"discriminator\"]), lr=2e-4, betas=(0.5, 0.999))\n","G_optimizer = optim.Adam(get_params(net, [\"generator\"]), lr=2e-4, betas=(0.5, 0.999))\n","BCE_criterion = nn.BCELoss().to(device)\n","\n","E_checkpoint = torch.load(\"./checkpoint/client_extractor.pkl\", map_location=torch.device('cpu'))\n","net[\"extractor\"].load_state_dict(E_checkpoint)\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","\n","def discriminator_loss(E, G, y):\n","    ones = torch.ones((E.size(0), 1)).to(device)\n","    ED = net[\"discriminator\"](E.detach(), y)\n","    ED_loss = BCE_criterion(ED, ones)\n","\n","    zeros = torch.zeros((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G.detach(), y)\n","    GD_loss = BCE_criterion(GD, zeros)\n","    return ED_loss + GD_loss\n","\n","\n","def generator_loss(G, y):\n","    ones = torch.ones((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G, y)\n","    G_loss = BCE_criterion(GD, ones)\n","    return G_loss\n","\n","\n","for epoch in range(500):\n","    # train\n","    frozen_net(net, [\"generator\", \"discriminator\"], False)\n","\n","    D_losses, G_losses = [], []\n","    for batch, (x, y) in enumerate(client_trainloader):\n","        x = x.to(device)\n","        y = y.to(device)\n","        z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","\n","        with torch.no_grad():\n","            E = net[\"extractor\"](x)\n","        G = net[\"generator\"](z, y)\n","\n","        # update D\n","        D_optimizer.zero_grad()\n","        D_loss = discriminator_loss(E, G, y)\n","        D_loss.backward()\n","        D_optimizer.step()\n","        D_losses.append(D_loss.item())\n","\n","        # update G\n","        G_optimizer.zero_grad()\n","        G_loss = generator_loss(G, y)\n","        G_loss.backward()\n","        G_optimizer.step()\n","        G_losses.append(G_loss.item())\n","    \n","    frozen_net(net, [\"generator\", \"discriminator\"], True)\n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            y = y.to(device)\n","            z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","\n","            G = net[\"generator\"](z, y)\n","            GC = net[\"classifier\"](G)\n","\n","            correct += torch.sum((torch.argmax(GC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], D_loss:%2.6f, G_loss:%2.6f, acc:%2.6f\"\n","        %(epoch, np.mean(D_losses), np.mean(G_losses), acc))\n","\n","    if (epoch+1) % 10 == 0:\n","        torch.save(net[\"generator\"].state_dict(), \"./checkpoint/client_generator.pkl\")\n","        torch.save(net[\"discriminator\"].state_dict(), \"./checkpoint/client_discriminator.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mcjqTXxQ_Y9","outputId":"0c4f3f6d-56d8-456e-b9e2-599d0943ddc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], D_loss:0.322451, G_loss:3.091217, acc:0.073742\n","epoch:[ 1], D_loss:0.152293, G_loss:4.494790, acc:0.254111\n","epoch:[ 2], D_loss:0.220933, G_loss:4.630026, acc:0.460389\n","epoch:[ 3], D_loss:0.537664, G_loss:3.878116, acc:0.871450\n","epoch:[ 4], D_loss:0.692956, G_loss:3.268778, acc:0.961136\n","epoch:[ 5], D_loss:0.770612, G_loss:2.770874, acc:0.984554\n","epoch:[ 6], D_loss:0.824883, G_loss:2.588488, acc:0.970603\n","epoch:[ 7], D_loss:0.810655, G_loss:2.484770, acc:0.957150\n","epoch:[ 8], D_loss:0.766253, G_loss:2.524640, acc:0.983558\n","epoch:[ 9], D_loss:0.766418, G_loss:2.554713, acc:0.976582\n","epoch:[10], D_loss:0.714651, G_loss:2.658547, acc:0.963129\n","epoch:[11], D_loss:0.688639, G_loss:2.799146, acc:0.971599\n","epoch:[12], D_loss:0.676793, G_loss:2.882823, acc:0.975585\n","epoch:[13], D_loss:0.680232, G_loss:2.845784, acc:0.984554\n","epoch:[14], D_loss:0.711148, G_loss:2.872360, acc:0.981565\n","epoch:[15], D_loss:0.670146, G_loss:2.876335, acc:0.983059\n","epoch:[16], D_loss:0.693512, G_loss:2.877487, acc:0.987544\n","epoch:[17], D_loss:0.647747, G_loss:2.892514, acc:0.983558\n","epoch:[18], D_loss:0.675223, G_loss:2.982509, acc:0.983558\n","epoch:[19], D_loss:0.625351, G_loss:2.988697, acc:0.988042\n","epoch:[20], D_loss:0.615465, G_loss:3.054057, acc:0.985052\n","epoch:[21], D_loss:0.634923, G_loss:3.059779, acc:0.980070\n","epoch:[22], D_loss:0.679766, G_loss:3.028849, acc:0.986547\n","epoch:[23], D_loss:0.680806, G_loss:2.940638, acc:0.988042\n","epoch:[24], D_loss:0.572733, G_loss:3.102902, acc:0.986049\n","epoch:[25], D_loss:0.671040, G_loss:3.094810, acc:0.987544\n","epoch:[26], D_loss:0.677954, G_loss:3.070412, acc:0.985551\n","epoch:[27], D_loss:0.594047, G_loss:3.151199, acc:0.990035\n","epoch:[28], D_loss:0.635165, G_loss:3.168688, acc:0.985551\n","epoch:[29], D_loss:0.564145, G_loss:3.184989, acc:0.988540\n","epoch:[30], D_loss:0.615357, G_loss:3.204519, acc:0.992526\n","epoch:[31], D_loss:0.654622, G_loss:3.217117, acc:0.991530\n","epoch:[32], D_loss:0.571843, G_loss:3.272733, acc:0.984056\n","epoch:[33], D_loss:0.630948, G_loss:3.194645, acc:0.994519\n","epoch:[34], D_loss:0.613546, G_loss:3.281237, acc:0.990533\n","epoch:[35], D_loss:0.553310, G_loss:3.253249, acc:0.993523\n","epoch:[36], D_loss:0.647465, G_loss:3.249600, acc:0.991031\n","epoch:[37], D_loss:0.590221, G_loss:3.350008, acc:0.993523\n","epoch:[38], D_loss:0.490851, G_loss:3.416550, acc:0.993523\n","epoch:[39], D_loss:0.584758, G_loss:3.428574, acc:0.989038\n","epoch:[40], D_loss:0.522635, G_loss:3.475379, acc:0.989537\n","epoch:[41], D_loss:0.514089, G_loss:3.601842, acc:0.991530\n","epoch:[42], D_loss:0.573462, G_loss:3.543164, acc:0.989038\n","epoch:[43], D_loss:0.511452, G_loss:3.584914, acc:0.988042\n","epoch:[44], D_loss:0.576777, G_loss:3.585298, acc:0.987544\n","epoch:[45], D_loss:0.596372, G_loss:3.441918, acc:0.992028\n","epoch:[46], D_loss:0.534141, G_loss:3.517190, acc:0.993024\n","epoch:[47], D_loss:0.513577, G_loss:3.590560, acc:0.989038\n","epoch:[48], D_loss:0.537313, G_loss:3.664967, acc:0.992028\n","epoch:[49], D_loss:0.562108, G_loss:3.548462, acc:0.990035\n","epoch:[50], D_loss:0.497522, G_loss:3.605155, acc:0.991530\n","epoch:[51], D_loss:0.485355, G_loss:3.684849, acc:0.992028\n","epoch:[52], D_loss:0.522734, G_loss:3.782697, acc:0.989038\n","epoch:[53], D_loss:0.616813, G_loss:3.585596, acc:0.989038\n","epoch:[54], D_loss:0.521963, G_loss:3.628749, acc:0.989537\n","epoch:[55], D_loss:0.486521, G_loss:3.785153, acc:0.990035\n","epoch:[56], D_loss:0.473700, G_loss:3.796475, acc:0.990035\n","epoch:[57], D_loss:0.457277, G_loss:3.842569, acc:0.990035\n","epoch:[58], D_loss:0.449927, G_loss:3.878626, acc:0.991530\n","epoch:[59], D_loss:0.473221, G_loss:3.998824, acc:0.988540\n","epoch:[60], D_loss:0.448800, G_loss:3.857439, acc:0.991031\n","epoch:[61], D_loss:0.462025, G_loss:4.038507, acc:0.996014\n","epoch:[62], D_loss:0.402420, G_loss:4.042078, acc:0.996512\n","epoch:[63], D_loss:0.525067, G_loss:4.138161, acc:0.993024\n","epoch:[64], D_loss:0.492144, G_loss:3.856286, acc:0.992526\n","epoch:[65], D_loss:0.483516, G_loss:3.957502, acc:0.992028\n"]}]},{"cell_type":"markdown","source":["**GAN div train**"],"metadata":{"id":"Nj5bLTAsEguA"}},{"cell_type":"code","source":["net = nn.ModuleDict()\n","net[\"extractor\"] = Extractor()          \n","net[\"classifier\"] = Classifier()\n","net[\"generator\"] = Generator()\n","net[\"discriminator\"] = Discriminator()\n","net = net.to(device)\n","frozen_net(net, [\"extractor\", \"classifier\", \"generator\", \"discriminator\"], True)\n","\n","D_optimizer = optim.Adam(get_params(net, [\"discriminator\"]), lr=2e-4, betas=(0.5, 0.999))\n","G_optimizer = optim.Adam(get_params(net, [\"generator\"]), lr=4e-4, betas=(0.5, 0.999))\n","BCE_criterion = nn.BCELoss().to(device)\n","\n","E_checkpoint = torch.load(\"./checkpoint/client_extractor.pkl\", map_location=torch.device('cpu'))\n","net[\"extractor\"].load_state_dict(E_checkpoint)\n","C_checkpoint = torch.load(\"./checkpoint/client_classifier.pkl\", map_location=torch.device('cpu'))\n","net[\"classifier\"].load_state_dict(C_checkpoint)\n","\n","\n","def diversity_loss(G1, G2, z1, z2):\n","    lz = torch.mean(torch.abs(G2 - G1)) / torch.mean(torch.abs(z2 - z1))\n","    eps = 1 * 1e-5\n","    G_div = 1 / (lz + eps)\n","    return G_div\n","\n","\n","def discriminator_loss(E, G, y):\n","    ones = torch.ones((E.size(0), 1)).to(device)\n","    ED = net[\"discriminator\"](E.detach(), y)\n","    ED_loss = BCE_criterion(ED, ones)\n","\n","    zeros = torch.zeros((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G.detach(), y)\n","    GD_loss = BCE_criterion(GD, zeros)\n","    return ED_loss + GD_loss\n","\n","\n","def generator_loss(G, y):\n","    ones = torch.ones((G.size(0), 1)).to(device)\n","    GD = net[\"discriminator\"](G, y)\n","    G_loss = BCE_criterion(GD, ones)\n","    return G_loss\n","\n","\n","for epoch in range(500):\n","    # train\n","    frozen_net(net, [\"generator\", \"discriminator\"], False)\n","\n","    D_losses, G_losses, G_divs = [], [], []\n","    for batch, (x, y) in enumerate(client_trainloader):\n","        x = x.to(device)\n","        y = y.to(device)\n","\n","        with torch.no_grad():\n","            E = net[\"extractor\"](x)\n","\n","        # update D\n","        D_optimizer.zero_grad()\n","        z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","        G = net[\"generator\"](z, y)\n","        D_loss = discriminator_loss(E, G, y)\n","\n","        D_loss.backward()\n","        D_optimizer.step()\n","        D_losses.append(D_loss.item())\n","\n","        # update G\n","        if (batch+1)%2 == 0:\n","            G_optimizer.zero_grad()\n","            ys = torch.cat([y,y], 0)\n","            zs = torch.randn(x.size(0)*2, 100, 1, 1).to(device)\n","            Gs = net[\"generator\"](zs, ys)\n","            G_loss = generator_loss(Gs, ys)\n","\n","            z1, z2 = torch.split(zs, x.size(0), 0)\n","            G1, G2 = torch.split(Gs, x.size(0), 0)\n","            G_div = diversity_loss(G1, G2, z1, z2)\n","\n","            (G_loss + G_div).backward()\n","            G_optimizer.step()\n","            G_losses.append(G_loss.item())\n","            G_divs.append(G_div.item())\n","    \n","    frozen_net(net, [\"generator\", \"discriminator\"], True)\n","\n","    # test\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in client_testloader:\n","            y = y.to(device)\n","            z = torch.randn(x.size(0), 100, 1, 1).to(device)\n","\n","            G = net[\"generator\"](z, y)\n","            GC = net[\"classifier\"](G)\n","\n","            correct += torch.sum((torch.argmax(GC, dim=1) == y).float()).item()\n","            total += x.size(0)\n","    acc = correct / total\n","\n","    print(\"epoch:[%2d], D_loss:%2.6f, G_loss:%2.6f, G_div:%2.6f, acc:%2.6f\"\n","        %(epoch, np.mean(D_losses), np.mean(G_losses), np.mean(G_divs), acc))\n","\n","    if (epoch+1) % 10 == 0:\n","        torch.save(net[\"generator\"].state_dict(), \"./checkpoint/client_generator_div.pkl\")\n","        torch.save(net[\"discriminator\"].state_dict(), \"./checkpoint/client_discriminator_div.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rGpwDDm5EjjR","outputId":"d4c8d352-ad48-41f5-f42d-204f38a575ae","executionInfo":{"status":"error","timestamp":1640312816713,"user_tz":-480,"elapsed":1030285,"user":{"displayName":"吴岳洲","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10140082103242733864"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:[ 0], D_loss:0.262058, G_loss:2.837883, G_div:10.942803, acc:0.073244\n","epoch:[ 1], D_loss:0.077761, G_loss:4.405389, G_div:5.222893, acc:0.148979\n","epoch:[ 2], D_loss:0.111567, G_loss:5.197156, G_div:4.489521, acc:0.270553\n","epoch:[ 3], D_loss:0.290063, G_loss:4.550537, G_div:4.395665, acc:0.462880\n","epoch:[ 4], D_loss:0.430056, G_loss:3.996388, G_div:4.533616, acc:0.700050\n","epoch:[ 5], D_loss:0.510822, G_loss:3.429086, G_div:4.508744, acc:0.820628\n","epoch:[ 6], D_loss:0.574429, G_loss:3.010144, G_div:4.640945, acc:0.859492\n","epoch:[ 7], D_loss:0.634799, G_loss:2.925211, G_div:4.714569, acc:0.924265\n","epoch:[ 8], D_loss:0.665389, G_loss:2.746478, G_div:4.610947, acc:0.936223\n","epoch:[ 9], D_loss:0.603460, G_loss:2.831660, G_div:4.719760, acc:0.944195\n","epoch:[10], D_loss:0.587366, G_loss:2.890055, G_div:4.652732, acc:0.947683\n","epoch:[11], D_loss:0.573084, G_loss:2.911556, G_div:4.700296, acc:0.943697\n","epoch:[12], D_loss:0.582995, G_loss:2.809167, G_div:4.607203, acc:0.953662\n","epoch:[13], D_loss:0.590737, G_loss:2.848091, G_div:4.660510, acc:0.951669\n","epoch:[14], D_loss:0.547253, G_loss:2.850974, G_div:4.665761, acc:0.946687\n","epoch:[15], D_loss:0.576617, G_loss:2.881333, G_div:4.666387, acc:0.958146\n","epoch:[16], D_loss:0.606989, G_loss:2.916766, G_div:4.634794, acc:0.951171\n","epoch:[17], D_loss:0.520862, G_loss:2.790035, G_div:4.645914, acc:0.960638\n","epoch:[18], D_loss:0.553797, G_loss:3.109367, G_div:4.591000, acc:0.967613\n","epoch:[19], D_loss:0.594878, G_loss:2.892460, G_div:4.662754, acc:0.964126\n","epoch:[20], D_loss:0.559331, G_loss:2.842394, G_div:4.700298, acc:0.977578\n","epoch:[21], D_loss:0.594484, G_loss:2.847591, G_div:4.830280, acc:0.968610\n","epoch:[22], D_loss:0.550725, G_loss:2.837846, G_div:4.720662, acc:0.964126\n","epoch:[23], D_loss:0.517308, G_loss:3.089458, G_div:4.722228, acc:0.971101\n","epoch:[24], D_loss:0.508202, G_loss:2.890800, G_div:4.750169, acc:0.974589\n","epoch:[25], D_loss:0.589742, G_loss:2.798006, G_div:4.682354, acc:0.976582\n","epoch:[26], D_loss:0.631161, G_loss:2.874975, G_div:4.840692, acc:0.970105\n","epoch:[27], D_loss:0.556856, G_loss:2.921780, G_div:4.685866, acc:0.964624\n","epoch:[28], D_loss:0.476044, G_loss:2.991840, G_div:4.709104, acc:0.965620\n","epoch:[29], D_loss:0.578029, G_loss:2.822060, G_div:4.758159, acc:0.966119\n","epoch:[30], D_loss:0.496744, G_loss:2.799432, G_div:4.721507, acc:0.971599\n","epoch:[31], D_loss:0.597322, G_loss:2.769480, G_div:4.717233, acc:0.974589\n","epoch:[32], D_loss:0.511307, G_loss:2.842125, G_div:4.798174, acc:0.972098\n","epoch:[33], D_loss:0.528058, G_loss:3.054484, G_div:4.750090, acc:0.976582\n","epoch:[34], D_loss:0.515047, G_loss:2.965967, G_div:4.725660, acc:0.976582\n","epoch:[35], D_loss:0.470999, G_loss:2.992679, G_div:4.630294, acc:0.968112\n","epoch:[36], D_loss:0.496198, G_loss:3.200462, G_div:4.646654, acc:0.977080\n","epoch:[37], D_loss:0.579898, G_loss:2.846038, G_div:4.676076, acc:0.976582\n","epoch:[38], D_loss:0.511069, G_loss:3.019639, G_div:4.671943, acc:0.984554\n","epoch:[39], D_loss:0.511493, G_loss:3.092905, G_div:4.745187, acc:0.980568\n","epoch:[40], D_loss:0.488546, G_loss:3.117254, G_div:4.687451, acc:0.976582\n","epoch:[41], D_loss:0.503751, G_loss:3.134231, G_div:4.798658, acc:0.979571\n","epoch:[42], D_loss:0.577341, G_loss:2.954023, G_div:4.773358, acc:0.978077\n","epoch:[43], D_loss:0.541371, G_loss:2.923083, G_div:4.737987, acc:0.979571\n","epoch:[44], D_loss:0.479706, G_loss:3.107839, G_div:4.701591, acc:0.976084\n","epoch:[45], D_loss:0.452770, G_loss:3.027946, G_div:4.693590, acc:0.977080\n","epoch:[46], D_loss:0.490530, G_loss:3.179792, G_div:4.702645, acc:0.970603\n","epoch:[47], D_loss:0.465461, G_loss:2.882584, G_div:4.607440, acc:0.983558\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-320b48b74fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mGs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-6edf0241ee2e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    923\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    924\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}